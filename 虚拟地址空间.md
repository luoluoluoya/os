#### 历史
* 早期,构建计算机操作系统非常简单. 你可能会问,为什么? 因为用户对操作系统的期望不高. 然而当"易于使用","高性能","可靠性"等要求提出后,这导致了所有这些令人头痛的问题. 

* 早期系统: 从内存来看,早期的机器并没有提供多少抽象给用户. 基本上,机器的物理内存就是一个连续的字节序列.

* 多道程序和时分共享
    * 过了一段时间,由于机器昂贵,人们开始更有效地共享机器. 因此,**多道程序(multiprogramming)系统**时代开启,其中多个进程在给定时间准备运行,比如当有
      一个进程在等待 I/O 操作的时候,操作系统会切换这些进程,这样增加了 CPU 的有效利用率(utilization). 那时候,效率(efficiency)的提高尤其重要,因为每台机器的成本是数十万美元甚至数百万美元. 
    * 但很快,人们开始对机器要求更多,**分时系统**的时代诞生了. 具体来说,许多人意识到批量计算的局限性,尤其是程序员本身,他们厌倦了长时间的(因此也是低效率的)编程—调试循环. **交互性**变得很重要,因为许多用户可能同时在使用机器,每个人都在等待(或希望)他们执行的任务及时响应. 
    * 一种实现时分共享的方法,是**让一个进程单独占用全部内存运行一小段时间,然后停止它,并将它所有的状态信息保存在磁盘上(包含所有的物理内存),加载其他进程的状态信息,再运行一段时间**,这就实现了某种比较粗糙的机器共享. 遗憾的是,这种方法有一个问题：太慢了,特别是当内存增长的时候. 虽然保存和恢复寄存器级的状态信息(程序计数器、通用寄存器等)相对较快,但将全部的内存信息保存到磁盘就太慢了. 因此,**在进程切换的时候,我们仍然将进程信息放在内存中**,这样操作系统可以更有效率地实现时分共享. 
    
* 随着时分共享变得更流行. 特别是多个程序同时驻留在内存中,使保护(protection)成为重要问题. **人们不希望一个进程可以读取其他进程的内存,更别说修改了**. 

* 地址空间 
    * 操作系统需要提供一个易用(easy to use)的物理内存抽象. 这个抽象叫作地址空间(address space),是运行的程序看到的系统中的内存. 理解这个基本的操作系统内存抽象,是了解内存虚拟化的关键. 
    * 一个进程的地址空间包含运行的程序的所有内存状态. 比如：程序的代码(code,指令)必须在内存中,因此它们在地址空间里. 当程序在运行的时候,利用栈(stack)来保存当前的函数调用信息,分配空间给局部变量,传递参数和函数返回值. 最后,堆(heap)用于管理动态分配的、用户管理的内存,就像你从 C 语言调用 malloc()或面向对象语言(如 C ++ 或 Java)中调用 new 获得内存. 当然,还有其他的东西(例如,静态初始化的变量)
    * 在程序运行时,地址空间有两个区域可能增长(或者收缩). 它们就是堆(在顶部)和栈(在底部). 把它们放在那里,是因为它们都希望能够增长. 通过将它们放在地址空间的两端,我们可以允许这样的增长：它们只需要在相反的方向增长. (堆栈和堆的这种放置方法只是一种约定,如果你愿意,可以用不同的方式安排地址空间). 
    
* 隔离原则
    * 隔离是建立可靠系统的关键原则. 如果两个实体相互隔离,这意味着一个实体的失败不会影响另一个实体. 操作系统力求让进程彼此隔离,从而防止相互造成伤害. 通过内存隔离,操作系统进一步确保
      运行程序不会影响底层操作系统的操作. 

#### 目标

* 虚拟内存(VM)系统的一个主要目标是**透明(transparency)**. 操作系统实现虚拟内存的方式,应该让运行的程序看不见. 因此,程序不应该感知到内存被虚拟化的事实,相
  反,程序的行为就好像它拥有自己的私有物理内存. 在幕后,操作系统(和硬件)完成了所有的工作,让不同的工作复用内存,从而实现这个假象. 

* 虚拟内存的另一个目标是**效率(efficiency)**. 操作系统应该追求虚拟化尽可能高效(efficient),包括时间上(即不会使程序运行得更慢)和空间上(即不需要太多额外的内存
  来支持虚拟化). 在实现高效率虚拟化时,操作系统将不得不依靠硬件支持,包括 TLB 这样的硬件功能. 

* 最后,虚拟内存第三个目标是**保护(protection)**. 操作系统应确保进程受到保护(protect),不会受其他进程影响,操作系统本身也不会受进程影响. 当一个进程执行加载、存储或指
  令提取时,它不应该以任何方式访问或影响任何其他进程或操作系统本身的内存内容(即在它的地址空间之外的任何内容). 因此,保护让我们能够在进程之间提供隔离(isolation)
  的特性,每个进程都应该在自己的独立环境中运行,避免其他出错或恶意进程的影响. 

* 你看到的所有地址都不是真的
    * 实际上,作为用户级程序的程序员,可以看到的任何地址都是虚拟地址. 只有操作系统,通过精妙的虚拟化内存技术,知道这些指令和数据所在的物理内存的位置. 所以永远不要忘记：如
      果你在一个程序中打印出一个地址,那就是一个虚拟的地址. 虚拟地址只是提供地址如何在内存中分布的假象,只有操作系统(和硬件)才知道物理地址. 


* 虚拟内存: 虚拟内存系统负责为程序提供一个巨大的、稀疏的、私有的地址空间的假象,其中保存了程序的所有指令和数据. 操作系统在专门硬件的帮助下,通过每一个虚拟内存的索引,
  将其转换为物理地址,物理内存根据获得的物理地址但获取所需的信息. 操作系统会同时对许多进程执行此操作,并且确保程序之间互相不会受到影响,也不会影响操作系统. 

* 在实现 CPU 虚拟化时,我们遵循的一般准则被称为受限直接访问(Limited Direct Execution,LDE). LDE 背后的想法很简单：让程序运行的大部分指令直接访问硬件,只在一些关键点(如进程发起系统调用或发生时钟中断)
  由操作系统介入来确保"在正确时间,正确的地点,做正确的事". **为了实现高效的虚拟化,操作系统应该尽量让程序自己运行,同时通过在关键点的及时介入(interposing),来保持对硬件的控制**. 高效和控制是现代操作系统的两个主要目标. 

* 在实现虚拟内存时,我们将追求类似的战略,在实现**高效**和**控制**的同时,提供期望的虚拟化. 高效决定了我们要利用硬件的支持,(比如 TLB、页表等). 控制意味着操作系统要确保应用程序只能访问它自己的内存空间.
  因此,要保护应用程序不会相互影响,也不会影响操作系统,我们需要硬件的帮助. 最后,我们对虚拟内存还有一点要求,即**灵活性**. 具体来说,我们希望程序能以任何方式访问它自己的地址空间,从而让系统更容易编程. 
  所以, 关键问题在于：如何实现高效的内存虚拟化?如何提供应用程序所需的灵活性?如何保持控制应用程序可访问的内存位置,从而确保应用程序的内存访问受到合理的限制?如何高效地实现这一切?

* 利用一种通用技术,有时被称为**基于硬件的地址转换(hardware-based address translation)**,简称为地址转换(address translation). 它可以看成是受限直接执行这种一般方
  法的补充. 利用地址转换,硬件对每次内存访问进行处理(即指令获取、数据读取或写入),将指令中的虚拟(virtual)地址转换为数据实际存储的物理(physical)地址. 因此,在每次内存引用时,
  硬件都会进行地址转换,将应用程序的内存引用重定位到内存中实际的位置.当然,仅仅依靠硬件不足以实现虚拟内存,因为它只是提供了底层机制来提高效率.操作系统必须在关键的位置介入,
  设置好硬件,以便完成正确的地址转换. 因此它必须管理内存(manage memory),记录被占用和空闲的内存位置,并明智而谨慎地介入,保持对内存使用的控制. 

* 同样,所有这些工作都是为了创造一种美丽的假象：**每个程序都拥有私有的内存**,那里存放着它自己的代码和数据. 虚拟现实的背后是丑陋的物理事实：**许多程序其实是在同一时间共享着内存**,就像 CPU(或多个 CPU)在不同的程序间切换运行. 通过虚拟化,操作系统(在硬件的帮助下)将丑陋的机器现实转化成一种有用的、强大的、易于使用的抽象. 
  
* 重定位的集中形式
    * 静态重定位
        * 在早期,在硬件支持重定位之前,一些系统曾经采用纯软件的重定位方式. 基本技术被称为**静态重定位(载入时重定位)**,其中一个名为加载程序(loader)的软件接手将要运行的可执行程序,将它的地址重写到物理内存中期望的偏移位置. 
        * 例如,程序中有一条指令是从地址 1000 加载到寄存器``movl 1000,%eax``,当整个程序的地址空间被加载到从 3000(不是程序认为的 0)开始的物理地址中,加载程序会重写指令中的地址(``movl 4000, %eax``),从而完成简单的静态重定位. 
        * 然而,静态重定位有许多问题,首先也是最重要的是*不提供访问保护*,进程中的错误地址可能导致对其他进程或操作系统内存的非法访问,一般来说,需要硬件支持来实现真正的访问保护. 
          静态重定位的另一个缺点是一旦完成,稍后很难将内存空间重定位到其他位置(_无法支持运行时的换入换出操作_).
        
    * 动态(基于硬件)重定位
        * 每个 CPU 需要两个硬件寄存器：**基址(base)寄存器**和**界限(bound)寄存器**,有时称为限制(limit)寄存器. 这组基址和界限寄存器,让我们能够将地址空间放在物
          理内存的任何位置,同时又能确保进程只能访问自己的地址空间. 采用这种方式,在编写和编译程序时假设地址空间从零开始. 但是,当程序真正执行时,操作系统会决定其在物
          理内存中的实际加载地址,并将起始地址记录在基址寄存器中.当进程运行时,该进程产生的所有内存引用,都会被处理器通过以下方式转换为物理地址：
          ```physical address = virtual address + base``` 

* **进程中使用的内存引用都是虚拟地址(virtual address)**,硬件接下来将虚拟地址加上基址寄存器中的内容,得到物理地址(physical address),再发给内存系统.为了更好地理解,
  让我们追踪一条指令执行的情况. 具体来看前面序列中的一条指令：``128: movl 0x0(%ebx), %eax``程序计数器(PC)首先被设置为 128. 当硬件需要获取这条指令时,它先将这个值加上基
  址寄存器中的值,得到实际的物理地址,然后硬件从这个物理地址获取指令.接下来,处理器开始执行该指令.

* 基于硬件的动态重定位: 在动态重定位的过程中,只有很少的硬件参与,但获得了很好的效果. 一个基址寄存器将虚拟地址转换为物理地址,一个界限寄存器确
  保这个地址在进程地址空间的范围内. 它们一起提供了既简单又高效的虚拟内存机制. 

* **界限寄存器提供了访问保护**. 如果进程需要访问超过这个界限或者为负数的虚拟地址,CPU 将触发异常,进程最终可能被终止. 界限寄存器的用处在于,它确保了进程产生的所有地址都在进程的地址"界限"中. 这种基址寄存器
  配合界限寄存器的硬件结构是芯片中的(每个 CPU 一对). 有时我们将CPU 的这个负责地址转换的部分统称为内存管理单元(Memory Management Unit,MMU).随着我们开发更复杂的内存管理技术,MMU也将有更复杂的电路和功能. 
  
    * 在一种方式中,它**记录地址空间的大小,硬件在将虚拟地址与基址寄存器内容求和前,就检查这个界限.** 
    * 另一种方式是界限寄存器中**记录地址空间结束的物理地址,硬件在转化虚拟地址到物理地址之后才去检查这个界限.** 这两种方式在逻辑上是等价的.
  
* 数据结构
    * 操作系统必须记录哪些空闲内存没有使用,以便能够为进程分配内存. 很多不同的数据结构可以用于这项任务,其中最简单的(也是我们假定在这里采用的)是空闲列表(free list). 它就是一个列表,记录当前没有使用的物理内存的范围. 

* 动态重定位：硬件要求

    | 操作                             | 解释                                                 |
    | -------------------------------- | ---------------------------------------------------- |
    | 特权模式                         | 以防用户模式的进程执行特权操作                       |
    | 基址/界限寄存器                  | 每个 CPU 需要一对寄存器来支持地址转换和界限检查      |
    | 能够转换虚拟地址并检查它是否越界 | 电路来完成转换和检查界限,在这种情况下,非常简单       |
    | 修改基址/界限寄存器的特权指令    | 在让用户程序运行之前,操作系统必须能够设置这些值      |
    | 注册异常处理程序的特权指令       | 操作系统必须能告诉硬件,如果异常发生,那么执行哪些代码 |
    | 能够触发异常                     | 如果进程试图使用特权指令或越界的内存                 |

* 操作系统的问题
  * 第一,在**进程创建时,操作系统必须采取行动,为进程的地址空间找到内存空间**. (我们假设每个进程的地址空间小于物理内存的大小,并且大小相同)它可以把整个物理内存看作一组槽块,标记了空闲或已用. 当新进程创建时,操作系统检索这个数据结构(常被称为空闲列表,free list),为新地址空间找到位置,并将其标记为已用. 
  * 第二, **在进程终止时,操作系统会将这些内存放回到空闲列表,并根据需要清除相关的数据结构**. 
  * 第三,**在上下文切换时,操作系统必须保存和恢复基础和界限寄存器**. 具体来说,当操作系统决定中止当前的运行进程时,它必须将当前基址和界限寄存器中的内容保存在内存中,放在某种每个进程都有的结构中,如进程控制块(Process Control Block,PCB)中. 类似地,当操作系统恢复执行某个进程时(或第一次执行),也必须给基址和界限寄存器设置正确的值. 
  * 第四,**操作系统必须提供异常处理程序,或要一些调用的函数.** 操作系统在启动时加载这些处理程序(通过特权命令). 例如,当一个进程试图越界访问内存时,CPU 会触发异常. 在这种异常产生时,操作系统必须准备采取行动. 通常操作系统会做出充满敌意的反应：终止错误进程. **操作系统应该尽力保护它运行的机器**,因此它不会对那些企图访问非法地址或执行非法指令的进程客气.

* 由于该进程的栈区和堆区可能并不很大,导致这些内存区域中大量的空间被浪费. 这种浪费通常称为**内部碎片(internal fragmentation),指的是已经分配的内存单元内部有未使用的空间(即碎片),造成了浪费**. 

#### 分段

* 将所有进程的地址空间完整地加载到内存中. 利用基址和界限寄存器,操作系统很容易将不同进程重定位到不同的物理内存区域
  * 如果我们将整个地址空间放入物理内存,那么栈和堆之间的空间并没有被进程使用,却依然占用了实际的物理内存.
  * 程序本身由若干部分(段)组成，每个段有各自的特点、用途：代码段只读，代码/数据段不会动态增长...

* 为了解决这个问题,分段(segmentation)的概念应运而生. 这个想法很简单,在 MMU中引入不止一个基址和界限寄存器对,而是给地址空间内的每个逻辑段一对. 一个段只是地址空间里的一个连续定长的区域,在典型的地址空间里有几个逻辑不同的段：代码、数据、栈和堆.  分段的机制使得操作系统能够将不同的段放到不同的物理内存区域,从而避免了虚拟地址空间中的未使用部分占用物理内存. 
* 段错误: 段错误指的是在支持分段的机器上发生了非法的内存访问. 硬件会发现该地址越界,因此陷入操作系统,很可能导致终止出错进程. 

* 硬件在地址转换时使用段寄存器. 它如何知道段内的偏移量,以及地址引用了哪个段?
  * 显式(explicit)方式,就是用虚拟地址的开头几位来标识不同的段. 
  * 隐式(implicit)方式中,硬件通过地址产生的方式来确定段. 例如,如果地址由程序计数器产生(即它是指令获取),那么地址
    在代码段. 如果基于栈或基址指针,它一定在栈段. 其他地址则在堆段.  

* 栈怎么办
  * 栈有一点关键区别,它反向增长. 首先,我们需要一点硬件支持. 除了基址和界限外,硬件还需要知道段的增长方向(用一位区分,比如 1 代表自小而大增长,0 反之).
  * 硬件理解段可以反向增长后,这种虚拟地址的地址转换必须有点不同: 为了得到正确的反向偏移,我们必须从虚拟地址的偏移量中减去最大的段地址.只要用这个反向偏移量加上基址,就得到了正确的物理地址. 用户可以进行界限检查,确保反向偏移量的绝对值小于段的大小. 

* 保护位
  * 基本为每个段增加了几个位,标识程序是否能够读写该段,或执行其中的代码. 通过将代码段标记为只读,同样的代码可以被多个进程共享,而不用担心破坏隔离. 虽然每个进程都认为自己独占这块内存,但操作系统秘密地共享了内存,进程不能修改这些内存,所以假象得以保持. 
  * 代码段的权限是可读和可执行,因此物理内存中的一个段可以映射到多个虚拟地址空间. 有了保护位,前面描述的硬件算法也必须改变. 除了检查虚拟地址是否越界,硬件还需要检查特定访问是否允许. 如果用户进程试图写入只读段,或从非执行段执行指令,硬件会触发异常,让操作系统来处理出错进程. 

* 分段的基本原理. 系统运行时,地址空间中的不同段被重定位到物理内存中. 与我们之前介绍的整个地址空间只有一个基址/界限寄存器对的方式相比,大量节省了物理内存. 具体来说,栈和堆之间没有使用的区域就不需要再分配物理内存,让我们能将更多地址空间放进物理内存. 程序不同段也可以提供不同的保护位.

* 分段的问题

  * 操作系统在上下文切换时应该做什么?你可能已经猜到了：各个段寄存器中的内容必须保存和恢复. 显然,每个进程都有自己独立的虚拟地址空间,操作系统必须在进程运行前,确保这些寄存器被正确地赋值. 
  * 管理物理内存的空闲空间. 新的地址空间被创建时,操作系统需要在物理内存中为它的段找到空间.  每个进程都有一些段,每个段的大小也可能不同. 一般会遇到的问题是,物理内存很快充满了许多空闲空间的小洞,因而很难分配给新的段,或扩大已有的段. 这种问题被称为外部碎片(external fragmentation)
    * 该问题的一种解决方案是紧凑(compact)物理内存,重新安排原有的段. 例如,操作统先终止运行的进程,将它们的数据复制到连续的内存区域中去,改变它们的段寄存器中的值,指向新的物理地址,从而得到了足够大的连续空闲空间. 这样做,操作系统能让新的内存分配请求成功. 但是,内存紧凑成本很高,因为拷贝段是内存密集型的,一般会占用大量的处理器时间. 

* 如果需要管理的空间被划分为固定大小的单元,就很容易. 在这种情况下,只需要维护这些大小固定的单元的列表,如果有请求,就返回列表中的第一项. 如果要管理的空闲空间由大小不同的单元构成,管理就变得困难(而且有趣). 这种情况出现在用户级的内存分配库(如 malloc()和 free()),或者操作系统用分段(segmentation)的方式实现虚拟内存. 在这两种情况下,出现了外部碎片(external fragmentation)的问题：空闲空间被分割成不同大小的小块,成为碎片,后续的请求可能失败,因为没有一块足够大的连续空闲空间,即使这时总的空闲空间超出了请求的大小. 

* 该库管理的空间由于历史原因被称为堆,在堆上管理空闲空间的数据结构通常称为空闲列表(free list). 该结构包含了管理内存区域中所有空闲块的引用. 当然,该数据结构不一定真的是列表,而只是某种可以追踪空闲空间的数据结构. 

* 追踪已分配空间的大小

* 你可能注意到, ``free(void *ptr)``接口没有块大小的参数. 因此它是假定,对于给定的指针,内存分配库可以很快确定要释放空间的大小,从而将它放回空闲列表. 要完成这个任务,大多数分配程序都会在头块(header)中保存一点额外的信息,它在内存中,通常就在返回的内存块之前.该头块中至少包含所分配空间的大小. 它也可能包含一些额外的指针来加速空间释放,包含一个幻数来提供完整性检查,以及其他信息. 我们假定,一个简单的头块包含了分配空间的大小和一个幻数：

  ```
  typedef struct header_t {
      int size;
      int magic;
  } header_t;
  ```

  用户调用 free(ptr)时,库会通过简单的指针运算得到头块的位置：

  ```
  void free(void *ptr) {
  	header_t *hptr = (void *)ptr - sizeof(header_t);
  } 
  ```

  [!image](./images/14.png)
  获得头块的指针后,库可以很容易地确定幻数是否符合预期的值,作为正常性检查,并简单计算要释放的空间大小(即头块的大小加区域长度). (因此,如果用户请求 N 字节的内存,库不是寻找大小为 N 的空闲块,而是寻找N 加上头块大小的空闲块. )

* 如何在空闲内存自己内部建立这样一个列表呢?

* 假设我们需要管理一个 4096 字节的内存块(即堆是 4KB). 为了将它作为一个空闲空间列表来管理,首先要初始化这个列表. 开始,列表中只有一个条目,记录了大小为4096的空间(减去头块的大小). 下面是该列表中一个节点描述：

  ```
  typedef struct node_t {
      int size; 
      struct node_t *next;
  } node_t; 
  ```

  

  现在来看一些代码,它们初始化堆,并将空闲列表的第一个元素放在该空间中. 假设堆构建在某块空闲空间上,这块空间通过系统调用 mmap()获得. 这不是构建这种堆的唯一选择,但在这个例子中很合适. 下面是代码：

  ```
  // mmap() returns a pointer to a chunk of free space
  node_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, -1, 0);
  head->size = 4096 - sizeof(node_t);
  head->next = NULL;
  ```

  执行这段代码之后,列表的状态是它只有一个条目,记录大小为 4088. 是的,这是一个小堆,但对我们是一个很好的例子. head 指针指向这块区域的起始地址,假设是 16KB(尽管任何虚拟地址都可以). 堆看起来如图 17.3 所示. 现在,假设有一个 100 字节的内存请求. 为了满足这个请求,库首先要找到一个足够大小的块. 因为只有一个 4088 字节的块,所以选中这个块. 然后,这个块被分割(split)为两块：一块足够满足请求(以及头块,如前所述),一块是剩余的空闲块. 假设记录头块为 8 个字节(一个整数记录大小,一个整数记录幻数),堆中的空间如图 17.4 所示. 
  [!image](./images/15.png)
  至此,对于 100 字节的请求,库从原有的一个空闲块中分配了 108 字节,返回指向它的一个指针(在上图中用 ptr 表示),并在其之前连续的 8 字节中记录头块信息,供未来的free()函数使用. 同时将列表中的空闲节点缩小为 3980 字节(4088−108). 现在再来看该堆,其中有 3 个已分配区域,每个 100(加上头块是 108). 这个堆如图 17.5所示. 可以看出,堆的前 324 字节已经分配,因此我们看到该空间中有 3 个头块,以及 3 个 100字节的用户使用空间. 空闲列表还是无趣：只有一个节点(由 head 指向),但在 3 次分割后,现在大小只有 3764 字节. 但如果用户程序通过 free()归还一些内存,会发生什么?在这个例子中,应用程序调用 free(16500),归还了中间的一块已分配空间(内存块的起始地址 16384 加上前一块的 108,和这一块的头块的 8 字节,就得到了 16500). 这个值在前图中用 sptr 指向. 库马上弄清楚了这块要释放空间的大小,并将空闲块加回空闲列表. 假设我们将它插入到空闲列表的头位置,该空间如图 17.6 所示. 
  [!image](./images/16.png)
  现在的空闲列表包括一个小空闲块(100 字节,由列表的头指向)和一个大空闲块(3764字节). 现在假设剩余的两块已分配的空间也被释放. 没有合并,空闲列表将非常破碎,如图 17.7 所示. 从图中可以看出,我们现在一团糟! 为什么?简单,我们忘了合并(coalesce)列表项,虽然整个内存空间是空闲的,但却被分成了小段,因此形成了碎片化的内存空间. 解决方案很简单：遍历列表,合并(merge)相邻块. 完成之后,堆又成了一个整体. 
  [!image](./images/17.png)

* 让堆增长

  * 具体来说,如果堆中的内存空间耗尽,应该怎么办?最简单的方式就是返回失败. 在某些情况下这也是唯一的选择. 大多数传统的分配程序会从很小的堆开始,当空间耗尽时,再向操作系统申请更大的空间. 通常,这意味着它们进行了某种系统调用(例如,大多数 UNIX 系统中的 sbrk),让堆增长. 操作系统在执行 sbrk 系统调用时,会找到空闲的物理内存页,将它们映射到请求进程的地址空间中去,并返回新的堆的末尾地址. 这时,就有了更大的堆,请求就可以成功满足. 

#### 分配策略 

* **最优匹配(best fit)**：首先遍历整个空闲列表,找到和请求大小一样或更大的空闲块,然后返回这组候选者中最小的一块. 这就是所谓的最优匹配(也可以称为最
  小匹配). 只需要遍历一次空闲列表,就足以找到正确的块并返回. 最优匹配背后的想法很简单：**选择最接它用户请求大小的块,从而尽量避免空间浪费. 然而,这有代价. 简单的实现在遍历查找正确的空闲块时,要付出较高的性能代价. 而且会剩下很多难以利用的小块.**
* **最差匹配(worst fit)**方法与最优匹配相反,它尝试找最大的空闲块,分割并满足用户需求后,将剩余的块(很大)加入空闲列表. **最差匹配尝试在空闲列表中保留大小均匀的块. 然而, 最差匹配会导致出现内存中不存在较大空闲块的问题.**
* **首次匹配(first fit)**策略就是找到第一个足够大的块,将请求的空间返回给用户. 同样,剩余的空闲空间留给后续请求. 首次匹配有速度优势(不需要遍历所有空块),但**有时会让空闲列表开头的部分有很多小块**. 因此,分配程序如何管理空闲列表的顺序就变得很重要. 一种方式是基于地址排序(address-based ordering). 通过保持空闲块按内存地址有序,合并操作会很容易,从而减少了内存碎片. 
* **下次匹配(next fit)**算法多维护一个指针,指向上一次查找结束的位置. 其想法是**将对空闲空间的查找操作扩散到整个列表中去,避免对列表开头频繁的分割**. 这种策略的性能与首次匹配很接它,同样避免了遍历查找. 
* **分离空闲列表(segregated list).** 基本想法很简单：如果某个应用程序经常申请一种(或几种)大小的内存空间,那就用一个独立的列表,只管理这样大小的对象. 其他大小的请求都一给更通用的内存分配程序. 

* 伙伴系统
  * 因为合并对分配程序很关键,所以人们设计了一些方法,让合并变得简单,一个好例子就是二分伙伴分配程序(binary buddy allocator). 
  * 在这种系统中,空闲空间首先从概念上被看成大小为``2^N``的大空间. 当有一个内存分配请求时,空闲空间被递归地一分为二,直到刚好可以满足请求的大小(再一分为二就无法满足). 这时,请求的块被返回给用户. 在下面的例子中,一个 64KB 大小的空闲空间被切分,以便提供 7KB 的块：
    [!image](./images/18.png)
    在这个例子中,最左边的8KB块被分配给用户(如上图中深灰色部分所示). 请注意,这种分配策略只允许分配 2 的整数次幂大小的空闲块,因此会有内部碎片(internal fragment)的麻烦. 
  * 伙伴系统的漂亮之处在于块被释放时. 如果将这个 8KB 的块归还给空闲列表,分配程序会检查"伙伴"8KB 是否空闲. 如果是,就合二为一,变成 16KB 的块. 然后会检查这个 16KB 块的伙伴是否空闲,如果是,就合并这两块. 这个递归合并过程继续上溯,直到合并整个内存区域,或者某一个块的伙伴还没有被释放. 
  * 伙伴系统运转良好的原因,在于很容易确定某个块的伙伴. 每对互为伙伴的块只有一位不同,正是这一位决定了它们在整个伙伴树中的层次. 

* 将空间分割成固定长度的分片. 在虚拟内存中,我们称这种思想为分页. 分页不是将一个进程的地址空间分割成几个不同长度的逻辑段(即代码、堆、段),而是分割成固定大小的单元,每个单元称为一页. 相应地,我们把物理内存看成是定长槽块的阵列,叫作页帧(page frame). 每个这样的页帧包含一个虚拟内存页.

* 为了**记录地址空间的每个虚拟页放在物理内存中的位置**,操作系统通常为每个进程保存一个数据结构,称为**页表(page table)**. 页表的主要作用是**为地址空间的每个虚拟页面保存地址转换(address translation)**,从而让我们知道每个页在物理内存中的位置. 

* 重要的是要记住,这个页表是一个每进程的数据结构(我们讨论的大多数页表结构都是每进程的数据结构

* 页表可以变得非常大,比我们之前讨论过的小段表或基址/界限对要大得多. 例如,想象一个典型的 32 位地址空间,带有 4KB 的页. 这个虚拟地址分成 20 位的 VPN和 12 位的偏移量. 一个 20 位的 VPN 意味着, 操作系统必须为每个进程管理 2^20 个地址转换. 假设每个页表格条目(PTE)需要 4 个字节,来保存物理地址转换和任何其他有用的东西,每个页表就需要巨大的 4MB 内存!这非常大. 现在想象一下有 100 个进程在运行：这意味着操作系统会需要 400MB 内存...

* **页表就是一种数据结构,用于将虚拟地址(或者实际上,是虚拟页号)映射到物理地址(物理帧号)**. 因此,任何数据结构都可以采用. 最简单的形式称为**线性页表(linear page table)**,就是一个数组. 操作系统通过虚拟页号(VPN)检索该数组,并在该索引处查找页表项(PTE),以便找到期望的物理帧号(PFN).

* 至于每个 PTE 的内容,我们在其中有许多不同的位,值得有所了解. 

  * **有效位(valid bit)**: 通常用于指示特定地址转换是否有效. 例如,当一个程序开始运行时,它的代码和堆在其地址空间的一端,栈在另一端. 所有未使用的中间空间都将被标记为无效(invalid),如果进程尝试访问这种内存,就会陷入操作系统,可能会导致该进程终止. 因此,有效位对于支持稀疏地址空间至关重要. 通过简单地将地址空间中所有未使用的页面标记为无效,我们不再需要为这些页面分配物理帧,从而节省大量内存. 
  * **保护位(protection bit)**,表明页是否可以读取、写入或执行. 同样,以这些位不允许的方式访问页,会陷入操作系统.
  * **存在位(present bit)**表示该页是在物理存储器还是在磁盘上(即它已被换出,swapped out). 交换允许操作系统将很少使用的页面移到磁盘,从而释放物理内存. 
  * **脏位(dirty bit)**也很常见,表明页面被带入内存后是否被修改过. 
  * **参考位(reference bit)**,也被称为访问位, 有时用于追踪页是否被访问,也用于确定哪些页很受欢迎,因此应该保留在内存中. 

*  18.5 显示了来自 x86 架构的示例页表项. 它包含一个存在位(P),确定是否允许写入该页面的读/写位(R/W) 确定用户模式进程是否可以访问该页面的用户/超级用户位(U/S),有几位(PWT、PCD、PAT 和 G)确定硬件缓存如何为这些页面工作,一个访问位(A)和一个脏位(D),最后是页帧号(PFN)本身. 
  [!image](./images/23.png)
  
* 内存中的页表,我们已经知道它们可能太大了. 事实证明,它们也会让速度变慢. 以简单的指令为例：``movl addr, %eax`` 同样,我们只看对地址 addr 的显式引用,而不关心指令获取. 在这个例子中,我们假定硬件为我们执行地址转换. 要获取所需数据,系统必须首先将虚拟地址addr转换为正确的物理地址. 因此,在从正确的物理地址获取数据之前,系统必须首先从进程的页表中提取适当的页表项,执行转换,然后从物理内存中加载数据. 为此,硬件必须知道当前正在运行的进程的页表的位置. 现在让我们假设一个**页表基址寄存器(page-table base register)**包含页表的起始位置的物理地址. 为了找到想要的 PTE的位置,硬件将执行以下功能：
  
  ```
  VPN = (VirtualAddress & VPN_MASK) >> SHIFT;	// VPN_MASK 从完整的虚拟地址中挑选出 VPN 位; SHIFT 可以将 VPN 位向右移动以形成正确的整数虚拟页码
  PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE));	// 使用VPN作为页表基址寄存器指向的 PTE 数组的索引
  ```
  
  一旦知道了这个物理地址,硬件就可以从内存中获取 PTE,提取 PFN,并将它与来自虚拟地址的偏移量连接起来,形成所需的物理地址. 具体来说,你可以想象 PFN被 SHIFT左移,然后与偏移量进行逻辑或运算,以形成最终地址
  
  ```
  offset = VirtualAddress & OFFSET_MASK	// OFFSET_MASK 获取虚拟地址的页内偏移量
  PhysAddr = (PFN << SHIFT) | offset
  ```
  
  最后,硬件可以从内存中获取所需的数据并将其放入寄存器 eax. 程序现在已成功从内存中加载了一个值!

* 总之,我们现在描述了在每个内存引用上发生的情况的初始协议. **对于每个内存引用(无论是取指令还是显式加载或存储),分页都需要我们执行一个额外的内存引用,以便首先从页表中获取地址转换.** 额外的内存引用开销很大,在这种情况下,可能会使进程减慢两倍或更多. 现在你应该可以看到,有两个必须解决的实际问题. 如果不仔细设计硬件和软件,**页表会导致系统运行速度过慢,并占用太多内存.** 

#### TLB

* 我们要增加所谓的**地址转换旁路缓冲存储器(translation-lookaside buffer,TLB)**,它就是频繁发生的**虚拟到物理地址转换的硬件缓存(cache)**. 因此,更好的名称应该是地址转换缓存(address-translation cache). 对每次内存访问,硬件先检查 TLB,看看其中是否有期望的转换映射,如果有,就完成转换(很快),不用访问页表(其中有全部的转换映射). TLB 带来了巨大的性能提升,实际上,因此它使得虚拟内存成为可能. 

* TLB 的基本算法,说明硬件如何处理虚拟地址转换,假定使用简单的线性页表和硬件管理的 TLB

  ```
  VPN = (VirtualAddress & VPN_MASK) >> SHIFT			  // 从虚拟地址中提取页号
  (Success, TlbEntry) = TLB_Lookup(VPN)		    	  // 然后检查 TLB 是否有该 VPN 的转换映射
  if (Success == True) 								// TLB 命中
  	if (CanAccess(TlbEntry.ProtectBits) == True)	  // 保护检查
  		Offset = VirtualAddress & OFFSET_MASK
  		PhysAddr = (TlbEntry.PFN << SHIFT) | Offset	  // 从相关的 TLB 项中取出页帧号(PFN),与原来虚拟地址中的偏移量组合形成期望的物理地址(PA),并访问内存
  		AccessMemory(PhysAddr)
  	else
  		RaiseException(PROTECTION_FAULT)
  else 											   // 硬件访问页表来寻找转换映射,并用该转换映射更新 TLB
  	PTEAddr = PTBR + (VPN * sizeof(PTE))
  	PTE = AccessMemory(PTEAddr) 
  	if (PTE.Valid == False)
  		RaiseException(SEGMENTATION_FAULT)
  	else if (CanAccess(PTE.ProtectBits) == False)
  		RaiseException(PROTECTION_FAULT)
  	else
  		TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
  RetryInstruction()
  ```

* 类似其他缓存,TLB 依赖于空间和时间局部性. 如果某个程序表现出这样的局部性(许多程序是这样),TLB 的命中率可能很高. 
  * **时间局部性(temporal locality)**: 最近访问过的指令或数据项可能很快会再次访问. 想想循环中的循环变量或指令,它们被多次反复访问. 
  * **空间局部性(spatial locality)**: 当程序访问内存地址 x 时,可能很快会访问邻近 x 的内存. 想想遍历某种数组,访问一个接一个的元素. 
  * 当然,这些性质取决于程序的特点,并不是绝对的定律,而更像是一种经验法则. 硬件缓存,无论是指令、数据还是地址转换(如 TLB),都利用了局部性,在小而快的芯片内存储器中保存一份内存副本. 处理器可以先检查缓存中是否存在就近的副本,而不是必须访问(缓慢的)内存来满足请求. 如果存在,处理器就可以很快地访问它(例如在几个 CPU 时钟内),避免花很多时间来访问内存(好多纳秒). 
* 谁来处理 TLB 未命中
  * **硬件管理 TLB**: 以前的硬件有复杂的指令集(有时称为复杂指令集计算机),造硬件的人不太相信那些搞操作系统的人. 因此,硬件全权处理 TLB未命中. 为了做到这一点,硬件必须知道页表在内存中的确切位置(通过页表基址寄存器),以及页表的确切格式. 发生未命中时,硬件会"遍历"页表,找到正确的页表项,取出想要的转换映射,用它更新 TLB,并重试该指令. 这种"旧"体系结构有硬件管理的 TLB,
  * **软件管理 TLB**: 发生 TLB 未命中时,硬件系统会抛出一个异常,这会暂停当前的指令流,将特权级提升至内核模式,跳转至陷阱处理程序. 接下来这个陷阱处理程序是操作系统的一段代码,用于处理 TLB 未命中. 这段代码在运行时,会查找页表中的转换映射,然后用特别的"特权"指令更新 TLB,并从陷阱返回. 此时,硬件会重试该指令(导致 TLB 命中). 
    * 这里的从陷阱返回指令稍稍不同于之前提到的服务于系统调用的从陷阱返回. 在后一种情况下,从陷阱返回应该继续执行陷入操作系统之后那条指令,就像从函数调用返回后,会继续执行此次调用之后的语句. 在前一种情况下,在从 TLB 未命中的陷阱返回后,硬件必须从导致陷阱的指令继续执行. 这次重试因此导致该指令再次执行,但这次会命中 TLB. 因此,**根据陷阱或异常的原因,系统在陷入内核时必须保存不同的程序计数器,以便将来能够正确地继续执行.** 
    * 在运行 TLB 未命中处理代码时,**操作系统需要格外小心避免引起 TLB 未命中的无限递归**. 

* TLB 替换策略
  * **替换最近最少使用(least-recently-used,LRU)的项**. LRU尝试利用内存引用流中的局部性,假定最近没有用过的项,可能是好的换出候选项. 
  * **随机(random)策略**,即随机选择一项换出去. 这种策略很简单,并且可以避免一种极端情况. 例如,一个程序循环访问 n+1 个页,但 TLB 大小只能存放 n 个页. 这时之前看似"合理"的 LRU 策略就会表现得不可理喻,因为每次访问内存都会触发 TLB未命中,而随机策略在这种情况下就好很多. 

* TLB 也不能满足所有的程序需求. 具体来说,如果一个程序短时间内访问的页数超过了 TLB 中的页数,就会产生大量的 TLB 未命中,运行速度就会变慢. 这种现象被称为超出**TLB 覆盖范围(TLB coverage)**,这对某些程序可能是相当严重的问题. 解决这个问题的一种方案是**支持更大的页,把关键数据结构放在程序地址空间的某些区域,这些区域被映射到更大的页,使 TLB 的有效覆盖率增加**. 对更大页的支持通常被数据库管理系统(Database Management System,DBMS)这样的程序利用,它们的数据结构比较大,而且是随机访问. 

#### 多级页表

* 我们现在来解决分页引入的第二个问题：页表太大,因此消耗的内存太多
* **减小页表大小：使用更大的页**: 以 32 位地址空间为例,但这次假设用 16KB 的页. 因此,会有 18 位的 VPN 加上 14 位的偏移量. 假设每个页表项(4字节)的大小相同,现在线性页表中有 218 个项,因此每个页表的总大小为 1MB,页表缩到四分之一. 
  * 这种方法的主要问题在于,大内存页会导致每页内的浪费,这被称为**内部碎片问题**(因为浪费在分配单元内部). 因此,结果是应用程序会分配页,但只用每页的一小部分,而内存很快就会充满这些过大的页. 因此,大多数系统在常见的情况下使用相对较小的页大小：4KB(如 x86)或 8KB(如 SPARCv9). 
* **去掉页表中的所有无效区域**,而不是将它们全部保留在内存中
  * 去掉无效区域之后, 但页表中的页号不连续，就需要比较、查找，折半等查找策略. 假设使用二分查找 `log_2(2^20) = 20`. 最坏情况需要20次内存访问才能定位到当前页所在位置.
* **多级页表**, 将线性页表变成了类似树的东西. 这种方法非常有效,许多现代系统都用它多级页表的基本思想很简单. 首先,将页表分成页大小的单元. 然后,如果整页的页表项(PTE)无效,就完全不分配该页的页表. 为了追踪页表的页是否有效(以及如果有效,它在内存中的位置),使用了名为页目录(page directory)的新结构. 页目录因此可以告诉你页表的页在哪里,或者页表的整个页不包含有效页. 

* 以32位寻址系统为例

  * 我们将虚拟地址的前10位用于页目录(2^10), 低12位用于页内偏移(4K),其余用于只想页目录下的页表项偏移. 故映射2^32的物理地址空间时,只需要加载2^10大小的页目录再内存中(若每项大小位4,则需要4K). 依据虚拟地址的页目录偏移找到对应目录, 再依据虚拟地址的页表项偏移获取该目录下的对应页表项. 

    [[!image](./images/41.png)

* 理解时空折中
  * 在构建数据结构时,应始终考虑时间和空间的折中(time-space trade-off). 通常,如果你希望更快地访问特定的数据结构,就必须为该结构付出空间的代价. 
  * 应该指出,多级页表是有成本的. 在 TLB 未命中时,需要从内存加载多次,才能从页表中获取正确的地址转换信息(用于页目录和PTE 本身),而用线性页表只需要一次加载. 因此,多级表是一个时间—空间折中(time-space trade-off)的例子. 我们想要更小的表(并得到了),但不是没代价. 尽管在常见情况下(TLB 命中),性能显然是相同的,但 TLB 未命中时,则会因较小的表而导致较高的成本. 另一个明显的缺点是复杂性. 无论是硬件还是操作系统来处理页表查找,这样做无疑都比简单的线性页表查找更复杂. 通常我们愿意增加复杂性以提高性能或降低管理费用. 在多级表的情况下,为了节省宝贵的内存,我们使页表查找更加复杂. 


#### 内存操作API

* 关键问题：在 UNIX/C 程序中,如何分配和管理内存? 通常使用哪些接口?哪些错误需要避免?

* 内存类型 
    * 在运行一个 C 程序的时候,会分配两种类型的内存. 第一种称为栈内存,它的申请和释放操作是编译器来隐式管理的,所以有时也称为自动(automatic)内存. 函数中申请的内存都是栈内存
      编译器确保在你进入函数的时候,在栈上开辟空间. 当你从该函数退出时,编译器释放内存. 因此,如果你希望某些信息存在于函数调用之外,建议不要将它们放在栈上. 
    * 堆(heap)内存,其中所有的申请和释放操作都由程序员显式地完成. 下面的例子展示了如何在堆上分配一个整数,得到指向它的指针：
      [!image](./images/38.png)
      关于这一小段代码有两点说明. 首先,你可能会注意到栈和堆的分配都发生在这一行： 首先编译器看到指针的声明(int *p)时,知道为一个整型指针分配空间,随后,当程序调
      用 malloc()时,它会在堆上请求整数的空间,函数返回这样一个整数的地址(成功时,失败时则返回 NULL),然后将其存储在栈中以供程序使用. 
    
* malloc()调用
    * malloc 函数非常简单：传入要申请的堆空间的大小,它成功就返回一个指向新申请空间的指针,失败就返回 NULL

* free()调用 
  事实证明,分配内存是等式的简单部分. 知道何时、如何以及是否释放内存是困难的部分. 要释放不再使用的堆内存,程序员只需调用 free()：
  ```
    int *x = malloc(10 * sizeof(int)); 
    free(x); 
  ```
  该函数接受一个参数,即一个由 malloc()返回的指针. 因此,你可能会注意到,**分配区域的大小不会被用户传入,必须由内存分配库本身记录追踪**. 

* 其他调用: 内存分配库还支持一些其他调用. 例如,calloc()分配内存,并在返回之前将其置零. 如果你但为内存已归零并忘记自己初始化它,这可以防止出现一些错误. 当你为某些东西(比如一个数组)分配空间,然后需
  要添加一些东西时,例程 realloc()也会很有用：realloc()创建一个新的本大的内存区域,将旧区域复制到其中,并返回新区域的指针. 

* 许多新语言都支持自动内存管理(automatic memory management). 在这样的语言中,当你调用类似 malloc()的机制来分配内存时,你永远不需要调用某些东西来释放空间. 实实上,
  垃圾收集器(garbage collector)会运行,找出你不再引用的内存,替你释放它. 
  
* 常见错误
    * **忘记分配内存**: 许多例程在调用之前,都希望你为它们分配内存. 例如,例程 strcpy(dst, src)将源字符串中的字符串复制到目标指针. 但是,如果不小心,你可能会这样做：
      [!image](./images/39.png)
      运行这段代码时,可能会导致段错误(segmentation fault). 

    * **它编译过了或它运行了!=它对了**: 仅仅因为程序编译过了甚至正确运行了一次或多次,并不意味着程序是正确的. 许多事件可能会让你相信它能工作,但是之后有些事情会发生变化,它停止了. 
    * **没有分配足够的内存**: 另一个相关的错误是没有分配足够的内存,有时称为缓冲区溢出(buffer overflow). 在
      [!image](./images/40.png)
      奇怪的是,这个程序通常看起来会正确运行,这取决于如何实现 malloc 和许多其他细节. 在某些情况下,当字符串拷贝执行时,它会在超过分配空间的末尾处写入一个字我,但在某些情况下,
      这是无害的,可能会覆盖不再使用的变量. 在某些情况下,这些溢出可能具有令人难以置信的危害,实实上是系统中许多安全漏洞的来源. 在其他情况下,malloc 库总是分配一些额外的空间,
      因此你的程序实实上不会在其他某个变量的值上涂写,并且工作得很好. 还有一些情况下,该程序确实会发生故障和崩溃. 因此,我们学到了另一个宝贵的教训：**即使它正确运行过一次,也不意味着它是正确的**. 

    * **忘记初始化分配的内存**: 在这个错误中,你正确地调用 malloc(),但忘记在新分配的数据类型中填写一些值. 不要这样做!如果你忘记了,你的程序最终会遇到未初始化的读取(uninitialized read),它从
      堆中读取了一些未知值的数据. 谁知道那里可能会有什么?如果走运,读到的值使程序仍然有效(例如,零). 如果不走运,会读到一些随机和有害的东西. 

    * **忘记释放内存**: 另一个常见错误称为内存泄露(memory leak),如果忘记释放内存,就会发生. 在长时间运行的应用程序或系统(如操作系统本身)中,这是一个巨大的问题,因为缓慢泄露的
      内存会导致内存不足,此时需要重新启动. 因此,一般来说,当你用完一段内存时,应该确保释放它. 在某些情况下,不调用 free()似乎是合理的. 例如,你的程序运行时间很短,很但就会退出. 在这种情况下,
      当进程死亡时,操作系统将清理其分配的所有页面,因此不会发生内存泄露. 虽然这肯定"有效",但这可能是一个坏习惯,所以请谨慎选择这样的策略.
    
    * **在用完之前释放内存**: 有时候程序会在用完之前释放内存,这种错误称为*悬挂指针(dangling pointer)*. 随后的使用可能会导致程序崩溃或覆盖有效的内存. 
    
    * **反复释放内存**: 程序有时还会不止一次地释放内存,这被称为重复释放(double free). 这样做的结果是未定义的. 正如你所能想象的那样,内存分配库可能会感到困惑,并且会做各种奇怪的事情,崩溃是常见的结果. 
    
    * **错误地调用 free()**: free()期望你只传入之前从 malloc()得到的一个指针. 如果传入一些其他的值,坏事就可能发生(并且会发生). 因此,这种无效的释放(invalid free)是危险的,当然也应该避免. 

* 为什么在你的进程退出时没有内存泄露
    * 当你编写一个短时间运行的程序时,可能会使用 malloc()分配一些空间. 程序运行并即将完成：是否需要在退出前调用几次 free()?虽然不释放似乎不对,但在真正的意义上,没有任何内存会"丢失". 
    * 原因很简单：系统中实际存在两级内存管理. 
        * **第一级是由操作系统执行的内存管理**,操作系统在进程运行时将内存交给进程,并在进程退出(或以其他方式结束)时将其回收. 
        * **第二级管理在每个进程中**,例如在调用 malloc()和 free()时,在堆内管理. 即使你没有调用 free()(并因此泄露了堆中的内存),操作系统也会在程序结束运行时,收回进程的所
          有内存(包括用于代码、栈,以及相关堆的内存页). 无论地址空间中堆的状态如何,操作系统都会在进程终止时收回所有这些页面,从而确保即使没有释放内存,也不会丢失内存. 
    * 因此,对于短时间运行的程序,泄露内存通常不会导致任何操作问题(尽管它可能被认为是不好的形式). 如果你编写一个长期运行的服务器(例如 Web 服务器或数据库管理系统,它永远不会退出),泄露内存就是很大的问题,最终会导致应用程序在内存不足时崩溃. 

#### 交换

* 不仅是一个进程，增加交换空间让操作系统为多个并发运行的进程都提供巨大地址空间
  的假象。多道程序（能够“同时”运行多个程序，更好地利用机器资源）的出现，强烈要
  求能够换出一些页，因为早期的机器显然不能将所有进程需要的所有页同时放在内存中。
  因此，多道程序和易用性都需要操作系统支持比物理内存更大的地址空间。这是所有现代
  虚拟内存系统都会做的事情

##### 交换空间 

* 在硬盘上开辟一部分空间用于物理页的移入和移出。在
  操作系统中，一般这样的空间称为交换空间（swap space），因为我们将内存中的页交换到
  其中，并在需要的时候又交换回去。因此，我们会假设操作系统能够以页大小为单元读取
  或者写入交换空间。为了达到这个目的，操作系统需要记住给定页的硬盘地址（disk address）。
* 交换空间的大小是非常重要的，它决定了系统在某一时刻能够使用的最大内存页数。
  简单起见，现在假设它非常大。
  
  
##### 存在位 
现在我们在硬盘上有一些空间，需要在系统中增加一些更高级的机制，来支持从硬盘
交换页。简单起见，假设有一个硬件管理 TLB 的系统。
先回想一下内存引用发生了什么。正在运行的进程生成虚拟内存引用（用于获取指令
或访问数据），在这种情况下，硬件将其转换为物理地址，再从内存中获取所需数据。
硬件首先从虚拟地址获得 VPN，检查 TLB 是否匹配（TLB 命中），如果命中，则获得最
终的物理地址并从内存中取回。这希望是常见情形，因为它很快（不需要额外的内存访问）。
如果在 TLB 中找不到 VPN（即 TLB 未命中），则硬件在内存中查找页表（使用页表基
址寄存器），并使用 VPN 查找该页的页表项（PTE）作为索引。如果页有效且存在于物理内
存中，则硬件从 PTE 中获得 PFN，将其插入 TLB，并重试该指令，这次产生 TLB 命中。到
现在为止还挺好。
但是，如果希望允许页交换到硬盘，必须添加更多的机制。具体来说，当硬件在 PTE
中查找时，可能发现页不在物理内存中。硬件（或操作系统，在软件管理 TLB 时）判断是
否在内存中的方法，是通过页表项中的一条新信息，即存在位（present bit）。如果存在位设
置为 1，则表示该页存在于物理内存中，并且所有内容都如上所述进行。如果存在位设置为
零，则页不在内存中，而在硬盘上。访问不在物理内存中的页，这种行为通常被称为页错
误（page fault）。
* 在页错误时，操作系统被唤起来处理页错误。一段称为“页错误处理程序（page-fault
  handler）”的代码会执行，来处理页错误
  
##### 页错误
  
* 在 TLB 未命中的情况下，我们有两种类型的系统：硬件管理的 TLB（硬件
  在页表中找到需要的转换映射）和软件管理的 TLB（操作系统执行查找过程）。不论在哪种
  系统中，如果页不存在，都由操作系统负责处理页错误。操作系统的页错误处理程序
  （page-fault handler）确定要做什么。几乎所有的系统都在软件中处理页错误。即使是硬件管
  理的 TLB，硬件也信任操作系统来管理这个重要的任务。
* 如果一个页不存在，它已被交换到硬盘，在处理页错误的时候，操作系统需要将该页
  交换到内存中。那么，问题来了：操作系统如何知道所需的页在哪儿？在许多系统中，页
  表是存储这些信息最自然的地方。因此，操作系统可以用 PTE 中的某些位来存储硬盘地址，
  这些位通常用来存储像页的 PFN 这样的数据。当操作系统接收到页错误时，它会在 PTE 中
  查找地址，并将请求发送到硬盘，将页读取到内存中。
  
* 当硬盘 I/O 完成时，操作系统会更新页表，将此页标记为存在，更新页表项（PTE）的
  PFN 字段以记录新获取页的内存位置，并重试指令。下一次重新访问 TLB 还是未命中，然
  而这次因为页在内存中，因此会将页表中的地址更新到 TLB 中（也可以在处理页错误时更
  新 TLB 以避免此步骤）。最后的重试操作会在 TLB 中找到转换映射，从已转换的内存物理
  地址，获取所需的数据或指令。

* 请注意，当 I/O 在运行时，进程将处于阻塞（blocked）状态。因此，当页错误正常处
  理时，操作系统可以自由地运行其他可执行的进程。因为 I/O 操作是昂贵的，一个进程进行
  I/O（页错误）时会执行另一个进程，这种交叠（overlap）是多道程序系统充分利用硬件的
  一种方式。

##### 内存满了怎么办 
  
*在上面描述的过程中，你可能会注意到，我们假设有足够的空闲内存来从存储交换空间
 换入（page in）的页。当然，情况可能并非如此。内存可能已满（或接近满了）。因此，操作
 系统可能希望先交换出（page out）一个或多个页，以便为操作系统即将交换入的新页留出空
 间。选择哪些页被交换出或被替换（replace）的过程，被称为页交换策略（page-replacement
 policy）。
 事实表明，人们在创建好页交换策略上投入了许多思考，因为换出不合适的页会导致
 程序性能上的巨大损失，也会导致程序以类似硬盘的速度运行而不是以类似内存的速度。
 在现有的技术条件下，这意味着程序可能会运行慢 10000～100000 倍。因此，这样的策略
 是我们应该详细研究的。实际上，这也正是我们下一章要做的。现在，我们只要知道有这
 样的策略存在，建立在之前描述的机制之上。
 
 
##### 交换何时真正发生
  
* 到目前为止，我们一直描述的是操作系统会等到内存已经完全满了以后才会执行交换
  流程，然后才替换（踢出）一个页为其他页腾出空间。正如你想象的那样，这有点不切实
  际的，因为操作系统可以更主动地预留一小部分空闲内存。
  为了保证有少量的空闲内存，大多数操作系统会设置高水位线（High Watermark，HW）
  和低水位线（Low Watermark，LW），来帮助决定何时从内存中清除页。原理是这样：当操
  作系统发现有少于 LW 个页可用时，后台负责释放内存的线程会开始运行，直到有 HW 个
  可用的物理页。这个后台线程有时称为交换守护进程（swap daemon）或页守护进程（page
  daemon）①，它然后会很开心地进入休眠状态，因为它毕竟为操作系统释放了一些内存。
  通过同时执行多个交换过程，我们可以进行一些性能优化。例如，许多系统会把多个
  要写入的页聚集（cluster）或分组（group），同时写入到交换区间，从而提高硬盘的效率[LL82]。
  我们稍后在讨论硬盘时将会看到，这种合并操作减少了硬盘的寻道和旋转开销，从而显著
  提高了性能。
  为了配合后台的分页线程，图 21.3 中的控制流需要稍作修改。交换算法需要先简单检
  查是否有空闲页，而不是直接执行替换。如果没有空闲页，会通知后台分页线程按需要释
  放页。当线程释放一定数目的页时，它会重新唤醒原来的线程，然后就可以把需要的页交
  换进内存，继续它的工作。
  
* 交换算法需要先简单检
  查是否有空闲页，而不是直接执行替换。如果没有空闲页，会通知后台分页线程按需要释
  放页。当线程释放一定数目的页时，它会重新唤醒原来的线程，然后就可以把需要的页交
  换进内存，继续它的工作。
  
* 把一些工作放在后台
  当你有一些工作要做的时候，把这些工作放在后台（background）运行是一个好注意，可以提高效
  率，并允许将这些操作合并执行。操作系统通常在后台执行很多工作。例如，在将数据写入硬盘之前，
  许多系统在内存中缓冲要写入的数据。这样做有很多好处：提高硬盘效率，因为硬盘现在可以一次写入
  多次要写入的数据，因此能够更好地调度这些写入。优化了写入延迟，因为数据写入到内存就可以返回。
  可能减少某些操作，因为写入操作可能不需要写入硬盘（例如，如果文件马上又被删除），也能更好地
  利用系统空闲时间（idle time），因为系统可以在空闲时完成后台工作，从而更好地利用硬件资源
  

* 很重要的是（并且令人惊讶的是），这些行为对进程都是透明的。对进程而
  言，它只是访问自己私有的、连续的虚拟内存。在后台，物理页被放置在物理内存中的任
  意（非连续）位置，有时它们甚至不在内存中，需要从硬盘取回。虽然我们希望在一般情
  况下内存访问速度很快，但在某些情况下，它需要多个硬盘操作的时间。像执行单条指令
  这样简单的事情，在最坏的情况下，可能需要很多毫秒才能完成。
  
  

##### 交换策略
*在虚拟内存管理程序中，如果拥有大量空闲内存，操作就会变得很容易。页错误发生
 了，你在空闲页列表中找到空闲页，将它分配给不在内存中的页。嘿，操作系统，恭喜！ 你
 又成功了。
 遗憾的是，当内存不够时事情会变得更有趣。在这种情况下，由于内存压力（memory
 pressure）迫使操作系统换出（paging out）一些页，为常用的页腾出空间。确定要踢出（evict）
 哪个页（或哪些页）封装在操作系统的替换策略（replacement policy）中。历史上，这是早
 期的虚拟内存系统要做的最重要的决定之一，因为旧系统的物理内存非常小。至少，有一
 些策略是非常值得了解的。
 
##### 缓存管理
* 在深入研究策略之前，先详细描述一下我们要解决的问题。由于内存只包含系统中所
  有页的子集，因此可以将其视为系统中虚拟内存页的缓存（cache）。因此，在为这个缓存选
  择替换策略时，我们的目标是让缓存未命中（cache miss）最少，即使得从磁盘获取页的次
  数最少。或者，可以将目标看成让缓存命中（cache hit）最多，即在内存中找到待访问页的
  次数最多。
  知道了缓存命中和未命中的次数，就可以计算程序的平均内存访问时间
在现代系统中，磁盘访问的成本非常高，即
使很小概率的未命中也会拉低正在运行的程序的总体平均内存访问时间。显然，我们必须尽可能地避
免缓存未命中，避免程序以磁盘的速度运行。要做到这一点，有一种方法就是仔细开发一
个聪明的策略

##### 最优替换策略

* 最优替换策略能达到总体未命中数量最少。Belady 展示了一个简单
  的方法（但遗憾的是，很难实现！），即替换内存中在最远将来才会被访问到的页，可以达
  到缓存未命中率最低。
* 与最优策略对比非常有用
  虽然最优策略非常不切实际，但作为仿真或其他研究的比较者还是非常有用的。比如，单说你喜欢
  的新算法有 80%的命中率是没有意义的，但加上最优算法只有 82%的命中率（因此你的新方法非常接近
  最优），就会使得结果很有意义，并给出了它的上下文。因此，在你进行的任何研究中，知道最优策略
  可以方便进行对比，知道你的策略有多大的改进空间，也用于决定当策略已经非常接近最优策略时，停
  止做无谓的优化
* 最优策略背后的想法你能理解。这样想：如果你不得不踢出一些页，为什么不踢
  出在最远将来才会访问的页呢？这样做基本上是说，缓存中所有其他页都比这个页重要。


* 缓存未命中的类型
  在计算机体系结构世界中，架构师有时会将未命中分为 3 类：强制性、容量和冲突未命中，有时称为
  3C 。发生强制性（compulsory miss）未命中（或冷启动未命中，cold-start miss ）是因为缓存
  开始是空的，而这是对项目的第一次引用。与此不同，由于缓存的空间不足而不得不踢出一个项目以将新
  项目引入缓存，就发生了容量未命中（capacity miss）。第三种类型的未命中（冲突未命中，conflict miss）
  出现在硬件中，因为硬件缓存中对项的放置位置有限制，这是由于所谓的集合关联性（set-associativity）。
  它不会出现在操作系统页面缓存中，因为这样的缓存总是完全关联的（fully-associative），即对页面可以放
  置的内存位置没有限制 。

* 遗憾的是，正如我们之前在开发调度策略时所看到的那样，未来的访问是无法知道的，
  你无法为通用操作系统实现最优策略①。因此，在开发一个真正的、可实现的策略时，我们
  将聚焦于寻找其他决定把哪个页面踢出的方法。因此，最优策略只能作为比较，知道我们
  的策略有多接近“完美”。


* 简单策略：FIFO 许多早期的系统避免了尝试达到最优的复杂性，采用了非常简单的替换策略。例如，
一些系统使用 FIFO（先入先出）替换策略。页在进入系统时，简单地放入一个队列。当发
生替换时，队列尾部的页（“先入”页）被踢出。FIFO 有一个很大的优势：实现相当简单。

* 另一个类似的替换策略是随机，在内存满的时候它随机选择一个页进行替换。随机具
  有类似于 FIFO 的属性。实现我来很简单，但是它在挑选替换哪个页时不够智能。
  随机策略取决于当时的运气。

##### 利用历史数据：LRU 

* 遗憾的是，任何像 FIFO 或随机这样简单的策略都可能会有一个共同的问题：它可能会
  踢出一个重要的页，而这个页马上要被引用。先进先出（FIFO）将先进入的页踢出。如果
  这恰好是一个包含重要代码或数据结构的页，它还是会被踢出，尽管它很快会被重新载入。
  因此，FIFO、Random 和类似的策略不太可能达到最优，需要更智能的策略。
  正如在调度策略所做的那样，为了提高后续的命中率，我们再次通过历史的访问情况
  作为参考。例如，如果某个程序在过去访问过某个页，则很有可能在不久的将来会再次访
  问该页。
  页替换策略可以使用的一个历史信息是频率（frequency）。如果一个页被访问了很多次，
  也许它不应该被替换，因为它显然更有价值。页更常用的属性是访问的近期性（recency），
  越近被访问过的页，也许再次访问的可能性也就越大。
  这一系列的策略是基于人们所说的局部性原则（principle of locality），基本上只
  是对程序及其行为的观察。这个原理简单地说就是程序倾向于频繁地访问某些代码（例如
  循环）和数据结构（例如循环访问的数组）。因此，我们应该尝试用历史数据来确定哪些页
  面更重要，并在需要踢出页时将这些页保存在内存中。
  因此，一系列简单的基于历史的算法诞生了。“最不经常使用”（Least-Frequently-Used，
  LFU）策略会替换最不经常使用的页。同样，“最少最近使用”（Least-Recently-Used，LRU）
  策略替换最近最少使用的页面。这些算法很容易记住：一旦知道这个名字，就能确切知道
  它是什么，这种名字就非常好。

##### 近似 LRU
* 正如你所看到的，像 LRU 这样的算法通常优于简单的策略（如 FIFO 或随机），它们可能
  会踢出重要的页。遗憾的是，基于历史信息的策略带来了一个新的挑战：应该如何实现呢？
  以 LRU 为例。为了实现它，我们需要做很多工作。具体地说，在每次页访问（即每次
  内存访问，不管是取指令还是加载指令还是存储指令）时，我们都必须更新一些数据，从
  而将该页移动到列表的前面（即 MRU 侧）。与 FIFO 相比，FIFO 的页列表仅在页被踢出（通
  过移除最先进入的页）或者当新页添加到列表（已到列表尾部）时才被访问。为了记录哪
  些页是最少和最近被使用，系统必须对每次内存引用做一些记录工作。显然，如果不十分
  小心，这样的记录反而会极大地影响性能。
* 有一种方法有助于加快速度，就是增加一点硬件支持。例如，硬件可以在每个页访问
  时更新内存中的时间字段（时间字段可以在每个进程的页表中，或者在内存的某个单独的
  数组中，每个物理页有一个）。因此，当页被访问时，时间字段将被硬件设置为当前时间。
  然后，在需要替换页时，操作系统可以简单地扫描系统中所有页的时间字段以找到最近最
  少使用的页。
  遗憾的是，随着系统中页数量的增长，扫描所有页的时间字段只是为了找到最精确最
  少使用的页，这个代价太昂贵。想象一下一台拥有 4GB 内存的机器，内存切成 4KB 的页。
  这台机器有一百万页，即使以现代 CPU 速度找到 LRU 页也将需要很长时间。这就引出了一
  个问题：我们是否真的需要找到绝对最旧的页来替换？找到差不多最旧的页可以吗?
  
* 从计算开销的角度来看，近似 LRU 更为可行，实际上这也
  是许多现代系统的做法。这个想法需要硬件增加一个使用位（use bit，有时称为引用位，
  reference bit）。
        
* 系统的每个页有一个使用位，然后这些使用位存储在某个地方（例如，它们可能在每个进程
                  的页表中，或者只在某个数组中）。每当页被引用（即读或写）时，硬件将使用位设置为 1。
                  但是，硬件不会清除该位（即将其设置为 0），这由操作系统负责。
* 操作系统如何利用使用位来实现近似 LRU？可以有很多方法，有一个简单的方法称作
  时钟算法（clock algorithm）。想象一下，系统中的所有页都放在一个循环列表中。时
  钟指针（clock hand）开始时指向某个特定的页（哪个页不重要）。当必须进行页替换时，操
  作系统检查当前指向的页 P 的使用位是 1 还是 0。如果是 1，则意味着页面 P 最近被使用，
  因此不适合被替换。然后，P 的使用位设置为 0，时钟指针递增到下一页（P + 1）。该算法
  一直持续到找到一个使用位为 0 的页，使用位为 0 意味着这个页最近没有被使用过（在最
  坏的情况下，所有的页都已经被使用了，那么就将所有页的使用位都设置为 0）。
  请注意，这种方法不是通过使用位来实现近似 LRU 的唯一方法。实际上，任何周期性
  地清除使用位，然后通过区分使用位是 1 和 0 来判定该替换哪个页的方法都是可以的。
  Corbato 的时钟算法只是一个早期成熟的算法，并且具有不重复扫描内存来寻找未使用页的
  特点，也就是它在最差情况下，只会遍历一次所有内存。

* 考虑脏页
  时钟算法的一个小修改，是对内存中的页是否被修改的
  额外考虑。这样做的原因是：如果页已被修改（modified）并因此变脏（dirty），则踢出它
  就必须将它写回磁盘，这很昂贵。如果它没有被修改（因此是干净的，clean），踢出就没成
  本。物理帧可以简单地重用于其他目的而无须额外的 I/O。因此，一些虚拟机系统更倾向于
  踢出干净页，而不是脏页。
  为了支持这种行为，硬件应该包括一个修改位（modified bit，又名脏位，dirty bit）。每次
  写入页时都会设置此位，因此可以将其合并到页面替换算法中。例如，时钟算法可以被改变，
  以扫描既未使用又干净的页先踢出。无法找到这种页时，再查找脏的未使用页面，等等。


##### 其他虚拟内存策略

* 页面替换不是虚拟内存子系统采用的唯一策略（尽管它可能是最重要的）。例如，操作
  系统还必须决定何时将页载入内存。该策略有时称为页选择（page selection）策略（因为
  Denning 这样命名），它向操作系统提供了一些不同的选项。
  对于大多数页而言，操作系统只是使用按需分页（demand paging），这意味着操作系统
  在页被访问时将页载入内存中，“按需”即可。当然，操作系统可能会猜测一个页面即将被
  使用，从而提前载入。这种行为被称为预取（prefetching），只有在有合理的成功机会时才
  应该这样做。例如，一些系统将假设如果代码页 P 被载入内存，那么代码页 P + 1 很可能很
  快被访问，因此也应该被载入内存。
  另一个策略决定了操作系统如何将页面写入磁盘。当然，它们可以简单地一次写出一
  个。然而，许多系统会在内存中收集一些待完成写入，并以一种（更高效）的写入方式将
  它们写入硬盘。这种行为通常称为聚集（clustering）写入，或者就是分组写入（grouping），
  这样做有效是因为硬盘驱动器的性质，执行单次大的写操作，比许多小的写操作更有效。
  
##### 抖动

* 在结束之前，我们解决了最后一个问题：当内存就是被超额请求时，操作系统应该做
什么，这组正在运行的进程的内存需求是否超出了可用物理内存？在这种情况下，系统将
不断地进行换页，这种情况有时被称为抖动（thrashing）。
一些早期的操作系统有一组相当复杂的机制，以便在抖动发生时检测并应对。例如，
给定一组进程，系统可以决定不运行部分进程，希望减少的进程工作集（它们活跃使用的
页面）能放入内存，从而能够取得进展。这种方法通常被称为准入控制（admission control），
它表明，少做工作有时比尝试一下子做好所有事情更好，这是我们在现实生活中以及在现
代计算机系统中经常遇到的情况（令人遗憾）。
目前的一些系统采用更严格的方法处理内存过载。例如，当内存超额请求时，某些版
本的 Linux 会运行“内存不足的杀手程序（out-of-memory killer）”。这个守护进程选择一个
内存密集型进程并杀死它，从而以不怎么委婉的方式减少内存。虽然成功地减轻了内存压
力，但这种方法可能会遇到问题，例如，如果它杀死 X 服务器，就会导致所有需要显示的
应用程序不可用。


































