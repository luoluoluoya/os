#### 历史
* 早期,构建计算机操作系统非常简单. 你可能会问,为什么? 因为用户对操作系统的期望不高. 然而当"易于使用","高性能","可靠性"等要求提出后,这导致了所有这些令人头痛的问题. 

* 早期系统: 从内存来看,早期的机器并没有提供多少抽象给用户. 基本上,机器的物理内存就是一个连续的字节序列.

* 多道程序和时分共享
    * 过了一段时间,由于机器昂贵,人们开始更有效地共享机器. 因此,**多道程序(multiprogramming)系统**时代开启,其中多个进程在给定时间准备运行,比如当有
      一个进程在等待 I/O 操作的时候,操作系统会切换这些进程,这样增加了 CPU 的有效利用率(utilization). 那时候,效率(efficiency)的提高尤其重要,因为每台机器的成本是数十万美元甚至数百万美元. 
    * 但很快,人们开始对机器要求更多,**分时系统**的时代诞生了. 具体来说,许多人意识到批量计算的局限性,尤其是程序员本身,他们厌倦了长时间的(因此也是低效率的)编程—调试循环. **交互性**变得很重要,因为许多用户可能同时在使用机器,每个人都在等待(或希望)他们执行的任务及时响应. 
    * 一种实现时分共享的方法,是**让一个进程单独占用全部内存运行一小段时间,然后停止它,并将它所有的状态信息保存在磁盘上(包含所有的物理内存),加载其他进程的状态信息,再运行一段时间**,这就实现了某种比较粗糙的机器共享. 遗憾的是,这种方法有一个问题：太慢了,特别是当内存增长的时候. 虽然保存和恢复寄存器级的状态信息(程序计数器、通用寄存器等)相对较快,但将全部的内存信息保存到磁盘就太慢了. 因此,**在进程切换的时候,我们仍然将进程信息放在内存中**,这样操作系统可以更有效率地实现时分共享. 
    
* 随着时分共享变得更流行. 特别是多个程序同时驻留在内存中,使保护(protection)成为重要问题. **人们不希望一个进程可以读取其他进程的内存,更别说修改了**. 

* 地址空间 
    * 操作系统需要提供一个易用(easy to use)的物理内存抽象. 这个抽象叫作地址空间(address space),是运行的程序看到的系统中的内存. 理解这个基本的操作系统内存抽象,是了解内存虚拟化的关键. 
    * 一个进程的地址空间包含运行的程序的所有内存状态. 比如：程序的代码(code,指令)必须在内存中,因此它们在地址空间里. 当程序在运行的时候,利用栈(stack)来保存当前的函数调用信息,分配空间给局部变量,传递参数和函数返回值. 最后,堆(heap)用于管理动态分配的、用户管理的内存,就像你从 C 语言调用 malloc()或面向对象语言(如 C ++ 或 Java)中调用 new 获得内存. 当然,还有其他的东西(例如,静态初始化的变量)
    * 在程序运行时,地址空间有两个区域可能增长(或者收缩). 它们就是堆(在顶部)和栈(在底部). 把它们放在那里,是因为它们都希望能够增长. 通过将它们放在地址空间的两端,我们可以允许这样的增长：它们只需要在相反的方向增长. (堆栈和堆的这种放置方法只是一种约定,如果你愿意,可以用不同的方式安排地址空间). 
    
* 隔离原则
    * 隔离是建立可靠系统的关键原则. 如果两个实体相互隔离,这意味着一个实体的失败不会影响另一个实体. 操作系统力求让进程彼此隔离,从而防止相互造成伤害. 通过内存隔离,操作系统进一步确保
      运行程序不会影响底层操作系统的操作. 

#### 目标

* 虚拟内存(VM)系统的一个主要目标是**透明(transparency)**. 操作系统实现虚拟内存的方式,应该让运行的程序看不见. 因此,程序不应该感知到内存被虚拟化的事实,相
  反,程序的行为就好像它拥有自己的私有物理内存. 在幕后,操作系统(和硬件)完成了所有的工作,让不同的工作复用内存,从而实现这个假象. 

* 虚拟内存的另一个目标是**效率(efficiency)**. 操作系统应该追求虚拟化尽可能高效(efficient),包括时间上(即不会使程序运行得更慢)和空间上(即不需要太多额外的内存
  来支持虚拟化). 在实现高效率虚拟化时,操作系统将不得不依靠硬件支持,包括 TLB 这样的硬件功能. 

* 最后,虚拟内存第三个目标是**保护(protection)**. 操作系统应确保进程受到保护(protect),不会受其他进程影响,操作系统本身也不会受进程影响. 当一个进程执行加载、存储或指
  令提取时,它不应该以任何方式访问或影响任何其他进程或操作系统本身的内存内容(即在它的地址空间之外的任何内容). 因此,保护让我们能够在进程之间提供隔离(isolation)
  的特性,每个进程都应该在自己的独立环境中运行,避免其他出错或恶意进程的影响. 

* 你看到的所有地址都不是真的
    * 实际上,作为用户级程序的程序员,可以看到的任何地址都是虚拟地址. 只有操作系统,通过精妙的虚拟化内存技术,知道这些指令和数据所在的物理内存的位置. 所以永远不要忘记：如
      果你在一个程序中打印出一个地址,那就是一个虚拟的地址. 虚拟地址只是提供地址如何在内存中分布的假象,只有操作系统(和硬件)才知道物理地址. 


* 虚拟内存: 虚拟内存系统负责为程序提供一个巨大的、稀疏的、私有的地址空间的假象,其中保存了程序的所有指令和数据. 操作系统在专门硬件的帮助下,通过每一个虚拟内存的索引,
  将其转换为物理地址,物理内存根据获得的物理地址但获取所需的信息. 操作系统会同时对许多进程执行此操作,并且确保程序之间互相不会受到影响,也不会影响操作系统. 

* 在实现 CPU 虚拟化时,我们遵循的一般准则被称为受限直接访问(Limited Direct Execution,LDE). LDE 背后的想法很简单：让程序运行的大部分指令直接访问硬件,只在一些关键点(如进程发起系统调用或发生时钟中断)
  由操作系统介入来确保"在正确时间,正确的地点,做正确的事". **为了实现高效的虚拟化,操作系统应该尽量让程序自己运行,同时通过在关键点的及时介入(interposing),来保持对硬件的控制**. 高效和控制是现代操作系统的两个主要目标. 

* 在实现虚拟内存时,我们将追求类似的战略,在实现**高效**和**控制**的同时,提供期望的虚拟化. 高效决定了我们要利用硬件的支持,(比如 TLB、页表等). 控制意味着操作系统要确保应用程序只能访问它自己的内存空间.
  因此,要保护应用程序不会相互影响,也不会影响操作系统,我们需要硬件的帮助. 最后,我们对虚拟内存还有一点要求,即**灵活性**. 具体来说,我们希望程序能以任何方式访问它自己的地址空间,从而让系统更容易编程. 
  所以, 关键问题在于：如何实现高效的内存虚拟化?如何提供应用程序所需的灵活性?如何保持控制应用程序可访问的内存位置,从而确保应用程序的内存访问受到合理的限制?如何高效地实现这一切?

* 利用一种通用技术,有时被称为**基于硬件的地址转换(hardware-based address translation)**,简称为地址转换(address translation). 它可以看成是受限直接执行这种一般方
  法的补充. 利用地址转换,硬件对每次内存访问进行处理(即指令获取、数据读取或写入),将指令中的虚拟(virtual)地址转换为数据实际存储的物理(physical)地址. 因此,在每次内存引用时,
  硬件都会进行地址转换,将应用程序的内存引用重定位到内存中实际的位置.当然,仅仅依靠硬件不足以实现虚拟内存,因为它只是提供了底层机制来提高效率.操作系统必须在关键的位置介入,
  设置好硬件,以便完成正确的地址转换. 因此它必须管理内存(manage memory),记录被占用和空闲的内存位置,并明智而谨慎地介入,保持对内存使用的控制. 

* 同样,所有这些工作都是为了创造一种美丽的假象：**每个程序都拥有私有的内存**,那里存放着它自己的代码和数据. 虚拟现实的背后是丑陋的物理事实：**许多程序其实是在同一时间共享着内存**,就像 CPU(或多个 CPU)在不同的程序间切换运行. 通过虚拟化,操作系统(在硬件的帮助下)将丑陋的机器现实转化成一种有用的、强大的、易于使用的抽象. 
  
* 重定位的集中形式
    * 静态重定位
        * 在早期,在硬件支持重定位之前,一些系统曾经采用纯软件的重定位方式. 基本技术被称为**静态重定位(载入时重定位)**,其中一个名为加载程序(loader)的软件接手将要运行的可执行程序,将它的地址重写到物理内存中期望的偏移位置. 
        * 例如,程序中有一条指令是从地址 1000 加载到寄存器``movl 1000,%eax``,当整个程序的地址空间被加载到从 3000(不是程序认为的 0)开始的物理地址中,加载程序会重写指令中的地址(``movl 4000, %eax``),从而完成简单的静态重定位. 
        * 然而,静态重定位有许多问题,首先也是最重要的是*不提供访问保护*,进程中的错误地址可能导致对其他进程或操作系统内存的非法访问,一般来说,需要硬件支持来实现真正的访问保护. 
          静态重定位的另一个缺点是一旦完成,稍后很难将内存空间重定位到其他位置(_无法支持运行时的换入换出操作_).
        
    * 动态(基于硬件)重定位
        * 每个 CPU 需要两个硬件寄存器：**基址(base)寄存器**和**界限(bound)寄存器**,有时称为限制(limit)寄存器. 这组基址和界限寄存器,让我们能够将地址空间放在物
          理内存的任何位置,同时又能确保进程只能访问自己的地址空间. 采用这种方式,在编写和编译程序时假设地址空间从零开始. 但是,当程序真正执行时,操作系统会决定其在物
          理内存中的实际加载地址,并将起始地址记录在基址寄存器中.当进程运行时,该进程产生的所有内存引用,都会被处理器通过以下方式转换为物理地址：
          ```physical address = virtual address + base``` 

* **进程中使用的内存引用都是虚拟地址(virtual address)**,硬件接下来将虚拟地址加上基址寄存器中的内容,得到物理地址(physical address),再发给内存系统.为了更好地理解,
  让我们追踪一条指令执行的情况. 具体来看前面序列中的一条指令：``128: movl 0x0(%ebx), %eax``程序计数器(PC)首先被设置为 128. 当硬件需要获取这条指令时,它先将这个值加上基
  址寄存器中的值,得到实际的物理地址,然后硬件从这个物理地址获取指令.接下来,处理器开始执行该指令.

* 基于硬件的动态重定位: 在动态重定位的过程中,只有很少的硬件参与,但获得了很好的效果. 一个基址寄存器将虚拟地址转换为物理地址,一个界限寄存器确
  保这个地址在进程地址空间的范围内. 它们一起提供了既简单又高效的虚拟内存机制. 

* **界限寄存器提供了访问保护**. 如果进程需要访问超过这个界限或者为负数的虚拟地址,CPU 将触发异常,进程最终可能被终止. 界限寄存器的用处在于,它确保了进程产生的所有地址都在进程的地址"界限"中. 这种基址寄存器
  配合界限寄存器的硬件结构是芯片中的(每个 CPU 一对). 有时我们将CPU 的这个负责地址转换的部分统称为内存管理单元(Memory Management Unit,MMU).随着我们开发更复杂的内存管理技术,MMU也将有更复杂的电路和功能. 
  
    * 在一种方式中,它**记录地址空间的大小,硬件在将虚拟地址与基址寄存器内容求和前,就检查这个界限.** 
    * 另一种方式是界限寄存器中**记录地址空间结束的物理地址,硬件在转化虚拟地址到物理地址之后才去检查这个界限.** 这两种方式在逻辑上是等价的.
  
* 数据结构
    * 操作系统必须记录哪些空闲内存没有使用,以便能够为进程分配内存. 很多不同的数据结构可以用于这项任务,其中最简单的(也是我们假定在这里采用的)是空闲列表(free list). 它就是一个列表,记录当前没有使用的物理内存的范围. 

* 动态重定位：硬件要求

    | 操作                             | 解释                                                 |
    | -------------------------------- | ---------------------------------------------------- |
    | 特权模式                         | 以防用户模式的进程执行特权操作                       |
    | 基址/界限寄存器                  | 每个 CPU 需要一对寄存器来支持地址转换和界限检查      |
    | 能够转换虚拟地址并检查它是否越界 | 电路来完成转换和检查界限,在这种情况下,非常简单       |
    | 修改基址/界限寄存器的特权指令    | 在让用户程序运行之前,操作系统必须能够设置这些值      |
    | 注册异常处理程序的特权指令       | 操作系统必须能告诉硬件,如果异常发生,那么执行哪些代码 |
    | 能够触发异常                     | 如果进程试图使用特权指令或越界的内存                 |

* 操作系统的问题
  * 第一,在**进程创建时,操作系统必须采取行动,为进程的地址空间找到内存空间**. (我们假设每个进程的地址空间小于物理内存的大小,并且大小相同)它可以把整个物理内存看作一组槽块,标记了空闲或已用. 当新进程创建时,操作系统检索这个数据结构(常被称为空闲列表,free list),为新地址空间找到位置,并将其标记为已用. 
  * 第二, **在进程终止时,操作系统会将这些内存放回到空闲列表,并根据需要清除相关的数据结构**. 
  * 第三,**在上下文切换时,操作系统必须保存和恢复基础和界限寄存器**. 具体来说,当操作系统决定中止当前的运行进程时,它必须将当前基址和界限寄存器中的内容保存在内存中,放在某种每个进程都有的结构中,如进程控制块(Process Control Block,PCB)中. 类似地,当操作系统恢复执行某个进程时(或第一次执行),也必须给基址和界限寄存器设置正确的值. 
  * 第四,**操作系统必须提供异常处理程序,或要一些调用的函数.** 操作系统在启动时加载这些处理程序(通过特权命令). 例如,当一个进程试图越界访问内存时,CPU 会触发异常. 在这种异常产生时,操作系统必须准备采取行动. 通常操作系统会做出充满敌意的反应：终止错误进程. **操作系统应该尽力保护它运行的机器**,因此它不会对那些企图访问非法地址或执行非法指令的进程客气.

* 由于该进程的栈区和堆区可能并不很大,导致这些内存区域中大量的空间被浪费. 这种浪费通常称为**内部碎片(internal fragmentation),指的是已经分配的内存单元内部有未使用的空间(即碎片),造成了浪费**. 

#### 分段

* 将所有进程的地址空间完整地加载到内存中. 利用基址和界限寄存器,操作系统很容易将不同进程重定位到不同的物理内存区域
  * 如果我们将整个地址空间放入物理内存,那么栈和堆之间的空间并没有被进程使用,却依然占用了实际的物理内存.
  * 程序本身由若干部分(段)组成，每个段有各自的特点、用途：代码段只读，代码/数据段不会动态增长...

* 为了解决这个问题,分段(segmentation)的概念应运而生. 这个想法很简单,在 MMU中引入不止一个基址和界限寄存器对,而是给地址空间内的每个逻辑段一对. 一个段只是地址空间里的一个连续定长的区域,在典型的地址空间里有几个逻辑不同的段：代码、数据、栈和堆.  分段的机制使得操作系统能够将不同的段放到不同的物理内存区域,从而避免了虚拟地址空间中的未使用部分占用物理内存. 
* 段错误: 段错误指的是在支持分段的机器上发生了非法的内存访问. 硬件会发现该地址越界,因此陷入操作系统,很可能导致终止出错进程. 

* 硬件在地址转换时使用段寄存器. 它如何知道段内的偏移量,以及地址引用了哪个段?
  * 显式(explicit)方式,就是用虚拟地址的开头几位来标识不同的段. 
  * 隐式(implicit)方式中,硬件通过地址产生的方式来确定段. 例如,如果地址由程序计数器产生(即它是指令获取),那么地址
    在代码段. 如果基于栈或基址指针,它一定在栈段. 其他地址则在堆段.  

* 栈怎么办
  * 栈有一点关键区别,它反向增长. 首先,我们需要一点硬件支持. 除了基址和界限外,硬件还需要知道段的增长方向(用一位区分,比如 1 代表自小而大增长,0 反之).
  * 硬件理解段可以反向增长后,这种虚拟地址的地址转换必须有点不同: 为了得到正确的反向偏移,我们必须从虚拟地址的偏移量中减去最大的段地址.只要用这个反向偏移量加上基址,就得到了正确的物理地址. 用户可以进行界限检查,确保反向偏移量的绝对值小于段的大小. 

* 保护位
  * 基本为每个段增加了几个位,标识程序是否能够读写该段,或执行其中的代码. 通过将代码段标记为只读,同样的代码可以被多个进程共享,而不用担心破坏隔离. 虽然每个进程都认为自己独占这块内存,但操作系统秘密地共享了内存,进程不能修改这些内存,所以假象得以保持. 
  * 代码段的权限是可读和可执行,因此物理内存中的一个段可以映射到多个虚拟地址空间. 有了保护位,前面描述的硬件算法也必须改变. 除了检查虚拟地址是否越界,硬件还需要检查特定访问是否允许. 如果用户进程试图写入只读段,或从非执行段执行指令,硬件会触发异常,让操作系统来处理出错进程. 

* 分段的基本原理. 系统运行时,地址空间中的不同段被重定位到物理内存中. 与我们之前介绍的整个地址空间只有一个基址/界限寄存器对的方式相比,大量节省了物理内存. 具体来说,栈和堆之间没有使用的区域就不需要再分配物理内存,让我们能将更多地址空间放进物理内存. 程序不同段也可以提供不同的保护位.

* 分段的问题

  * 操作系统在上下文切换时应该做什么?你可能已经猜到了：各个段寄存器中的内容必须保存和恢复. 显然,每个进程都有自己独立的虚拟地址空间,操作系统必须在进程运行前,确保这些寄存器被正确地赋值. 
  * 管理物理内存的空闲空间. 新的地址空间被创建时,操作系统需要在物理内存中为它的段找到空间.  每个进程都有一些段,每个段的大小也可能不同. 一般会遇到的问题是,物理内存很快充满了许多空闲空间的小洞,因而很难分配给新的段,或扩大已有的段. 这种问题被称为外部碎片(external fragmentation)
    * 该问题的一种解决方案是紧凑(compact)物理内存,重新安排原有的段. 例如,操作统先终止运行的进程,将它们的数据复制到连续的内存区域中去,改变它们的段寄存器中的值,指向新的物理地址,从而得到了足够大的连续空闲空间. 这样做,操作系统能让新的内存分配请求成功. 但是,内存紧凑成本很高,因为拷贝段是内存密集型的,一般会占用大量的处理器时间. 

* 如果需要管理的空间被划分为固定大小的单元,就很容易. 在这种情况下,只需要维护这些大小固定的单元的列表,如果有请求,就返回列表中的第一项. 如果要管理的空闲空间由大小不同的单元构成,管理就变得困难(而且有趣). 这种情况出现在用户级的内存分配库(如 malloc()和 free()),或者操作系统用分段(segmentation)的方式实现虚拟内存. 在这两种情况下,出现了外部碎片(external fragmentation)的问题：空闲空间被分割成不同大小的小块,成为碎片,后续的请求可能失败,因为没有一块足够大的连续空闲空间,即使这时总的空闲空间超出了请求的大小. 

* 该库管理的空间由于历史原因被称为堆,在堆上管理空闲空间的数据结构通常称为空闲列表(free list). 该结构包含了管理内存区域中所有空闲块的引用. 当然,该数据结构不一定真的是列表,而只是某种可以追踪空闲空间的数据结构. 

* 追踪已分配空间的大小

* 你可能注意到, ``free(void *ptr)``接口没有块大小的参数. 因此它是假定,对于给定的指针,内存分配库可以很快确定要释放空间的大小,从而将它放回空闲列表. 要完成这个任务,大多数分配程序都会在头块(header)中保存一点额外的信息,它在内存中,通常就在返回的内存块之前.该头块中至少包含所分配空间的大小. 它也可能包含一些额外的指针来加速空间释放,包含一个幻数来提供完整性检查,以及其他信息. 我们假定,一个简单的头块包含了分配空间的大小和一个幻数：

  ```
  typedef struct header_t {
      int size;
      int magic;
  } header_t;
  ```

  用户调用 free(ptr)时,库会通过简单的指针运算得到头块的位置：

  ```
  void free(void *ptr) {
  	header_t *hptr = (void *)ptr - sizeof(header_t);
  } 
  ```

  [!image](./images/14.png)
  获得头块的指针后,库可以很容易地确定幻数是否符合预期的值,作为正常性检查,并简单计算要释放的空间大小(即头块的大小加区域长度). (因此,如果用户请求 N 字节的内存,库不是寻找大小为 N 的空闲块,而是寻找N 加上头块大小的空闲块. )

* 如何在空闲内存自己内部建立这样一个列表呢?

* 假设我们需要管理一个 4096 字节的内存块(即堆是 4KB). 为了将它作为一个空闲空间列表来管理,首先要初始化这个列表. 开始,列表中只有一个条目,记录了大小为4096的空间(减去头块的大小). 下面是该列表中一个节点描述：

  ```
  typedef struct node_t {
      int size; 
      struct node_t *next;
  } node_t; 
  ```

  

  现在来看一些代码,它们初始化堆,并将空闲列表的第一个元素放在该空间中. 假设堆构建在某块空闲空间上,这块空间通过系统调用 mmap()获得. 这不是构建这种堆的唯一选择,但在这个例子中很合适. 下面是代码：

  ```
  // mmap() returns a pointer to a chunk of free space
  node_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, -1, 0);
  head->size = 4096 - sizeof(node_t);
  head->next = NULL;
  ```

  执行这段代码之后,列表的状态是它只有一个条目,记录大小为 4088. 是的,这是一个小堆,但对我们是一个很好的例子. head 指针指向这块区域的起始地址,假设是 16KB(尽管任何虚拟地址都可以). 堆看起来如图 17.3 所示. 现在,假设有一个 100 字节的内存请求. 为了满足这个请求,库首先要找到一个足够大小的块. 因为只有一个 4088 字节的块,所以选中这个块. 然后,这个块被分割(split)为两块：一块足够满足请求(以及头块,如前所述),一块是剩余的空闲块. 假设记录头块为 8 个字节(一个整数记录大小,一个整数记录幻数),堆中的空间如图 17.4 所示. 
  [!image](./images/15.png)
  至此,对于 100 字节的请求,库从原有的一个空闲块中分配了 108 字节,返回指向它的一个指针(在上图中用 ptr 表示),并在其之前连续的 8 字节中记录头块信息,供未来的free()函数使用. 同时将列表中的空闲节点缩小为 3980 字节(4088−108). 现在再来看该堆,其中有 3 个已分配区域,每个 100(加上头块是 108). 这个堆如图 17.5所示. 可以看出,堆的前 324 字节已经分配,因此我们看到该空间中有 3 个头块,以及 3 个 100字节的用户使用空间. 空闲列表还是无趣：只有一个节点(由 head 指向),但在 3 次分割后,现在大小只有 3764 字节. 但如果用户程序通过 free()归还一些内存,会发生什么?在这个例子中,应用程序调用 free(16500),归还了中间的一块已分配空间(内存块的起始地址 16384 加上前一块的 108,和这一块的头块的 8 字节,就得到了 16500). 这个值在前图中用 sptr 指向. 库马上弄清楚了这块要释放空间的大小,并将空闲块加回空闲列表. 假设我们将它插入到空闲列表的头位置,该空间如图 17.6 所示. 
  [!image](./images/16.png)
  现在的空闲列表包括一个小空闲块(100 字节,由列表的头指向)和一个大空闲块(3764字节). 现在假设剩余的两块已分配的空间也被释放. 没有合并,空闲列表将非常破碎,如图 17.7 所示. 从图中可以看出,我们现在一团糟! 为什么?简单,我们忘了合并(coalesce)列表项,虽然整个内存空间是空闲的,但却被分成了小段,因此形成了碎片化的内存空间. 解决方案很简单：遍历列表,合并(merge)相邻块. 完成之后,堆又成了一个整体. 
  [!image](./images/17.png)

* 让堆增长

  * 具体来说,如果堆中的内存空间耗尽,应该怎么办?最简单的方式就是返回失败. 在某些情况下这也是唯一的选择. 大多数传统的分配程序会从很小的堆开始,当空间耗尽时,再向操作系统申请更大的空间. 通常,这意味着它们进行了某种系统调用(例如,大多数 UNIX 系统中的 sbrk),让堆增长. 操作系统在执行 sbrk 系统调用时,会找到空闲的物理内存页,将它们映射到请求进程的地址空间中去,并返回新的堆的末尾地址. 这时,就有了更大的堆,请求就可以成功满足. 

#### 分配策略 

* **最优匹配(best fit)**：首先遍历整个空闲列表,找到和请求大小一样或更大的空闲块,然后返回这组候选者中最小的一块. 这就是所谓的最优匹配(也可以称为最
  小匹配). 只需要遍历一次空闲列表,就足以找到正确的块并返回. 最优匹配背后的想法很简单：**选择最接它用户请求大小的块,从而尽量避免空间浪费. 然而,这有代价. 简单的实现在遍历查找正确的空闲块时,要付出较高的性能代价. 而且会剩下很多难以利用的小块.**
* **最差匹配(worst fit)**方法与最优匹配相反,它尝试找最大的空闲块,分割并满足用户需求后,将剩余的块(很大)加入空闲列表. **最差匹配尝试在空闲列表中保留大小均匀的块. 然而, 最差匹配会导致出现内存中不存在较大空闲块的问题.**
* **首次匹配(first fit)**策略就是找到第一个足够大的块,将请求的空间返回给用户. 同样,剩余的空闲空间留给后续请求. 首次匹配有速度优势(不需要遍历所有空块),但**有时会让空闲列表开头的部分有很多小块**. 因此,分配程序如何管理空闲列表的顺序就变得很重要. 一种方式是基于地址排序(address-based ordering). 通过保持空闲块按内存地址有序,合并操作会很容易,从而减少了内存碎片. 
* **下次匹配(next fit)**算法多维护一个指针,指向上一次查找结束的位置. 其想法是**将对空闲空间的查找操作扩散到整个列表中去,避免对列表开头频繁的分割**. 这种策略的性能与首次匹配很接它,同样避免了遍历查找. 
* **分离空闲列表(segregated list).** 基本想法很简单：如果某个应用程序经常申请一种(或几种)大小的内存空间,那就用一个独立的列表,只管理这样大小的对象. 其他大小的请求都一给更通用的内存分配程序. 

* 伙伴系统
  * 因为合并对分配程序很关键,所以人们设计了一些方法,让合并变得简单,一个好例子就是二分伙伴分配程序(binary buddy allocator). 
  * 在这种系统中,空闲空间首先从概念上被看成大小为``2^N``的大空间. 当有一个内存分配请求时,空闲空间被递归地一分为二,直到刚好可以满足请求的大小(再一分为二就无法满足). 这时,请求的块被返回给用户. 在下面的例子中,一个 64KB 大小的空闲空间被切分,以便提供 7KB 的块：
    [!image](./images/18.png)
    在这个例子中,最左边的8KB块被分配给用户(如上图中深灰色部分所示). 请注意,这种分配策略只允许分配 2 的整数次幂大小的空闲块,因此会有内部碎片(internal fragment)的麻烦. 
  * 伙伴系统的漂亮之处在于块被释放时. 如果将这个 8KB 的块归还给空闲列表,分配程序会检查"伙伴"8KB 是否空闲. 如果是,就合二为一,变成 16KB 的块. 然后会检查这个 16KB 块的伙伴是否空闲,如果是,就合并这两块. 这个递归合并过程继续上溯,直到合并整个内存区域,或者某一个块的伙伴还没有被释放. 
  * 伙伴系统运转良好的原因,在于很容易确定某个块的伙伴. 每对互为伙伴的块只有一位不同,正是这一位决定了它们在整个伙伴树中的层次. 

* 将空间分割成固定长度的分片. 在虚拟内存中,我们称这种思想为分页. 分页不是将一个进程的地址空间分割成几个不同长度的逻辑段(即代码、堆、段),而是分割成固定大小的单元,每个单元称为一页. 相应地,我们把物理内存看成是定长槽块的阵列,叫作页帧(page frame). 每个这样的页帧包含一个虚拟内存页.

* 为了**记录地址空间的每个虚拟页放在物理内存中的位置**,操作系统通常为每个进程保存一个数据结构,称为**页表(page table)**. 页表的主要作用是**为地址空间的每个虚拟页面保存地址转换(address translation)**,从而让我们知道每个页在物理内存中的位置. 

* 重要的是要记住,这个页表是一个每进程的数据结构(我们讨论的大多数页表结构都是每进程的数据结构

* 页表可以变得非常大,比我们之前讨论过的小段表或基址/界限对要大得多. 例如,想象一个典型的 32 位地址空间,带有 4KB 的页. 这个虚拟地址分成 20 位的 VPN和 12 位的偏移量. 一个 20 位的 VPN 意味着, 操作系统必须为每个进程管理 2^20 个地址转换. 假设每个页表格条目(PTE)需要 4 个字节,来保存物理地址转换和任何其他有用的东西,每个页表就需要巨大的 4MB 内存!这非常大. 现在想象一下有 100 个进程在运行：这意味着操作系统会需要 400MB 内存...

* **页表就是一种数据结构,用于将虚拟地址(或者实际上,是虚拟页号)映射到物理地址(物理帧号)**. 因此,任何数据结构都可以采用. 最简单的形式称为**线性页表(linear page table)**,就是一个数组. 操作系统通过虚拟页号(VPN)检索该数组,并在该索引处查找页表项(PTE),以便找到期望的物理帧号(PFN).

* 至于每个 PTE 的内容,我们在其中有许多不同的位,值得有所了解. 

  * **有效位(valid bit)**: 通常用于指示特定地址转换是否有效. 例如,当一个程序开始运行时,它的代码和堆在其地址空间的一端,栈在另一端. 所有未使用的中间空间都将被标记为无效(invalid),如果进程尝试访问这种内存,就会陷入操作系统,可能会导致该进程终止. 因此,有效位对于支持稀疏地址空间至关重要. 通过简单地将地址空间中所有未使用的页面标记为无效,我们不再需要为这些页面分配物理帧,从而节省大量内存. 
  * **保护位(protection bit)**,表明页是否可以读取、写入或执行. 同样,以这些位不允许的方式访问页,会陷入操作系统.
  * **存在位(present bit)**表示该页是在物理存储器还是在磁盘上(即它已被换出,swapped out). 交换允许操作系统将很少使用的页面移到磁盘,从而释放物理内存. 
  * **脏位(dirty bit)**也很常见,表明页面被带入内存后是否被修改过. 
  * **参考位(reference bit)**,也被称为访问位, 有时用于追踪页是否被访问,也用于确定哪些页很受欢迎,因此应该保留在内存中. 

*  18.5 显示了来自 x86 架构的示例页表项. 它包含一个存在位(P),确定是否允许写入该页面的读/写位(R/W) 确定用户模式进程是否可以访问该页面的用户/超级用户位(U/S),有几位(PWT、PCD、PAT 和 G)确定硬件缓存如何为这些页面工作,一个访问位(A)和一个脏位(D),最后是页帧号(PFN)本身. 
  [!image](./images/23.png)
  
* 内存中的页表,我们已经知道它们可能太大了. 事实证明,它们也会让速度变慢. 以简单的指令为例：``movl addr, %eax`` 同样,我们只看对地址 addr 的显式引用,而不关心指令获取. 在这个例子中,我们假定硬件为我们执行地址转换. 要获取所需数据,系统必须首先将虚拟地址addr转换为正确的物理地址. 因此,在从正确的物理地址获取数据之前,系统必须首先从进程的页表中提取适当的页表项,执行转换,然后从物理内存中加载数据. 为此,硬件必须知道当前正在运行的进程的页表的位置. 现在让我们假设一个**页表基址寄存器(page-table base register)**包含页表的起始位置的物理地址. 为了找到想要的 PTE的位置,硬件将执行以下功能：
  
  ```
  VPN = (VirtualAddress & VPN_MASK) >> SHIFT;	// VPN_MASK 从完整的虚拟地址中挑选出 VPN 位; SHIFT 可以将 VPN 位向右移动以形成正确的整数虚拟页码
  PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE));	// 使用VPN作为页表基址寄存器指向的 PTE 数组的索引
  ```
  
  一旦知道了这个物理地址,硬件就可以从内存中获取 PTE,提取 PFN,并将它与来自虚拟地址的偏移量连接起来,形成所需的物理地址. 具体来说,你可以想象 PFN被 SHIFT左移,然后与偏移量进行逻辑或运算,以形成最终地址
  
  ```
  offset = VirtualAddress & OFFSET_MASK	// OFFSET_MASK 获取虚拟地址的页内偏移量
  PhysAddr = (PFN << SHIFT) | offset
  ```
  
  最后,硬件可以从内存中获取所需的数据并将其放入寄存器 eax. 程序现在已成功从内存中加载了一个值!

* 总之,我们现在描述了在每个内存引用上发生的情况的初始协议. **对于每个内存引用(无论是取指令还是显式加载或存储),分页都需要我们执行一个额外的内存引用,以便首先从页表中获取地址转换.** 额外的内存引用开销很大,在这种情况下,可能会使进程减慢两倍或更多. 现在你应该可以看到,有两个必须解决的实际问题. 如果不仔细设计硬件和软件,**页表会导致系统运行速度过慢,并占用太多内存.** 

#### TLB

* 我们要增加所谓的**地址转换旁路缓冲存储器(translation-lookaside buffer,TLB)**,它就是频繁发生的**虚拟到物理地址转换的硬件缓存(cache)**. 因此,更好的名称应该是地址转换缓存(address-translation cache). 对每次内存访问,硬件先检查 TLB,看看其中是否有期望的转换映射,如果有,就完成转换(很快),不用访问页表(其中有全部的转换映射). TLB 带来了巨大的性能提升,实际上,因此它使得虚拟内存成为可能. 

* TLB 的基本算法,说明硬件如何处理虚拟地址转换,假定使用简单的线性页表和硬件管理的 TLB

  ```
  VPN = (VirtualAddress & VPN_MASK) >> SHIFT			  // 从虚拟地址中提取页号
  (Success, TlbEntry) = TLB_Lookup(VPN)		    	  // 然后检查 TLB 是否有该 VPN 的转换映射
  if (Success == True) 								// TLB 命中
  	if (CanAccess(TlbEntry.ProtectBits) == True)	  // 保护检查
  		Offset = VirtualAddress & OFFSET_MASK
  		PhysAddr = (TlbEntry.PFN << SHIFT) | Offset	  // 从相关的 TLB 项中取出页帧号(PFN),与原来虚拟地址中的偏移量组合形成期望的物理地址(PA),并访问内存
  		AccessMemory(PhysAddr)
  	else
  		RaiseException(PROTECTION_FAULT)
  else 											   // 硬件访问页表来寻找转换映射,并用该转换映射更新 TLB
  	PTEAddr = PTBR + (VPN * sizeof(PTE))
  	PTE = AccessMemory(PTEAddr) 
  	if (PTE.Valid == False)
  		RaiseException(SEGMENTATION_FAULT)
  	else if (CanAccess(PTE.ProtectBits) == False)
  		RaiseException(PROTECTION_FAULT)
  	else
  		TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
  RetryInstruction()
  ```

* 类似其他缓存,TLB 依赖于空间和时间局部性. 如果某个程序表现出这样的局部性(许多程序是这样),TLB 的命中率可能很高. 
  * **时间局部性(temporal locality)**: 最近访问过的指令或数据项可能很快会再次访问. 想想循环中的循环变量或指令,它们被多次反复访问. 
  * **空间局部性(spatial locality)**: 当程序访问内存地址 x 时,可能很快会访问邻近 x 的内存. 想想遍历某种数组,访问一个接一个的元素. 
  * 当然,这些性质取决于程序的特点,并不是绝对的定律,而更像是一种经验法则. 硬件缓存,无论是指令、数据还是地址转换(如 TLB),都利用了局部性,在小而快的芯片内存储器中保存一份内存副本. 处理器可以先检查缓存中是否存在就近的副本,而不是必须访问(缓慢的)内存来满足请求. 如果存在,处理器就可以很快地访问它(例如在几个 CPU 时钟内),避免花很多时间来访问内存(好多纳秒). 
* 谁来处理 TLB 未命中
  * **硬件管理 TLB**: 以前的硬件有复杂的指令集(有时称为复杂指令集计算机),造硬件的人不太相信那些搞操作系统的人. 因此,硬件全权处理 TLB未命中. 为了做到这一点,硬件必须知道页表在内存中的确切位置(通过页表基址寄存器),以及页表的确切格式. 发生未命中时,硬件会"遍历"页表,找到正确的页表项,取出想要的转换映射,用它更新 TLB,并重试该指令. 这种"旧"体系结构有硬件管理的 TLB,
  * **软件管理 TLB**: 发生 TLB 未命中时,硬件系统会抛出一个异常,这会暂停当前的指令流,将特权级提升至内核模式,跳转至陷阱处理程序. 接下来这个陷阱处理程序是操作系统的一段代码,用于处理 TLB 未命中. 这段代码在运行时,会查找页表中的转换映射,然后用特别的"特权"指令更新 TLB,并从陷阱返回. 此时,硬件会重试该指令(导致 TLB 命中). 
    * 这里的从陷阱返回指令稍稍不同于之前提到的服务于系统调用的从陷阱返回. 在后一种情况下,从陷阱返回应该继续执行陷入操作系统之后那条指令,就像从函数调用返回后,会继续执行此次调用之后的语句. 在前一种情况下,在从 TLB 未命中的陷阱返回后,硬件必须从导致陷阱的指令继续执行. 这次重试因此导致该指令再次执行,但这次会命中 TLB. 因此,**根据陷阱或异常的原因,系统在陷入内核时必须保存不同的程序计数器,以便将来能够正确地继续执行.** 
    * 在运行 TLB 未命中处理代码时,**操作系统需要格外小心避免引起 TLB 未命中的无限递归**. 

* TLB 替换策略
  * **替换最近最少使用(least-recently-used,LRU)的项**. LRU尝试利用内存引用流中的局部性,假定最近没有用过的项,可能是好的换出候选项. 
  * **随机(random)策略**,即随机选择一项换出去. 这种策略很简单,并且可以避免一种极端情况. 例如,一个程序循环访问 n+1 个页,但 TLB 大小只能存放 n 个页. 这时之前看似"合理"的 LRU 策略就会表现得不可理喻,因为每次访问内存都会触发 TLB未命中,而随机策略在这种情况下就好很多. 

* TLB 也不能满足所有的程序需求. 具体来说,如果一个程序短时间内访问的页数超过了 TLB 中的页数,就会产生大量的 TLB 未命中,运行速度就会变慢. 这种现象被称为超出**TLB 覆盖范围(TLB coverage)**,这对某些程序可能是相当严重的问题. 解决这个问题的一种方案是**支持更大的页,把关键数据结构放在程序地址空间的某些区域,这些区域被映射到更大的页,使 TLB 的有效覆盖率增加**. 对更大页的支持通常被数据库管理系统(Database Management System,DBMS)这样的程序利用,它们的数据结构比较大,而且是随机访问. 

#### 多级页表

* 我们现在来解决分页引入的第二个问题：页表太大,因此消耗的内存太多
* **减小页表大小：使用更大的页**: 以 32 位地址空间为例,但这次假设用 16KB 的页. 因此,会有 18 位的 VPN 加上 14 位的偏移量. 假设每个页表项(4字节)的大小相同,现在线性页表中有 218 个项,因此每个页表的总大小为 1MB,页表缩到四分之一. 
  * 这种方法的主要问题在于,大内存页会导致每页内的浪费,这被称为**内部碎片问题**(因为浪费在分配单元内部). 因此,结果是应用程序会分配页,但只用每页的一小部分,而内存很快就会充满这些过大的页. 因此,大多数系统在常见的情况下使用相对较小的页大小：4KB(如 x86)或 8KB(如 SPARCv9). 
* **去掉页表中的所有无效区域**,而不是将它们全部保留在内存中
  * 去掉无效区域之后, 但页表中的页号不连续，就需要比较、查找，折半等查找策略. 假设使用二分查找 `log_2(2^20) = 20`. 最坏情况需要20次内存访问才能定位到当前页所在位置.
* **多级页表**, 将线性页表变成了类似树的东西. 这种方法非常有效,许多现代系统都用它多级页表的基本思想很简单. 首先,将页表分成页大小的单元. 然后,如果整页的页表项(PTE)无效,就完全不分配该页的页表. 为了追踪页表的页是否有效(以及如果有效,它在内存中的位置),使用了名为页目录(page directory)的新结构. 页目录因此可以告诉你页表的页在哪里,或者页表的整个页不包含有效页. 

* 以32位寻址系统为例

  * 我们将虚拟地址的前10位用于页目录(2^10), 低12位用于页内偏移(4K),其余用于只想页目录下的页表项偏移. 故映射2^32的物理地址空间时,只需要加载2^10大小的页目录再内存中(若每项大小位4,则需要4K). 依据虚拟地址的页目录偏移找到对应目录, 再依据虚拟地址的页表项偏移获取该目录下的对应页表项. 

    [[!image](./images/41.png)

* 理解时空折中
  * 在构建数据结构时,应始终考虑时间和空间的折中(time-space trade-off). 通常,如果你希望更快地访问特定的数据结构,就必须为该结构付出空间的代价. 
  * 应该指出,多级页表是有成本的. 在 TLB 未命中时,需要从内存加载多次,才能从页表中获取正确的地址转换信息(用于页目录和PTE 本身),而用线性页表只需要一次加载. 因此,多级表是一个时间—空间折中(time-space trade-off)的例子. 我们想要更小的表(并得到了),但不是没代价. 尽管在常见情况下(TLB 命中),性能显然是相同的,但 TLB 未命中时,则会因较小的表而导致较高的成本. 另一个明显的缺点是复杂性. 无论是硬件还是操作系统来处理页表查找,这样做无疑都比简单的线性页表查找更复杂. 通常我们愿意增加复杂性以提高性能或降低管理费用. 在多级表的情况下,为了节省宝贵的内存,我们使页表查找更加复杂. 


#### 内存操作API

* 关键问题：在 UNIX/C 程序中,如何分配和管理内存? 通常使用哪些接口?哪些错误需要避免?

* 内存类型 
    * 在运行一个 C 程序的时候,会分配两种类型的内存. 第一种称为栈内存,它的申请和释放操作是编译器来隐式管理的,所以有时也称为自动(automatic)内存. 函数中申请的内存都是栈内存
      编译器确保在你进入函数的时候,在栈上开辟空间. 当你从该函数退出时,编译器释放内存. 因此,如果你希望某些信息存在于函数调用之外,建议不要将它们放在栈上. 
    * 堆(heap)内存,其中所有的申请和释放操作都由程序员显式地完成. 下面的例子展示了如何在堆上分配一个整数,得到指向它的指针：
      [!image](./images/38.png)
      关于这一小段代码有两点说明. 首先,你可能会注意到栈和堆的分配都发生在这一行： 首先编译器看到指针的声明(int *p)时,知道为一个整型指针分配空间,随后,当程序调
      用 malloc()时,它会在堆上请求整数的空间,函数返回这样一个整数的地址(成功时,失败时则返回 NULL),然后将其存储在栈中以供程序使用. 
    
* malloc()调用
    * malloc 函数非常简单：传入要申请的堆空间的大小,它成功就返回一个指向新申请空间的指针,失败就返回 NULL

* free()调用 
  事实证明,分配内存是等式的简单部分. 知道何时、如何以及是否释放内存是困难的部分. 要释放不再使用的堆内存,程序员只需调用 free()：
  ```
    int *x = malloc(10 * sizeof(int)); 
    free(x); 
  ```
  该函数接受一个参数,即一个由 malloc()返回的指针. 因此,你可能会注意到,**分配区域的大小不会被用户传入,必须由内存分配库本身记录追踪**. 

* 其他调用: 内存分配库还支持一些其他调用. 例如,calloc()分配内存,并在返回之前将其置零. 如果你但为内存已归零并忘记自己初始化它,这可以防止出现一些错误. 当你为某些东西(比如一个数组)分配空间,然后需
  要添加一些东西时,例程 realloc()也会很有用：realloc()创建一个新的本大的内存区域,将旧区域复制到其中,并返回新区域的指针. 

* 许多新语言都支持自动内存管理(automatic memory management). 在这样的语言中,当你调用类似 malloc()的机制来分配内存时,你永远不需要调用某些东西来释放空间. 实实上,
  垃圾收集器(garbage collector)会运行,找出你不再引用的内存,替你释放它. 
  
* 常见错误
    * **忘记分配内存**: 许多例程在调用之前,都希望你为它们分配内存. 例如,例程 strcpy(dst, src)将源字符串中的字符串复制到目标指针. 但是,如果不小心,你可能会这样做：
      [!image](./images/39.png)
      运行这段代码时,可能会导致段错误(segmentation fault). 

    * **它编译过了或它运行了!=它对了**: 仅仅因为程序编译过了甚至正确运行了一次或多次,并不意味着程序是正确的. 许多事件可能会让你相信它能工作,但是之后有些事情会发生变化,它停止了. 
    * **没有分配足够的内存**: 另一个相关的错误是没有分配足够的内存,有时称为缓冲区溢出(buffer overflow). 在
      [!image](./images/40.png)
      奇怪的是,这个程序通常看起来会正确运行,这取决于如何实现 malloc 和许多其他细节. 在某些情况下,当字符串拷贝执行时,它会在超过分配空间的末尾处写入一个字我,但在某些情况下,
      这是无害的,可能会覆盖不再使用的变量. 在某些情况下,这些溢出可能具有令人难以置信的危害,实实上是系统中许多安全漏洞的来源. 在其他情况下,malloc 库总是分配一些额外的空间,
      因此你的程序实实上不会在其他某个变量的值上涂写,并且工作得很好. 还有一些情况下,该程序确实会发生故障和崩溃. 因此,我们学到了另一个宝贵的教训：**即使它正确运行过一次,也不意味着它是正确的**. 

    * **忘记初始化分配的内存**: 在这个错误中,你正确地调用 malloc(),但忘记在新分配的数据类型中填写一些值. 不要这样做!如果你忘记了,你的程序最终会遇到未初始化的读取(uninitialized read),它从
      堆中读取了一些未知值的数据. 谁知道那里可能会有什么?如果走运,读到的值使程序仍然有效(例如,零). 如果不走运,会读到一些随机和有害的东西. 

    * **忘记释放内存**: 另一个常见错误称为内存泄露(memory leak),如果忘记释放内存,就会发生. 在长时间运行的应用程序或系统(如操作系统本身)中,这是一个巨大的问题,因为缓慢泄露的
      内存会导致内存不足,此时需要重新启动. 因此,一般来说,当你用完一段内存时,应该确保释放它. 在某些情况下,不调用 free()似乎是合理的. 例如,你的程序运行时间很短,很但就会退出. 在这种情况下,
      当进程死亡时,操作系统将清理其分配的所有页面,因此不会发生内存泄露. 虽然这肯定"有效",但这可能是一个坏习惯,所以请谨慎选择这样的策略.
    
    * **在用完之前释放内存**: 有时候程序会在用完之前释放内存,这种错误称为*悬挂指针(dangling pointer)*. 随后的使用可能会导致程序崩溃或覆盖有效的内存. 
    
    * **反复释放内存**: 程序有时还会不止一次地释放内存,这被称为重复释放(double free). 这样做的结果是未定义的. 正如你所能想象的那样,内存分配库可能会感到困惑,并且会做各种奇怪的事情,崩溃是常见的结果. 
    
    * **错误地调用 free()**: free()期望你只传入之前从 malloc()得到的一个指针. 如果传入一些其他的值,坏事就可能发生(并且会发生). 因此,这种无效的释放(invalid free)是危险的,当然也应该避免. 

* 为什么在你的进程退出时没有内存泄露
    * 当你编写一个短时间运行的程序时,可能会使用 malloc()分配一些空间. 程序运行并即将完成：是否需要在退出前调用几次 free()?虽然不释放似乎不对,但在真正的意义上,没有任何内存会"丢失". 
    * 原因很简单：系统中实际存在两级内存管理. 
        * **第一级是由操作系统执行的内存管理**,操作系统在进程运行时将内存交给进程,并在进程退出(或以其他方式结束)时将其回收. 
        * **第二级管理在每个进程中**,例如在调用 malloc()和 free()时,在堆内管理. 即使你没有调用 free()(并因此泄露了堆中的内存),操作系统也会在程序结束运行时,收回进程的所
          有内存(包括用于代码、栈,以及相关堆的内存页). 无论地址空间中堆的状态如何,操作系统都会在进程终止时收回所有这些页面,从而确保即使没有释放内存,也不会丢失内存. 
    * 因此,对于短时间运行的程序,泄露内存通常不会导致任何操作问题(尽管它可能被认为是不好的形式). 如果你编写一个长期运行的服务器(例如 Web 服务器或数据库管理系统,它永远不会退出),泄露内存就是很大的问题,最终会导致应用程序在内存不足时崩溃. 
