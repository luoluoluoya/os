* 操作系统提供的基本的抽象——进程。进程的非正式定义非常简单：进程就是运行中的程序。程序本身是没有生命周期的，它只是存在磁盘上面的一些指令(也可能是一些静态数据)。是操作系统让这些字节运行起来，让程序发挥作用。
  
* 事实表明，人们常常希望同时运行多个程序。比如：在使用计算机或者笔记本的时候，我们会同时运行浏览器、邮件、游戏、音乐播放器，等等。实际上，一个正常的系统可能会有上百个进程同时在运行。操作系统通过虚拟化 CPU 来提供这种假象。通过让一个进程只运行一个时间片，然后切换到其他进程，操作系统提供了存在多个虚拟 CPU 的假象。这就是时分共享CPU 技术，允许用户如愿运行多个并发进程。潜在的开销就是性能损失，因为如果 CPU 必须共享，每个进程的运行就会慢一点。
  
* 使用时分共享(和空分共享)
    * 时分共享是操作系统共享资源(例如，CPU 或网络链接，可以被许多人共享)所使用的最基本的技术之一。通过允许资源由一个实体使用一小段时间，然后由另一个实体使用一小段时间，如此下去。
    * 时分共享的自然对应技术是空分共享，资源在空间上被划分给希望使用它的人。例如，磁盘空间自然是一个空分共享资源，因为一旦将块分配给文件，在用户删除文件之前，不可能将它分配给其他文件。
    
* 操作系统为正在运行的程序提供的抽象，就是所谓的进程。正如我们上面所说的，一个进程只是一个正在运行的程序。在任何时刻，我们都可以清点它在执行过程中访问或影响的系统的不同部分，从而概括一个进程。
* 为了理解构成进程的是什么，我们必须理解它的机器状态：程序在运行时可以读取或更新的内容。
    * 进程的机器状态有一个明显组成部分，就是它的内存。指令存在内存中。正在运行的程序读取和写入的数据也在内存中。因此进程可以访问的内存(称为地址空间，address space)是该进程的一部分。
    * 进程的机器状态的另一部分是寄存器。许多指令明确地读取或更新寄存器，因此显然，它们对于执行该进程很重要(有一些非常特殊的寄存器构成了该机器状态的一部分)。
    * 最后，程序也经常访问持久存储设备。此类 I/O 信息可能包含当前打开的文件列表。


##### 操作系统如何启动并运行一个程序?进程创建实际如何进行?

* 操作系统运行程序必须做的第一件事是将代码和所有静态数据(例如初始化变量)加载到内存中，加载到进程的地址空间中。程序最初以某种可执行格式驻留在磁盘上(或者在某些现代系统中，在基于闪存的 SSD 上)。因此，将程序和静态数据加载到内存中的过程，需要操作系统从磁盘读取这些字节，并将它们放在内存中的某处。

* 在早期的(或简单的)操作系统中，加载过程尽早完成，即在运行程序之前全部完成。现代操作系统惰性执行该过程，即仅在程序执行期间需要加载的代码或数据片段，才会加载。要真正理解代码和数据的惰性加载是如何工作的，必须更多地了解分页和交换的机制。
* 必须为程序的运行时栈分配一些内存。程序使用栈存放局部变量、函数参数和返回地址。操作系统分配这些内存，并提供给进程。
* 操作系统也可能会用参数初始化栈。具体来说，它会将参数填入 main()函数，即 argc 和 argv 数组。
* 操作系统也可能为程序的堆分配一些内存。在 C 程序中，堆用于显式请求的动态分配数据。程序通过调用 malloc() 来请求这样的空间，并通过调用free()来明确地释放它。数据结构(如链表、散列表、树和其他有趣的数据结构)需要堆。起初堆会很小。随着程序运行，通过 malloc() 库 API 请求更多内存，操作系统可能会参与分配更多内存给进程，以满足这些调用。
* 操作系统还将执行一些其他初始化任务，特别是与输入/输出(I/O)相关的任务。例如，在 UNIX 系统中，默认情况下每个进程都有 3 个打开的文件描述符，用于标准输入、输出和错误。这些描述符让程序轻松读取来自终端的输入以及打印输出到屏幕。
  
* 启动程序，在入口处运行，即 main()。通过跳转到 main()例程，OS 将 CPU 的控制权转移到新创建的进程中，从而程序开始执行。

##### 进程状态 

* 初始(initial)状态：表示进程在创建时处于的状态。
* 运行(running)状态：在运行状态下，进程正在处理器上运行。这意味着它正在执行指令。
* 就绪(ready)状态：在就绪状态下，进程已准备好运行，但由于某种原因，操作系统选择不在此时运行。
* 阻塞(blocked)状态：在阻塞状态下，一个进程执行了某种操作，直到发生其他事件时才会准备运行。一个常见的例子是，当进程向磁盘发起 I/O 请求时，它会被阻塞，因此其他进程可以使用处理器。
* 已退出但尚未清理的最终(final)状态(在基于 UNIX 的系统中，这称为僵尸状态)：这个最终状态非常有用，因为它允许其他进程(通常是创建进程的父进程)检查进程的返回代码，并查看刚刚完成的进程是否成功执行(通常，在基于 UNIX 的系统中，程序成功完成任务时返回零，否则返回非零)。

* 可以根据操作系统的载量，让进程在就绪状态和运行状态之间转换。从就绪到运行意味着该进程已经被调度。从运行转移到就绪意味着该进程已经取消调度。一旦进程被阻塞(例如，通过发起 I/O 操作)，OS 将保持进程的这种状态，直到发生某种事件(例如，I/O 完成)。此时，进程再次转入就绪状态(也可能立即再次运行，如果操作系统这样决定)。

##### 数据结构 

* 操作系统是一个程序，和其他程序一样，它有一些关键的数据结构来跟踪各种相关的信息。例如，为了跟踪每个进程的状态，操作系统可能会为所有就绪的进程保留某种进程列表(process list)，以及跟踪当前正在运行的进程的一些附加信息。操作系统还必须以某种方式跟踪被阻塞的进程。当 I/O 事件完成时，操作系统应确保唤醒正确的进程，让它准备好再次运行。

##### 进程 API

* 创建(create)：操作系统必须包含一些创建新进程的方法。在 shell 中键入命令或双击应用程序图标时，会调用操作系统来创建新进程，运行指定的程序。
    * 系统调用 fork() 用于创建新进程
    * 子进程并非是完全拷贝了父进程。具体来说，它拥有自己的地址空间(即拥有自己的私有内存)、寄存器、程序计数器等，它从 fork()返回的值是不同的。父进程获得的返回值是新创建子进程的 PID，而子进程获得的返回值是 0。
* 销毁(destroy)：由于存在创建进程的接口，因此系统还提供了一个强制销毁进程的接口。当然，很多进程会在运行完成后自行退出。但是，如果它们不退出，用户可能希望终止它们，因此停止失控进程的接口非常有用。
* 等待(wait)：有时等待进程停止运行是有用的，因此经常提供某种等待接口。
    * 父进程调用 wait()，延迟自己的执行，直到子进程执行完毕。当子进程结束谁，wait()才返回父进程。
* 其他控制(miscellaneous control)：除了杀死或等待进程外，有时还可能有其他更多细节控制。例如，大多数操作系统提供某种方法来暂停进程(停止运行一段时间)，然后恢复(继续运行)。
* 状态(statu)：通常也有一些接口可以获得有关进程的状态信息，例如运行了多长时间，或者处于什么状态。
* exec()系统调用：它也是创建进程 API 的一个重要部分。这个系统调用可以让子进程执行与父进程不同的程序。给可执行程序的名称(如 wc)及需要的参数后，exec()会从可执行程序中加载代码和静态数据，并用它覆写自己的代码段(以及静态数据)，堆、栈及其他内存空间也会被重新初始化。然后操作系统就执行该程序，将参数通过 argv 传递给该进程。因此，它并非创建新进程，而是直接将当前运行的程序替换为不同的运行程序。对 exec()的成功调用永远不会回。

#### 进程调度
##### 调度指标

* 周转时间(turnaround time)：任务的周转时间定义为任务完成时间减去任务到达系统的时间，周转时间是一个性能指标。
* 公平(fairness)：性能和公平在调度系统中往往是矛盾的。例如，调度程序可以优化性能，但代价是以阻止一些任务运行，这就降低了公平。
* 响应时间(response time)：响应时间定义为从任务到达系统到首次运行的时间。

##### 调度算法
* 先进先出(First In First Out 或 FIFO)调度：先到先服务。
    * 它很简单，而且易于实现。
    * 护航效应(convoy effect)，一些耗时较少的潜在资源消费者被排在重量级的资源消费者之后。
    * 未考虑到任务的优先级
* 最短任务优先(shortest job first SJF) ：一种非抢占式调度程序。先运行最短的任务，然后是次短的任务。
    * 平均周转时间好。
    * 假设 A 在 t = 0 时到达，且需要运行 100s。而 B 和 C 在 t = 10 到达，且各需要运行 10s。用 SJF 时，B/C需要再A执行完成之后才能执行。当短任务再长任务之后到达，任然会产生护航效应。
* 最短完成时间优(Shortest Time-to-Completion First，STCF)：向 SJF 添加抢占。每当新工作进入系统时，它就会确定剩余工作和新工作中，谁的剩余时间最少，然后调度该工作。
    * 长任务存在饥饿
* 轮转(Round-Robin，RR)调度：RR 在一个时间片(time slice，有时称为调度量子，scheduling quantum)内运行一个工作，然后切换到运行队列的下一个任务，而不是运行一个任务直到结束。它反复执行，直到所有任务完成。因此，RR 有时被称为时间切片(time-slicing)。
  * 请注意，时间片长度必须是时钟中断周期的倍数。因此，如果时钟中断是每 10ms 中断一次，则时间片可以是 10ms、20ms 或 10ms 的任何其他倍数。
  * 时间片长度对于 RR 是至关重要的。越短，RR 在响应时间上表现越好。然而，时间片太短是有问题的：突然上下文切换的成本将影响整体性能。因此，系统设计者需要权衡时间片的长度，使其足够长，以便摊销上下文切换成本，而又不会使系统不及时响应。
  * 如果响应时间是我们的唯一指标，那么带有合理时间片的 RR，就会是非常好的调度程序。RR延伸每个工作，只运行每个工作一小段时间，就转向下一个工作。
  * 周转时间只关心作业何时完成，RR 几乎是最差的，在很多情况下甚至比简单的 FIFO 更差。
 * 任何公平的政策(如 RR)，即在小规模的时间内将 CPU 均匀分配到活动进程之间，在周转时间这类指标上表现不佳。事实上，这是固有的权衡：如果你愿意不公平，你可以运行较短的工作直到完成，但是要以响应时间为代价。如果你重视公平性，则响应时间会较短，但会以周转时间为代价。这种权衡在系统中很常见。
 * 抢占式调度程序。过去的批处理计算中，开发了一些非抢占式调度程序。这样的系统会将每项工作做完，再考虑是否运行新工作。几乎所有现代化的调度程序都是抢占式的。这意味着调度程序采用了我们之前学习的机制。特别是调度程序可以进行上下文切换，临时停止一个运行进程，并恢复(或启动)另一个进程。
* 调度程序显然要在工作发起 I/O 请求时做出决定，因为当前正在运行的作业在 I/O 期间不会使用 CPU，它被阻塞等待 I/O 完成。如果将 I/O 发送到硬盘驱动器，则进程可能会被阻塞几毫秒或更长时间，具体取决于驱动器当前的 I/O 负载。因此，这时调度程序应该在 CPU上安排另一项工作。调度程序还必须在 I/O 完成时做出决定。发生这种情况时，会产生中断，操作系统运行并将发出 I/O 的进程从阻塞状态移回就绪状态。

#### 多级反馈队列

* 多级反馈队列是用历史经验预测未来的一个典型的例子，操作系统中有很多地方采用了这种技术。如果工作有明显的阶段性行为，因此可以预测，那么这种方式会很有效。当然，必须十分小心地使用这种技术，因为它可能出错，让系统做出比一无所知的时候更糟的决定。
* MLFQ：基本规则
  * MLFQ 中有许多独立的队列(queue)，每个队列有不同的优先级。任何时刻，一个工作只能存在于一个队列中。MLFQ 总是优先执行较高优先级的工作(即在较高级队列中的工作)。当然，每个队列中可能会有多个工作，因此具有同样的优先级。在这种情况下，我们就对这些工作采用轮转调度。
  * MLFQ 调度策略的关键在于如何设置优先级。MLFQ 没有为每个工作指定不变的优先级，而是根据观察到的行为调整它的优先级。例如，如果一个工作不断放弃CPU 去等待键盘输入，这是交互型进程的可能行为，MLFQ 因此会让它保持高优先级。相反，如果一个工作长时间地占用 CPU，MLFQ 会降低其优先级。通过这种方式，MLFQ 在进程运行过程中学习其行为，从而利用工作的历史来预测它未来的行为。
*  MLFQ 的两条基本规则。
  * 规则 1：如果 A 的优先级 > B 的优先级，运行 A(不运行 B)。
  * 规则 2：如果 A 的优先级 = B 的优先级，轮转运行

##### 如何改变优先级

* 新增规则

  * 规则 3：工作进入系统时，放在最高优先级(最上层队列)。
  * 规则 4a：工作用完整个时间片后，降低其优先级(移入下一个队列)。
  *  规则 4b：如果工作在其时间片以内主动释放 CPU，则优先级不变。

  * 这个算法的一个主要目标：如果不知道工作是短工作还是长工作，那么就在开始的时候假设其是短工作，并赋予最高优先级。如果确实是短工作，则很快会执行完毕，否则将被慢慢移入低优先级队列，而这时该工作也被认为是长工作了。通过这种方式，MLFQ 近似于 SJF。
  * 看一个有 I/O 的例子：根据上述规则 4b，如果进程在时间片用完之前主动放弃 CPU，则保持它的优先级不变。这条规则的意图很简单：假设交互型工作中有大量的 I/O 操作(比如等待用户的键盘或鼠标输入)，它会在时间片用完之前放弃 CPU。在这种情况下，我们不想处罚它，只是保持它的优先级不变。
  * 当前 MLFQ 的一些问题
    * 饥饿问题：如果系统有"太多"交互型工作，就会不断占用CPU，导致长工作永远无法得到 CPU(它们饿死了)。即使在这种情况下，我们希望这些长工作也能有所进展。
    * 攻击者可以再进程在时间片用完之前，调用一个 I/O 操作(比如访问一个无关的文件)，从而主动释放 CPU。如此便可以保持在高优先级，占用更多的CPU 时间。做得好时(比如，每运行 99%的时间片时间就主动放弃一次 CPU)，工作可以几乎独占 CPU。最后，一个程序可能在不同时间表现不同。一个计算密集的进程可能在某段时间表现为一个交互型的进程。用我们目前的方法，它不会享受系统中其他交互型工作的待遇。
  
* 提升优先级

  *  让我们试着改变之前的规则，看能否避免饥饿问题。要让 CPU 密集型工作也能取得一些进展(即使不多)。一个简单的思路是周期性地提升所有工作的优先级。可以有很多方法做到，但我们就用最简单的：将所有工作扔到最高优先级队列。于是有了如下的新规则。
    *  规则 5：经过一段时间 S，就将系统中所有工作重新加入最高优先级队列。
  *  新规则一下解决了两个问题。
     *  进程不会饿死——在最高优先级队列中，它会以轮转的方式，与其他高优先级工作分享 CPU，从而最终获得执行。
     *  如果一个 CPU 密集型工作变成了交互型，当它优先级提升时，调度程序会正确对待它。
  *  添加时间段 S 也导致了明显的问题：*S* 的值应该如何设置?如果 S 设置得太高，长工作会饥饿；如果设置得太低，交互型工作又得不到合适的 CPU 时间比例。

*  更好的计时方式 

  * 如何阻止调度程序被愚弄?可以看出，这里的元凶是规则4a 和 4b，导致工作在时间片以内释放 CPU，就保留它的优先级。这里的解决方案，是为 MLFQ 的每层队列提供更完善的 CPU 计时方式。调度程序应该记录一个进程在某一层中消耗的总时间，而不是在调度时重新计时。只要进程用完了自己的配额，就将它降到低一优先级的队列中去。不论它是一次用完的，还是拆成很多次用完。因此，我们重写规则 4a 和 4b。

    * 规则 4：一旦工作用完了其在某一层中的时间配额(无论中间主动放弃了多少次CPU)，就降低其优先级(移入低一级队列)。有了这样的保护后，不论进程的 I/O 行为如何，都会慢慢地降低优先级，因而无法获得超过公平的 CPU 时间比例。


##### MLFQ 调优及其他问题 

* 如何配置一个调度程序，例如，配置多少队列?每一层队列的时间片配置多大?为了避免饥饿问题以及进程行为改变，应该多久提升一次进程的优先级?
* 这些问题都没有显而易见的答案，因此只有利用对工作负载的经验，以及后续对调度程序的调优，才会导致令人满意的平衡。例如，大多数的 MLFQ 变体都支持不同队列可变的时间片长度。高优先级队列通常只有较短的时间片(比如 10ms 或者更少)，因而这一层的交互工作可以更快地切换。

* 操作系统很少知道什么策略对系统中的单个进程和每个进程算是好的，因此提供接口并允许用户或管理员给操作系统一些提示常常很有用。我们通常称之为建议，因为操作系统不一定要关注它，但是可能会将建议考虑在内，以便做出更好的决定。这种用户建议的方式在操作系统中的各个领域经常十分有用，包括调度程序(通过 nice)、内存管理(madvise)，以及文件系统(通知预取和缓存)。 
* 低优先级队列中更多的是 CPU 密集型工作，配置更长的时间片会取得更好的效果。
* MLFQ 有趣的原因是：它不需要对工作的运行方式有先验知识，而是通过观察工作的运行来给出对应的优先级。通过这种方式，MLFQ 可以同时满足各种工作的需求：对于短时间运行的交互型工作，获得类似于 SJF/STCF 的很好的全局性能，同时对长时间运行的CPU 密集型负载也可以公平地、不断地稳步向前。因此，许多系统使用某种类型的 MLFQ作为自己的基础调度程序。

#### 比例份额(proportional-share)调度

* 比例份额算法基于一个简单的想法：调度程序的最终目标，是确保每个工作获得一定比例的 CPU 时间，而不是优化周转时间和响应时间。
* 比例份额调度程序有一个非常优秀的现代例子，名为彩票调度(lottery scheduling)。基本思想很简单：每隔一段时间，都会举行一次彩票抽奖，以确定接下来应该运行哪个进程。越是应该频繁运行的进程，越是应该拥有更多地赢得彩票的机会。
* 彩票调度背后是一个非常基本的概念：彩票数代表了进程(或用户或其他)占有某个资源的份额。一个进程拥有的彩票数占总彩票数的百分比，就是它占有资源的份额。
* 假设有两个进程 A 和 B，A 拥有 75 张彩票，B 拥有 25 张。因此我们希望 A 占用 75% 的 CPU 时间，而 B 占用 25%。通过不断定时地(比如，每个时间片)抽取彩票，彩票调度从概率上(但不是确定的)获得这种份额比例。抽取彩票的过程很简单：调度程序知道总共的彩票数(在我们的例子中，有 100 张)。调度程序抽取中奖彩票，这是从 0 和 99之间的一个数，拥有这个数对应的彩票的进程中奖。假设进程 A 拥有 0 到 74 共 75 张彩票，进程 B 拥有 75 到 99 的 25 张，中奖的彩票就决定了运行 A 或 B。调度程序然后加载中奖进程的状态，并运行它。
* 彩票调度最精彩的地方在于利用了随机性(randomness)。当你需要做出决定时，采用随机的方式常常是既可靠又简单的选择。
  * 第一，随机方法常常可以避免奇怪的边角情况，较传统的算法可能在处理这些情况时遇到麻烦。例如 LRU 替换策略。虽然 LRU 通常是很好的替换算法，但在有重复序列的负载时表现非常差。但随机方法就没有这种最差情况。
  * 第二，随机方法很轻量，几乎不需要记录任何状态。在传统的公平份额调度算法中，记录每个进程已经获得了多少的 CPU 时间，需要对每个进程计时，这必须在每次运行结束后更新。而采用随机方式后每个进程只需要非常少的状态(即每个进程拥有的彩票号码)。
  * 第三，随机方法很快。只要能很快地产生随机数，做出决策就很快。因此，随机方式在对运行速度要求高的场景非常适用。当然，越是需要快的计算速度，随机就会越倾向于伪随机

##### 彩票调度策略

* 一种方式是利用彩票货币的概念。这种方式允许拥有一组彩票的用户以他们喜欢的某种货币，将彩票分给自己的不同工作。之后操作系统再自动将这种货币兑换为正确的全局彩票。比如，假设用户 A 和用户 B 每人拥有 100 张彩票。用户 A 有两个工作 A1 和 A2，他以自己的货币，给每个工作 500 张彩票(共 1000 张)。用户 B 只运行一个工作，给它 10 张彩票(总共 10 张)。操作系统将进行兑换，将 A1 和 A2 拥有的 A 的货币 500 张，兑换成全局货币 50 张。类似地，兑换给 B1 的 10 张彩票兑换成 100 张。然后会对全局彩票货币(共 200张)举行抽奖，决定哪个工作运行。
* 另一个有用的机制是彩票转让。通过转让，一个进程可以临时将自己的彩票交给另一个进程。这种机制在客户端/服务端交互的场景中尤其有用，在这种场景中，客户端进程向服务端发送消息，请求其按自己的需求执行工作，为了加速服务端的执行，客户端可以将自己的彩票转让给服务端，从而尽可能加速服务端执行自己请求的速度。服务端执行结束后会将这部分彩票归还给客户端。
* 彩票通胀有时也很有用。利用通胀，一个进程可以临时提升或降低自己拥有的彩票数量。当然在竞争环境中，进程之间互相不信任，这种机制就没什么意义。一个贪婪的进程可能给自己非常多的彩票，从而接管机器。但是，通胀可以用于进程之间相互信任的环境。在这种情况下，如果一个进程知道它需要更多CPU 时间，就可以增加自己的彩票，从而将自己的需求告知操作系统，这一切不需要与任何其他进程通信。

##### 如何为工作分配彩票

* 这是一个非常棘手的问题，系统的运行严重依赖于彩票的分配。假设用户自己知道如何分配，因此可以给每个用户一定量的彩票，由用户按照需要自主分配给自己的工作。然而这种方案似乎什么也没有解决——还是没有给出具体的分配策略。因此对于给定的一组工作，彩票分配的问题依然没有最佳答案。

##### 步长调度

* 虽然随机方式可以使得调度程序的实现简单(且大致正确)，但偶尔并不能产生正确的比例，尤其在工作运行时间很短的情况下。

*  系统中的每个工作都有自己的步长，这个值与票数值成反比。假设A、B、C 这 3 个工作的票数分别是 100、50 和 250，我们通过用一个大数分别除以他们的票数来获得每个进程的步长。比如用 10000 除以这些票数值，得到了 3 个进程的步长分别为 100、200 和 40。我们称这个值为每个进程的步长。
* 每次进程运行后，我们会让它的计数器(行程值)加它的步长，记录它的总体进展。之后，调度程序使用进程的步长及行程值来确定调度哪个进程。基本思路很简单：当需要进行调度时，选择目前拥有最小行程值的进程，并且在运行之后将该进程的行程值增加一个步长。
* 在我们的例子中，3 个进程(A、B、C)的步长值分别为 100、200 和 40，初始行程值都为 0。因此，最初，所有进程都可能被选择执行。假设选择 A(任意的，所有具有同样低的行程值的进程，都可能被选中)。A 执行一个时间片后，更新它的行程值为 100。然后运行 B，并更新其行程值为 200。最后执行 C，C 的行程值变为40。这时，算法选择最小的行程值，是 C，执行并增加为 80(C 的步长是 40)。然后 C 再次运行(依然行程值最小)，行程值增加到 120。现在运行A，更新它的行程值为 200(现在与 B 相同)。然后 C 再次连续运行两次，行程值也变为 200。此时，所有行程值再次相等，这个过程会无限地重复下去。

* 既然有了可以精确控制的步长调度算法，为什么还要彩票调度算法呢?彩票调度有一个步长调度没有的优势——不需要全局状态。假如一个新的进程在上面的步长调度执行过程中加入系统，应该怎么设置它的行程值呢?设置成 0 吗?这样的话，它就独占 CPU 了。而彩票调度算法不需要对每个进程记录全局状态，只需要用新进程的票数更新全局的总票数就可以了。因此彩票调度算法能够更合理地处理新加入的进程。
* 彩票调度和步长调度这两种方式都不能很好地适合 I/O 且最难的票数分配问题并没有确定的解决方式。比例份额调度程序只有在这些问题可以相对容易解决的领域更有用(例如容易确定份额比例)。例如在虚拟数据中心中，你可能会希望分配 1/4 的 CPU 周期给 Windows 虚拟机，剩余的给 Linux 系统，比例分配的方式可以更简单高效。

#### 多处理器调度

* 过去很多年，多处理器系统只存在于高端服务器中。现在，它们越来越多地出现在个人 PC、笔记本电脑甚至移动设备上。多核处理器将多个 CPU核组装在一块芯片上，是这种扩散的根源。由于计算机的架构师们当时难以让单核 CPU 更快，同时又不增加太多功耗，所以这种多核 CPU 很快就变得流行。
* 多核 CPU 带来了许多困难。主要困难是典型的应用程序(例如你写的很多 C 程序)都只使用一个 CPU，增加了更多的 CPU 并没有让这类程序运行得更快。为了解决这个问题，不得不重写这些应用程序，使之能并行执行，也许使用多线程。多线程应用可以将工作分散到多个 CPU 上，因此 CPU资源越多就运行越快。
* 除了应用程序，操作系统遇到的一个新的问题是多处理器调度。


##### 多处理器架构 

* 多处理器与单 CPU 之间的基本的核心在于对硬件缓存的使用，以及多处理器之间共享数据的方式。在单 CPU 系统中，存在多级的硬件缓存，一般来说会让处理器更快地执行程序。缓存是很小但很快的存储设备，通常拥有内存中最热的数据的备份。相比之下，内存很大且拥有所有的数据，但访问速度较慢。通过将频繁访问的数据放在缓存中，系统似乎拥有又大又快的内存。
* 举个例子，假设一个程序需要从内存中加载指令并读取一个值，系统只有一个 CPU，拥有较小的缓存(如 64KB)和较大的内存。程序第一次读取数据时，数据在内存中，因此需要花费较长的时间(可能数十或数百纳秒)。处理器判断该数据很可能会被再次使用，因此将其放入 CPU 缓存中。如果之后程序再次需要使用同样的数据，CPU 会先查找缓存。因为在缓存中找到了数据，所以取数据快得多(比如几纳秒)，程序也就运行更快。
* 缓存是基于局部性的概念，局部性有两种，即时间局部性和空间局部性。由于这两种局部性存在于大多数的程序中，硬件系统可以很好地预测哪些数据可以放入缓存，从而运行得很好。
  * 时间局部性是指当一个数据被访问后，它很有可能会在不久的将来被再次访问，比如循环代码中的数据或指令本身。
  * 空间局部性指的是，当程序访问地址为 x 的数据时，很有可能会紧接着访问 x 周围的数据，比如遍历数组或指令的顺序执行。
* 事实证明，多 CPU 的情况下缓存要复杂得多。例如，假设一个运行在 CPU 1 上的程序从内存地址 A 读取数据。由于不在 CPU 1 的缓存中，所以系统直接访问内存，得到值 D。程序然后修改了地址 A 处的值，只是将它的缓存更新为新值 D'。将数据写回内存比较慢，因此系统(通常)会稍后再做。假设这时操作系统中断了该程序的运行，并将其交给 CPU 2，重新读取地址 A 的数据，由于 CPU 2 的缓存中并没有该数据，所以会直接从内存中读取，得到了旧值 *D*，而不是正确的值 D'。哎呀! 这一普遍的问题称为缓存一致性(cache coherence)问题。
* 硬件提供了缓存一致性问题的基本解决方案：通过监控内存访问，硬件可以保证获得正确的数据，并保证共享内存的唯一性。在基于总线的系统中，一种方式是使用总线窥探。
* 每个缓存都通过监听链接所有缓存和内存的总线，来发现内存访问。如果 CPU 发现对它放在缓存中的数据的更新，会作废本地副本(从缓存中移除)，或更新它(修改为新值)。回写缓存。
* 既然缓存已经做了这么多工作来提供一致性，应用程序(或操作系统)还需要关心共享数据的访问吗?依然需要! 跨 CPU 访问(尤其是写入)共享数据或数据结构时，需要使用互斥原语(比如锁)，才能保证正确性。我们会看到，这里依然有问题，尤其是性能方面。具体来说，随着 CPU数量的增加，访问同步共享的数据结构会变得很慢。
* 在设计多处理器调度时遇到的最后一个问题，是所谓的缓存亲和度(cache affinity)。这个概念很简单：一个进程在某个 CPU 上运行时，会在该 CPU 的缓存中维护许多状态。下次该进程在相同 CPU 上运行时，由于缓存中的数据而执行得更快。相反，在不同的 CPU 上执行，会由于需要重新加载数据而很慢(好在硬件保证的缓存一致性可以保证正确执行)。因此多处理器调度应该考虑到这种缓存亲和性，并尽可能将进程保持在同一个 CPU 上。

##### 单队列调度 

* 将所有需要调度的工作放入一个单独的队列中，我们称之为单队列多处理器调度(Single Queue Multiprocessor Scheduling，SQMS)。
* 这个方法最大的优点是简单。它不需要太多修改，就可以将原有的策略用于多个 CPU，选择最适合的工作来运行(例如，如果有两个 CPU，它可能选择两个最合适的工作)。
* 缺乏可扩展性。为了保证在多 CPU 上正常运行，调度程序的开发者需要在代码中通过加锁来保证原子性，如上所述。在 SQMS 访问单个队列时(如寻找下一个运行的工作)，锁确保得到正确的结果。然而，锁可能带来巨大的性能损失，尤其是随着系统中的 CPU 数增加时。随着这种单个锁的争用增加，系统花费了越来越多的时间在锁的开销上，较少的时间用于系统应该完成的工作。
* 不能很好地保证缓存亲和度。

##### 多队列调度 

* 由于单队列调度程序存在的问题，有些系统使用了多队列的方案，比如每个 CPU 一个队列。我们称之为多队列多处理器调度(Multi-Queue Multiprocessor Scheduling，MQMS)。
*  在 MQMS 中，基本调度框架包含多个调度队列，每个队列可以使用不同的调度规则，比如轮转或其他任何可能的算法。当一个工作进入系统后，系统会依照一些启发性规则(如随机或选择较空的队列)将其放入某个调度队列。这样一来，每个 CPU 调度之间相互独立，就避免了单队列的方式中由于数据共享及同步带来的问题。
* MQMS 比 SQMS 有明显的优势，它天生更具有可扩展性。队列的数量会随着 CPU 的增加而增加，因此锁和缓存争用的开销不是大问题。此外，MQMS 天生具有良好的缓存亲和度。所有工作都保持在固定的 CPU 上，因而可以很好地利用缓存数据。
* 存在负载不均(load imbalance)问题。

* 如何应对负载不均?
  * 最明显的答案是让工作移动，这种技术我们称为迁移。通过工作的跨 CPU迁移，可以真正实现负载均衡。
  * 一个基本的方法是采用一种技术，名为工作窃取(work stealing)。通过这种方法，工作量较少的(源)队列不定期地"偷看"其他(目标)队列是不是比自己的工作多。如果目标队列比源队列(显著地)更满，就从目标队列"窃取"一个或多个工作，实现负载均衡。当然，这种方法也有让人抓狂的地方——如果太频繁地检查其他队列，就会带来较高的开销，可扩展性不好，而这是多队列调度最初的全部目标!相反，如果检查间隔太长，又可能会带来严重的负载不均。
* 在构建多处理器调度程序方面，Linux 社区一直没有达成共识。一直以来，存在 3 种不同的调度程序：O(1)调度程序、完全公平调度程序(CFS)以及 BF 调度程序(BFS) 。O(1)，CFS 采用多队列，而 BFS 采用单队列，这说明两种方法都可以成功。当然它们之间还有很多不同的细节。例如，O(1)调度程序是基于优先级的(类似于之前介绍的 MLFQ)，随时间推移改变进程的优先级，然后调度最高优先级进程，来实现各种调度目标。交互性得到了特别关注。与之不同，CFS 是确定的比例调度方法(类似之前介绍的步长调度)。BFS作为三个算法中唯一采用单队列的算法，也基于比例调度，但采用了更复杂的方案，称为最早最合适虚拟截止时间优先算法(EEVEF)读者可以自己去了解这些现代操作系统的调度算法，现在应该能够理解它们的工作原理了!