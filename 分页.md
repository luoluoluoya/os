* 有时候人们会说，操作系统有两种方法，来解决大多数空间管理问题。第一种是将空
  间分割成不同长度的分片，就像虚拟内存管理中的分段。遗憾的是，这个解决方法存在固
  有的问题。具体来说，将空间切成不同长度的分片以后，空间本身会碎片化（fragmented），
  随着时间推移，分配内存会变得比较困难。
* 因此，值得考虑第二种方法：将空间分割成固定长度的分片。在虚拟内存中，我们称
  这种思想为分页，可以追溯到一个早期的重要系统，Atlas。分页不是将一个进
  程的地址空间分割成几个不同长度的逻辑段（即代码、堆、段），而是分割成固定大小的单
  元，每个单元称为一页。相应地，我们把物理内存看成是定长槽块的阵列，叫作页帧（page
  frame）。每个这样的页帧包含一个虚拟内存页。我
* 如何通过页来实现虚拟内存
  如何通过页来实现虚拟内存，从而避免分段的问题？基本技术是什么？如何让这些技术运行良好，
  并尽可能减少空间和时间开销？
  
* 为了让该方法看起来更清晰，我们用一个简单例子来说明。图 18.1 展示了一个只有 64
  字节的小地址空间，有 4 个 16 字节的页（虚拟页 0、1、2、3）。真实的地址空间肯定大得
  多，通常 32 位有 4GB 的地址空间，甚至有 64 位①。在本书中，我们常常用小例子，让大家
  更容易理解。
  物理内存，如图 18.2 所示，也由一组固定大小的槽块组成。在这个例子中，有 8 个页
  帧（由 128 字节物理内存构成，也是极小的）。从图中可以看出，虚拟地址空间的页放在物
  理内存的不同位置。图中还显示，操作系统自己用了一些物理内存。
  可以看到，与我们以前的方法相比，分页有许多优点。可能最大的改进就是灵活性：
  通过完善的分页方法，操作系统能够高效地提供地址空间的抽象，不管进程如何使用地址
  空间。例如，我们不会假定堆和栈的增长方向，以及它们如何使用。
  另一个优点是分页提供的空闲空间管理的简单性。例如，如果操作系统希望将 64 字节
  的小地址空间放到 8 页的物理地址空间中，它只要找到 4 个空闲页。也许操作系统保存了
  一个所有空闲页的空闲列表（free list），只需要从这个列表中拿出 4 个空闲页。在这个例子
  里，操作系统将地址空间的虚拟页 0 放在物理页帧 3，虚拟页 1 放在物理页帧 7，虚拟页 2
  放在物理页帧 5，虚拟页 3 放在物理页帧 2。页帧 1、4、6 目前是空闲的。
  [!image](./images/19.png)
  为了记录地址空间的每个虚拟页放在物理内存中的位置，操作系统通常为每个进程保
  存一个数据结构，称为页表（page table）。页表的主要作用是为地址空间的每个虚拟页面保
  存地址转换（address translation），从而让我们知道每个页在物理内存中的位置。对于我们的
  简单示例（见图 18.2），页表因此具有以下 4 个条目：（虚拟页 0→物理帧 3）、（VP 1→PF 7）、
  （VP 2→PF 5）和（VP 3→PF 2）。
  重要的是要记住，这个页表是一个每进程的数据结构（我们讨论的大多数页表结构都
  是每进程的数据结构，我们将接触的一个例外是倒排页表，inverted page table）。如果在上
  面的示例中运行另一个进程，操作系统将不得不为它管理不同的页表，因为它的虚拟页显
  然映射到不同的物理页面（除了共享之外）。
  现在，我们了解了足够的信息，可以完成一个地址转换的例子。设想拥有这个小地址
  空间（64 字节）的进程正在访问内存：
  movl <virtual address>, %eax
  具体来说，注意从地址<virtual address>到寄存器 eax 的数据显式加载（因此忽略之前肯
  定会发生的指令获取）。
  为了转换（translate）该过程生成的虚拟地址，我们必须首先将它分成两个组件：虚拟
  页面号（virtual page number，VPN）和页内的偏移量（offset）。对于这个例子，因为进程的
  虚拟地址空间是 64 字节，我们的虚拟地址总共需要 6 位（26
   = 64）。因此，虚拟地址可以表
  示如下：  
  [!image](./images/20.png)
在该图中，Va5 是虚拟地址的最高位，Va0 是最低位。因为我们知道页的大小（16 字节），
所以可以进一步划分虚拟地址，如下所示：
  [!image](./images/21.png)
  页面大小为 16 字节，位于 64 字节的地址空间。因此我们需要能够选择 4 个页，地址
  的前 2 位就是做这件事的。因此，我们有一个 2 位的虚拟页号（VPN）。其余的位告诉我们，
  感兴趣该页的哪个字节，在这个例子中是 4 位，我们称之为偏移量。
  当进程生成虚拟地址时，操作系统和硬件必须协作，将它转换为有意义的物理地址。
  例如，让我们假设上面的加载是虚拟地址 21：
  movl 21, %eax
  将“21”变成二进制形式，是“010101”，因此我们可以检查这个虚拟地址，看看它是
  如何分解成虚拟页号（VPN）和偏移量的：
  [!image](./images/22.png)
  因此，虚拟地址“21”在虚拟页“01”（或 1）的第 5 个（“0101”）字节处。通过虚拟
  页号，我们现在可以检索页表，找到虚拟页 1 所在的物理页面。在上面的页表中，物理帧
  号（PFN）（有时也称为物理页号，physical page number 或 PPN）是 7（二进制 111）。因此，
  我们可以通过用 PFN 替换 VPN 来转换此虚拟地址，
  然后将载入发送给物理内存（见图 18.3）。
  请注意，偏移量保持不变（即未翻译），因为偏
  移量只是告诉我们页面中的哪个字节是我们想要的。
  我们的最终物理地址是 1110101（十进制 117），正是
  我们希望加载指令（见图 18.2）获取数据的地方。
  有了这个基本概念，我们现在可以询问（希望也
  可以回答）关于分页的一些基本问题。例如，这些页
  表在哪里存储？页表的典型内容是什么，表有多大？
  分页是否会使系统变（得很）慢？这些问题和其他迷
  人的问题（至少部分）在下文中回答。请继续阅读！
* 页表存在哪里
  页表可以变得非常大，比我们之前讨论过的小段表或基址/界限对要大得多。例如，想
  象一个典型的 32 位地址空间，带有 4KB 的页。这个虚拟地址分成 20 位的 VPN 和 12 位的
  偏移量（回想一下，1KB 的页面大小需要 10 位，只需增加两位即可达到 4KB）。
  一个 20 位的 VPN 意味着，操作系统必须为每个进程管理 220个地址转换（大约一百万）。
  假设每个页表格条目（PTE）需要 4 个字节，来保存物理地址转换和任何其他有用的东西，
  每个页表就需要巨大的 4MB 内存！这非常大。现在想象一下有 100 个进程在运行：这意味
  着操作系统会需要 400MB 内存，只是为了所有这些地址转换！即使是现在，机器拥有千兆
  字节的内存，将它的一大块仅用于地址转换，这似乎有点疯狂，不是吗？我们甚至不敢想
  64 位地址空间的页表有多大。那太可怕了，也许把你吓坏了。
  
* 让我们来谈谈页表的组织。页表就是一种数据结构，用于将虚拟地址（或者实际上，
  是虚拟页号）映射到物理地址（物理帧号）。因此，任何数据结构都可以采用。最简单的形
  式称为线性页表（linear page table），就是一个数组。操作系统通过虚拟页号（VPN）检索
  该数组，并在该索引处查找页表项（PTE），以便找到期望的物理帧号（PFN）。现在，我们
  将假设采用这个简单的线性结构。在后面的章节中，我们将利用更高级的数据结构来帮助
  解决一些分页问题。
* 至于每个 PTE 的内容，我们在其中有许多不同的位，值得有所了解。有效位（valid bit）
  通常用于指示特定地址转换是否有效。例如，当一个程序开始运行时，它的代码和堆在其
  地址空间的一端，栈在另一端。所有未使用的中间空间都将被标记为无效（invalid），如果
  进程尝试访问这种内存，就会陷入操作系统，可能会导致该进程终止。因此，有效位对于
  支持稀疏地址空间至关重要。通过简单地将地址空间中所有未使用的页面标记为无效，我
  们不再需要为这些页面分配物理帧，从而节省大量内存。
* 我们还可能有保护位（protection bit），表明页是否可以读取、写入或执行。同样，以这
  些位不允许的方式访问页，会陷入操作系统。
  还有其他一些重要的部分，但现在我们不会过多讨论。存在位（present bit）表示该页
  是在物理存储器还是在磁盘上（即它已被换出，swapped out）。当我们研究如何将部分地址
  空间交换（swap）到磁盘，从而支持大于物理内存的地址空间时，我们将进一步理解这一
  机制。交换允许操作系统将很少使用的页面移到磁盘，从而释放物理内存。脏位（dirty bit）
  也很常见，表明页面被带入内存后是否被修改过。
  参考位（reference bit，也被称为访问位，accessed bit）有时用于追踪页是否被访问，也
  用于确定哪些页很受欢迎，因此应该保留在内存中。这些知识在页面替换（page replacement）
  时非常重要
*  18.5 显示了来自 x86 架构的示例页表项[I09]。它包含一个存在位（P），确定是否允
  许写入该页面的读/写位（R/W） 确定用户模式进程是否可以访问该页面的用户/超级用户位
  （U/S），有几位（PWT、PCD、PAT 和 G）确定硬件缓存如何为这些页面工作，一个访问位
  （A）和一个脏位（D），最后是页帧号（PFN）本身。
  [!image](./images/23.png)

* 内存中的页表，我们已经知道它们可能太大了。事实证明，它们也会让速度变慢。以
  简单的指令为例：
  movl 21, %eax
  同样，我们只看对地址 21 的显式引用，而不关心指令获取。在这个例子中，我们假定
  硬件为我们执行地址转换。要获取所需数据，系统必须首先将虚拟地址（21）转换为正确
  的物理地址（117）。因此，在从地址 117 获取数据之前，系统必须首先从进程的页表中提取
  适当的页表项，执行转换，然后从物理内存中加载数据。
  为此，硬件必须知道当前正在运行的进程的页表的位置。现在让我们假设一个页表基
  址寄存器（page-table base register）包含页表的起始位置的物理地址。为了找到想要的 PTE
  的位置，硬件将执行以下功能：
  VPN = (VirtualAddress & VPN_MASK) >> SHIFT
  PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE))
  在我们的例子中，VPN MASK 将被设置为 0x30（十六进制 30，或二进制 110000），它
  从完整的虚拟地址中挑选出 VPN 位；SHIFT 设置为 4（偏移量的位数），这样我们就可以将
  VPN 位向右移动以形成正确的整数虚拟页码。例如，使用虚拟地址 21（010101），掩码将
  此值转换为 010000，移位将它变成 01，或虚拟页 1，正是我们期望的值。然后，我们使用
  该值作为页表基址寄存器指向的 PTE 数组的索引。
  一旦知道了这个物理地址，硬件就可以从内存中获取 PTE，提取 PFN，并将它与来自
  虚拟地址的偏移量连接起来，形成所需的物理地址。具体来说，你可以想象 PFN 被 SHIFT
  左移，然后与偏移量进行逻辑或运算，以形成最终地址
offset = VirtualAddress & OFFSET_MASK
PhysAddr = (PFN << SHIFT) | offset
1 // Extract the VPN from the virtual address
2 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
3
4 // Form the address of the page-table entry (PTE)
5 PTEAddr = PTBR + (VPN * sizeof(PTE))
6
7 // Fetch the PTE 
18.5 内存追踪 137
8 PTE = AccessMemory(PTEAddr)
9
10 // Check if process can access the page
11 if (PTE.Valid == False)
12 RaiseException(SEGMENTATION_FAULT)
13 else if (CanAccess(PTE.ProtectBits) == False)
14 RaiseException(PROTECTION_FAULT)
15 else
16 // Access is OK: form physical address and fetch it
17 offset = VirtualAddress & OFFSET_MASK
18 PhysAddr = (PTE.PFN << PFN_SHIFT) | offset
19 Register = AccessMemory(PhysAddr)
最后，硬件可以从内存中获取所需的数据并将其放入寄存器 eax。程序现在已成功从内
存中加载了一个值！
总之，我们现在描述了在每个内存引用上发生的情况的初始协议。基本方法如图 18.6
所示。对于每个内存引用（无论是取指令还是显式加载或存储），分页都需要我们执行一个
额外的内存引用，以便首先从页表中获取地址转换。工作量很大！额外的内存引用开销很
大，在这种情况下，可能会使进程减慢两倍或更多。
现在你应该可以看到，有两个必须解决的实际问题。如果不仔细设计硬件和软件，页
表会导致系统运行速度过慢，并占用太多内存。虽然看起来是内存虚拟化需求的一个很好
的解决方案，但这两个关键问题必须先克服

内存追踪
在结束之前，我们现在通过一个简单的内存访问示例，来演示使用分页时产生的所有
内存访问。我们感兴趣的代码片段（用 C 写的，名为 array.c）是这样的：
int array\[1000];
...
for (i = 0; i < 1000; i++)
 array\[i] = 0;
我们编译 array.c 并使用以下命令运行它：
补充：数据结构——页表
现代操作系统的内存管理子系统中最重要的数据结构之一就是页表（page table）。通常，页表存储
虚拟—物理地址转换（virtual-to-physical address translation），从而让系统知道地址空间的每个页实际驻
留在物理内存中的哪个位置。由于每个地址空间都需要这种转换，因此一般来说，系统中每个进程都有
一个页表。页表的确切结构要么由硬件（旧系统）确定，要么由 OS（现代系统）更灵活地管理。
当然，为了真正理解这个代码片段（它只是初始化一个数组）进程怎样的内存访问，
我们必须知道（或假设）一些东西。首先，我们必须反汇编结果二进制文件（在 Linux 上使
用 objdump 或在 Mac 上使用 otool），查看使用什么汇编指令来初始化循环中的数组。以下
是生成的汇编代码：
0x1024 movl $0x0,(%edi,%eax,4)
0x1028 incl %eax
0x102c cmpl $0x03e8,%eax
0x1030 jne 0x1024
如果懂一点 x86，代码实际上很容易理解①。第一条指令将零值（显示为$0x0）移动到
数组位置的虚拟内存地址，这个地址是通过取%edi 的内容并将其加上%eax 乘以 4 来计算的。
因此，%edi 保存数组的基址，而%eax 保存数组索引（i）。我们乘以 4，因为数组是一个整
型数组，每个元素的大小为 4 个字节。
第二条指令增加保存在%eax 中的数组索引，第三条指令将该寄存器的内容与十六进制
值 0x03e8 或十进制数 1000 进行比较。如果比较结果显示两个值不相等（这就是 jne 指令测
试），第四条指令跳回到循环的顶部。
为了理解这个指令序列（在虚拟层和物理层）所访问的内存，我们必须假设虚拟内存
中代码片段和数组的位置，以及页表的内容和位置。
对于这个例子，我们假设一个大小为 64KB 的虚拟地址空间（不切实际地小）。我们还
假定页面大小为 1KB。
我们现在需要知道页表的内容，以及它在物理内存中的位置。假设有一个线性（基于
数组）的页表，它位于物理地址 1KB（1024）。
我们已经引入了分页（paging）的概念，作为虚拟内存挑战的解决方案。与以前的方法
（如分段）相比，分页有许多优点。首先，它不会导致外部碎片，因为分页（按设计）将内
存划分为固定大小的单元。其次，它非常灵活，支持稀疏虚拟地址空间。
然而，实现分页支持而不小心考虑，会导致较慢的机器（有许多额外的内存访问来访问
页表）和内存浪费（内存被页表塞满而不是有用的应用程序数据）。因此，我们不得不努力想
出一个分页系统，它不仅可以工作，而且工作得很好。幸运的是，接下来的两章将告诉我们
如何去做。
我们现在准备好跟踪程序的内存引用了。当它运行时，每个获取指将产生两个内存引
用：一个访问页表以查找指令所在的物理框架，另一个访问指令本身将其提取到 CPU 进行
处理。另外，在 mov 指令的形式中，有一个显式的内存引用，这会首先增加另一个页表访
问（将数组虚拟地址转换为正确的物理地址），然后时数组访问本身。

使用分页作为核心机制来实现虚拟内存，可能会带来较高的性能开销。因为要使用分
页，就要将内存地址空间切分成大量固定大小的单元（页），并且需要记录这些单元的地址
映射信息。因为这些映射信息一般存储在物理内存中，所以在转换虚拟地址时，分页逻辑
上需要一次额外的内存访问。每次指令获取、显式加载或保存，都要额外读一次内存以得
到转换信息，这慢得无法接受。
因此我们面临如下问题：
关键问题：如何加速地址转换
如何才能加速虚拟地址转换，尽量避免额外的内存访问？需要什么样的硬件支持？操作系统该如何
支持？

想让某些东西更快，操作系统通常需要一些帮助。帮助常常来自操作系统的老朋友：硬
件。我们要增加所谓的（由于历史原因）地址转换旁路缓冲存储器（translation-lookaside
buffer，TLB），它就是频繁发生的虚拟到物理地址转换的硬件缓存（cache）。因
此，更好的名称应该是地址转换缓存（address-translation cache）。对每次内存访问，硬件先
检查 TLB，看看其中是否有期望的转换映射，如果有，就完成转换（很快），不用访问页表
（其中有全部的转换映射）。TLB 带来了巨大的性能提升，实际上，因此它使得虚拟内存成
为可能。

TLB 的基本算法
图 19.1 展示了一个大体框架，说明硬件如何处理虚拟地址转换，假定使用简单的线性
页表（linear page table，即页表是一个数组）和硬件管理的 TLB（hardware-managed TLB，
即硬件承担许多页表访问的责任，下面会有更多解释）。
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
2 (Success, TlbEntry) = TLB_Lookup(VPN)
3 if (Success == True) // TLB Hit
4 if (CanAccess(TlbEntry.ProtectBits) == True)
5 Offset = VirtualAddress & OFFSET_MASK
6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
7 AccessMemory(PhysAddr)
8 else
9 RaiseException(PROTECTION_FAULT)
10 else // TLB Miss
11 PTEAddr = PTBR + (VPN * sizeof(PTE))
12 PTE = AccessMemory(PTEAddr) 
19.2 示例：访问数组 143
13 if (PTE.Valid == False)
14 RaiseException(SEGMENTATION_FAULT)
15 else if (CanAccess(PTE.ProtectBits) == False)
16 RaiseException(PROTECTION_FAULT)
17 else
18 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
19 RetryInstruction()
图 19.1 TLB 控制流算法
硬件算法的大体流程如下：首先从虚拟地址中提取页号（VPN）（见图 19.1 第 1 行），
然后检查 TLB 是否有该 VPN 的转换映射（第 2 行）。如果有，我们有了 TLB 命中（TLB hit），
这意味着 TLB 有该页的转换映射。成功！接下来我们就可以从相关的 TLB 项中取出页帧号
（PFN），与原来虚拟地址中的偏移量组合形成期望的物理地址（PA），并访问内存（第 5～7
行），假定保护检查没有失败（第 4 行）。
如果 CPU 没有在 TLB 中找到转换映射（TLB 未命中），我们有一些工作要做。在本例
中，硬件访问页表来寻找转换映射（第 11～12 行），并用该转换映射更新 TLB（第 18 行），
假设该虚拟地址有效，而且我们有相关的访问权限（第 13、15 行）。上述系列操作开销较
大，主要是因为访问页表需要额外的内存引用（第 12 行）。最后，当 TLB 更新成功后，系
统会重新尝试该指令，这时 TLB 中有了这个转换映射，内存引用得到很快处理。
TLB 和其他缓存相似，前提是在一般情况下，转换映射会在缓存中（即命中）。如果是
这样，只增加了很少的开销，因为 TLB 处理器核心附近，设计的访问速度很快。如果 TLB
未命中，就会带来很大的分页开销。必须访问页表来查找转换映射，导致一次额外的内存
引用（或者更多，如果页表更复杂）。如果这经常发生，程序的运行就会显著变慢。相对于
大多数 CPU 指令，内存访问开销很大，TLB 未命中导致更多内存访问。因此，我们希望尽
可能避免 TLB 未命中。

示例：访问数组
为了弄清楚 TLB 的操作，我们来看一个简单虚拟地址
追踪，看看 TLB 如何提高它的性能。在本例中，假设有一
个由 10 个 4 字节整型数组成的数组，起始虚地址是 100。
进一步假定，有一个 8 位的小虚地址空间，页大小为 16B。
我们可以把虚地址划分为 4 位的 VPN（有 16 个虚拟内存
页）和 4 位的偏移量（每个页中有 16 个字节）。
图 19.2 展示了该数组的布局，在系统的 16 个 16 字节
的页上。如你所见，数组的第一项（a[0]）开始于（VPN=06，
offset=04），只有 3 个 4 字节整型数存放在该页。数组在下
一页（VPN=07）继续，其中有接下来 4 项（a[3] … a[6]）。
10 个元素的数组的最后 3 项（a[7] … a[9]）位于地址空间
的下一页（VPN=08）。
[!image](./images/24.png)
现在考虑一个简单的循环，访问数组中的每个元素，类似下面的 C 程序：
int sum = 0;
for (i = 0; i < 10; i++) {
 sum += a[i];
}
简单起见，我们假装循环产生的内存访问只是针对数组（忽略变量 i 和 sum，以及指令
本身）。当访问第一个数组元素（a[0]）时，CPU 会看到载入虚存地址 100。硬件从中提取
VPN（VPN=06），然后用它来检查 TLB，寻找有效的转换映射。假设这里是程序第一次访
问该数组，结果是 TLB 未命中。
接下来访问 a[1]，这里有好消息：TLB 命中！因为数组的第二个元素在第一个元素之
后，它们在同一页。因为我们之前访问数组的第一个元素时，已经访问了这一页，所以 TLB
中缓存了该页的转换映射。因此成功命中。访问 a[2]同样成功（再次命中），因为它和 a[0]、
a[1]位于同一页。
遗憾的是，当程序访问 a[3]时，会导致 TLB 未命中。但同样，接下来几项（a[4] … a[6]）
都会命中 TLB，因为它们位于内存中的同一页。
最后，访问 a[7]会导致最后一次 TLB 未命中。系统会再次查找页表，弄清楚这个虚拟
页在物理内存中的位置，并相应地更新 TLB。最后两次访问（a[8]、a[9]）受益于这次 TLB
更新，当硬件在 TLB 中查找它们的转换映射时，两次都命中。
我们来总结一下这 10 次数组访问操作中 TLB 的行为表现：未命中、命中、命中、未命
中、命中、命中、命中、未命中、命中、命中。命中的次数除以总的访问次数，得到 TLB
命中率（hit rate）为 70%。尽管这不是很高（实际上，我们希望命中率接近 100%），但也不
是零，是零我们就要奇怪了。即使这是程序首次访问该数组，但得益于空间局部性（spatial
locality），TLB 还是提高了性能。数组的元素被紧密存放在几页中（即它们在空间中紧密相
邻），因此只有对页中第一个元素的访问才会导致 TLB 未命中。
也要注意页大小对本例结果的影响。如果页大小变大一倍（32 字节，而不是 16），数
组访问遇到的未命中更少。典型页的大小一般为 4KB，这种情况下，密集的、基于数组的
访问会实现极好的 TLB 性能，每页的访问只会遇到一次未命中。
关于 TLB 性能还有最后一点：如果在这次循环后不久，该程序再次访问该数组，我们
会看到更好的结果，假设 TLB 足够大，能缓存所需的转换映射：命中、命中、命中、命中、
命中、命中、命中、命中、命中、命中。在这种情况下，由于时间局部性（temporal locality），
即在短时间内对内存项再次引用，所以 TLB 的命中率会很高。类似其他缓存，TLB 的成功
依赖于空间和时间局部性。如果某个程序表现出这样的局部性（许多程序是这样），TLB 的
命中率可能很高。


尽可能利用缓存
缓存是计算机系统中最基本的性能改进技术之一，一次又一次地用于让“常见的情况更快”[HP06]。
硬件缓存背后的思想是利用指令和数据引用的局部性（locality）。通常有两种局部性：时间局部性
（temporal locality）和空间局部性（spatial locality）。时间局部性是指，最近访问过的指令或数据项可能
很快会再次访问。想想循环中的循环变量或指令，它们被多次反复访问。空间局部性是指，当程序访问
内存地址 x 时，可能很快会访问邻近 x 的内存。想想遍历某种数组，访问一个接一个的元素。当然，这
些性质取决于程序的特点，并不是绝对的定律，而更像是一种经验法则。
硬件缓存，无论是指令、数据还是地址转换（如 TLB），都利用了局部性，在小而快的芯片内存储
器中保存一份内存副本。处理器可以先检查缓存中是否存在就近的副本，而不是必须访问（缓慢的）内
存来满足请求。如果存在，处理器就可以很快地访问它（例如在几个 CPU 时钟内），避免花很多时间来
访问内存（好多纳秒）。
你可能会疑惑：既然像 TLB 这样的缓存这么好，为什么不做更大的缓存，装下所有的数据？可惜
的是，这里我们遇到了更基本的定律，就像物理定律那样。如果想要快速地缓存，它就必须小，因为光
速和其他物理限制会起作用。大的缓存注定慢，因此无法实现目的。所以，我们只能用小而快的缓存。
剩下的问题就是如何利用好缓存来提升性能。

谁来处理 TLB 未命中
有一个问题我们必须回答：谁来处理 TLB 未命中？可能有两个答案：硬件或软件（操
作系统）。以前的硬件有复杂的指令集（有时称为复杂指令集计算机，Complex-Instruction Set
Computer，CISC），造硬件的人不太相信那些搞操作系统的人。因此，硬件全权处理 TLB
未命中。为了做到这一点，硬件必须知道页表在内存中的确切位置（通过页表基址寄存器，
page-table base register，在图 19.1 的第 11 行使用），以及页表的确切格式。发生未命中时，
硬件会“遍历”页表，找到正确的页表项，取出想要的转换映射，用它更新 TLB，并重试
该指令。这种“旧”体系结构有硬件管理的 TLB，一个例子是 x86 架构，它采用固定的多
级页表（multi-level page table，详见第 20 章），当前页表由 CR3 寄存器指出。
更现代的体系结构（例如，MIPS R10k、Sun 公司的 SPARC v9，都是精简
指令集计算机，Reduced-Instruction Set Computer，RISC），有所谓的软件管理 TLB（softwaremanaged TLB）。发生 TLB 未命中时，硬件系统会抛出一个异常（见图 19.3 第 11 行），这会
暂停当前的指令流，将特权级提升至内核模式，跳转至陷阱处理程序（trap handler）。接下
来你可能已经猜到了，这个陷阱处理程序是操作系统的一段代码，用于处理 TLB 未命中。
这段代码在运行时，会查找页表中的转换映射，然后用特别的“特权”指令更新 TLB，并
从陷阱返回。此时，硬件会重试该指令（导致 TLB 命中）。
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT
2 (Success, TlbEntry) = TLB_Lookup(VPN)
3 if (Success == True) // TLB Hit
4 if (CanAccess(TlbEntry.ProtectBits) == True)
5 Offset = VirtualAddress & OFFSET_MASK
6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
7 Register = AccessMemory(PhysAddr)
8 else
9 RaiseException(PROTECTION_FAULT)
10 else // TLB Miss
11 RaiseException(TLB_MISS)
图 19.3 TLB 控制流算法（操作系统处理）

接下来讨论几个重要的细节。首先，这里的从陷阱返回指令稍稍不同于之前提到的服
务于系统调用的从陷阱返回。在后一种情况下，从陷阱返回应该继续执行陷入操作系统之
后那条指令，就像从函数调用返回后，会继续执行此次调用之后的语句。在前一种情况下，
在从 TLB 未命中的陷阱返回后，硬件必须从导致陷阱的指令继续执行。这次重试因此导致
该指令再次执行，但这次会命中 TLB。因此，根据陷阱或异常的原因，系统在陷入内核时
必须保存不同的程序计数器，以便将来能够正确地继续执行。
第二，在运行 TLB 未命中处理代码时，操作系统需要格外小心避免引起 TLB 未命中的
无限递归。有很多解决方案，例如，可以把 TLB 未命中陷阱处理程序直接放到物理内存中 \[它
们没有映射过（unmapped），不用经过地址转换]。或者在 TLB 中保留一些项，记录永久有
效的地址转换，并将其中一些永久地址转换槽块留给处理代码本身，这些被监听的（wired）
地址转换总是会命中 TLB。
软件管理的方法，主要优势是灵活性：操作系统可以用任意数据结构来实现页表，不
需要改变硬件。另一个优势是简单性。从 TLB 控制流中可以看出（见图 19.3 的第 11 行，
对比图 19.1 的第 11～19 行），硬件不需要对未命中做太多工作，它抛出异常，操作系统的
未命中处理程序会负责剩下的工作。

RISC 与 CISC
在 20 世纪 80 年代，计算机体系结构领域曾发生过一场激烈的讨论。一方是 CISC 阵营，即复杂指令集
计算（Complex Instruction Set Computing），另一方是 RISC，即精简指令集计算（Reduced Instruction Set
Computing）。RISC 阵营以 Berkeley 的 David Patterson 和 Stanford 的 John Hennessy 为代表（他们写
了一些非常著名的书），尽管后来 John Cocke 凭借他在 RISC 上的早期工作获得了图灵奖。
CISC 指令集倾向于拥有许多指令，每条指令比较强大。例如，你可能看到一个字符串拷贝，它接
受两个指针和一个长度，将一些字节从源拷贝到目标。CISC 背后的思想是，指令应该是高级原语，这
让汇编语言本身更易于使用，代码更紧凑。
RISC 指令集恰恰相反。RISC 背后的关键观点是，指令集实际上是编译器的最终目标，所有编译
器实际上需要少量简单的原语，可以用于生成高性能的代码。因此，RISC 倡导者们主张，尽可能从硬
件中拿掉不必要的东西（尤其是微代码），让剩下的东西简单、统一、快速。
早期的 RISC 芯片产生了巨大的影响，因为它们明显更快。人们写了很多论文，一些相关的
公司相继成立（例如 MIPS 和 Sun 公司）。但随着时间的推移，像 Intel 这样的 CISC 芯片制造商采纳
了许多 RISC 芯片的优点，例如添加了早期流水线阶段，将复杂的指令转换为一些微指令，于是它们可
以像 RISC 的方式运行。这些创新，加上每个芯片中晶体管数量的增长，让 CISC 保持了竞争力。争论
最后平息了，现在两种类型的处理器都可以跑得很快。

TLB 的内容
我们来详细看一下硬件 TLB 中的内容。典型的 TLB 有 32 项、64 项或 128 项，并且是
全相联的（fully associative）。基本上，这就意味着一条地址映射可能存在 TLB 中的任意位
置，硬件会并行地查找 TLB，找到期望的转换映射。一条 TLB 项内容可能像下面这样：
 VPN ｜ PFN ｜ 其他位
注意，VPN 和 PFN 同时存在于 TLB 中，因为一条地址映射可能出现在任意位置（用硬件的
术语，TLB 被称为全相联的（fully-associative）缓存）。硬件并行地查找这些项，看看是否有匹配。
补充：TLB 的有效位!=页表的有效位
常见的错误是混淆 TLB 的有效位和页表的有效位。在页表中，如果一个页表项（PTE）被标记为无
效，就意味着该页并没有被进程申请使用，正常运行的程序不应该访问该地址。当程序试图访问这样的
页时，就会陷入操作系统，操作系统会杀掉该进程。
TLB 的有效位不同，只是指出 TLB 项是不是有效的地址映射。例如，系统启动时，所有的 TLB 项
通常被初始化为无效状态，因为还没有地址转换映射被缓存在这里。一旦启用虚拟内存，当程序开始运
行，访问自己的虚拟地址，TLB 就会慢慢地被填满，因此有效的项很快会充满 TLB。
TLB 有效位在系统上下文切换时起到了很重要的作用，后面我们会进一步讨论。通过将所有 TLB
项设置为无效，系统可以确保将要运行的进程不会错误地使用前一个进程的虚拟到物理地址转换映射。
更有趣的是“其他位”。例如，TLB 通常有一个有效（valid）位，用来标识该项是不是
有效地转换映射。通常还有一些保护（protection）位，用来标识该页是否有访问权限。例
如，代码页被标识为可读和可执行，而堆的页被标识为可读和可写。还有其他一些位，包
括地址空间标识符（address-space identifier）、脏位（dirty bit）等。下
上下文切换时对 TLB 的处理
有了 TLB，在进程间切换时（因此有地址空间切换），会面临一些新问题。具体来说，
TLB 中包含的虚拟到物理的地址映射只对当前进程有效，对其他进程是没有意义的。所以
在发生进程切换时，硬件或操作系统（或二者）必须注意确保即将运行的进程不要误读了
之前进程的地址映射。
为了更好地理解这种情况，我们来看一个例子。当一个进程（P1）正在运行时，假设
TLB 缓存了对它有效的地址映射，即来自 P1 的页表。对这个例子，假设 P1 的 10 号虚拟页
映射到了 100 号物理帧。
在这个例子中，假设还有一个进程（P2），操作系统不久后决定进行一次上下文切换，
运行 P2。这里假定 P2 的 10 号虚拟页映射到 170 号物理帧。如果这两个进程的地址映射都
在 TLB 中，TLB 的内容如表 19.1 所示
[!image](./images/25.png)
在上面的 TLB 中，很明显有一个问题：VPN 10 被转换成了 PFN 100（P1）和 PFN 170
148 第 19 章 分页：快速地址转换（TLB）
（P2），但硬件分不清哪个项属于哪个进程。所以我们还需要做一些工作，让 TLB 正确而高
效地支持跨多进程的虚拟化。因此，关键问题是：
关键问题：进程切换时如何管理 TLB 的内容
如果发生进程间上下文切换，上一个进程在 TLB 中的地址映射对于即将运行的进程是无意义的。
硬件或操作系统应该做些什么来解决这个问题呢？
这个问题有一些可能的解决方案。一种方法是在上下文切换时，简单地清空（flush）TLB，
这样在新进程运行前 TLB 就变成了空的。如果是软件管理 TLB 的系统，可以在发生上下文切
换时，通过一条显式（特权）指令来完成。如果是硬件管理 TLB，则可以在页表基址寄存器内
容发生变化时清空 TLB（注意，在上下文切换时，操作系统必须改变页表基址寄存器（PTBR）
的值）。不论哪种情况，清空操作都是把全部有效位（valid）置为 0，本质上清空了 TLB。
上下文切换的时候清空 TLB，这是一个可行的解决方案，进程不会再读到错误的地址
映射。但是，有一定开销：每次进程运行，当它访问数据和代码页时，都会触发 TLB 未命
中。如果操作系统频繁地切换进程，这种开销会很高。
为了减少这种开销，一些系统增加了硬件支持，实现跨上下文切换的 TLB 共享。比如
有的系统在 TLB 中添加了一个地址空间标识符（Address Space Identifier，ASID）。可以把
ASID 看作是进程标识符（Process Identifier，PID），但通常比 PID 位数少（PID 一般 32 位，
ASID 一般是 8 位）。
如果仍以上面的 TLB 为例，加上 ASID，很清楚不同进程可以共享 TLB 了：只要 ASID
字段来区分原来无法区分的地址映射。表 19.2 展示了添加 ASID 字段后的 TLB。
因此，有了地址空间标识符，TLB 可以同时缓存不同进程的地址空间映射，没有任何
冲突。当然，硬件也需要知道当前是哪个进程正在运行，以便进行地址转换，因此操作系
统在上下文切换时，必须将某个特权寄存器设置为当前进程的 ASID。
[!image](./images/26.png)
补充一下，你可能想到了另一种情况，TLB 中某两项非常相似。在表 19.3 中，属于两
个不同进程的两项，将两个不同的 VPN 指向了相同的物理页。
[!image](./images/27.png)
如果两个进程共享同一物理页（例如代码段的页），就可能出现这种情况。在上面的例
子中，进程 P1 和进程 P2 共享 101 号物理页，但是 P1 将自己的 10 号虚拟页映射到该物理
页，而 P2 将自己的 50 号虚拟页映射到该物理页。共享代码页（以二进制或共享库的方式）
是有用的，因为它减少了物理页的使用，从而减少了内存开销。

TLB 替换策略
TLB 和其他缓存一样，还有一个问题要考虑，即缓存替换（cache replacement）。具体来
说，向 TLB 中插入新项时，会替换（replace）一个旧项，这样问题就来了：应该替换那一个？
关键问题：如何设计 TLB 替换策略
在向 TLB 添加新项时，应该替换哪个旧项？目标当然是减小 TLB 未命中率（或提高命中率），从
而改进性能。
在讨论页换出到磁盘的问题时，我们将详细研究这样的策略。这里我们先简单指出几
个典型的策略。一种常见的策略是替换最近最少使用（least-recently-used，LRU）的项。LRU
尝试利用内存引用流中的局部性，假定最近没有用过的项，可能是好的换出候选项。另一
种典型策略就是随机（random）策略，即随机选择一项换出去。这种策略很简单，并且可
以避免一种极端情况。例如，一个程序循环访问 n+1 个页，但 TLB 大小只能存放 n 个页。
这时之前看似“合理”的 LRU 策略就会表现得不可理喻，因为每次访问内存都会触发 TLB
未命中，而随机策略在这种情况下就好很多。

7 实际系统的 TLB 表项
最后，我们简单看一下真实的 TLB。这个例子来自 MIPS R4000，它是一种现代
的系统，采用软件管理 TLB。图 19.4 展示了稍微简化的 MIPS TLB 项。
[!image](./images/28.png)
MIPS R4000 支持 32 位的地址空间，页大小为 4KB。所以在典型的虚拟地址中，预期
会看到 20 位的 VPN 和 12 位的偏移量。但是，你可以在 TLB 中看到，只有 19 位的 VPN。
事实上，用户地址只占地址空间的一半（剩下的留给内核），所以只需要 19 位的 VPN。VPN
转换成最大 24 位的物理帧号（PFN），因此可以支持最多有 64GB 物理内存（224个 4KB 内
存页）的系统。
MIPS TLB 还有一些有趣的标识位。比如全局位（Global，G），用来指示这个页是不是
所有进程全局共享的。因此，如果全局位置为 1，就会忽略 ASID。我们也看到了 8 位的 ASID，
150 第 19 章 分页：快速地址转换（TLB）
操作系统用它来区分进程空间（像上面介绍的一样）。这里有一个问题：如果正在运行的进
程数超过 256（28
）个怎么办？最后，我们看到 3 个一致性位（Coherence，C），决定硬件
如何缓存该页（其中一位超出了本书的范围）；脏位（dirty），表示该页是否被写入新数据（后
面会介绍用法）；有效位（valid），告诉硬件该项的地址映射是否有效。还有没在图 19.4 中
展示的页掩码（page mask）字段，用来支持不同的页大小。后面会介绍，为什么更大的页
可能有用。最后，64 位中有一些未使用（图 19.4 中灰色部分）。
MIPS 的 TLB 通常有 32 项或 64 项，大多数提供给用户进程使用，也有一小部分留给操
作系统使用。操作系统可以设置一个被监听的寄存器，告诉硬件需要为自己预留多少 TLB
槽。这些保留的转换映射，被操作系统用于关键时候它要使用的代码和数据，在这些时候，
TLB 未命中可能会导致问题（例如，在 TLB 未命中处理程序中）。
由于 MIPS 的 TLB 是软件管理的，所以系统需要提供一些更新 TLB 的指令。MIPS 提
供了 4 个这样的指令：TLBP，用来查找指定的转换映射是否在 TLB 中；TLBR，用来将 TLB
中的内容读取到指定寄存器中；TLBWI，用来替换指定的 TLB 项；TLBWR，用来随机替
换一个 TLB 项。操作系统可以用这些指令管理 TLB 的内容。当然这些指令是特权指令，这
很关键。如果用户程序可以任意修改 TLB 的内容，你可以想象一下会发生什么可怕的事情。
提示：RAM 不总是 RAM（Culler 定律）
随机存取存储器（Random-Access Memory，RAM）暗示你访问 RAM 的任意部分都一样快。虽然
一般这样想 RAM 没错，但因为 TLB 这样的硬件/操作系统功能，访问某些内存页的开销较大，尤其是
没有被 TLB 缓存的页。因此，最好记住这个实现的窍门：RAM 不总是 RAM。有时候随机访问地址空
间，尤其是 TLB 没有缓存的页，可能导致严重的性能损失。因为我的一位导师 David Culler 过去常常指
出 TLB 是许多性能问题的源头，所以我们以他来命名这个定律：Culler 定律（Culler’s Law）。

小结
我们了解了硬件如何让地址转换更快的方法。通过增加一个小的、芯片内的 TLB 作为
地址转换的缓存，大多数内存引用就不用访问内存中的页表了。因此，在大多数情况下，
程序的性能就像内存没有虚拟化一样，这是操作系统的杰出成就，当然对现代操作系统中
的分页非常必要。
但是，TLB 也不能满足所有的程序需求。具体来说，如果一个程序短时间内访问的页
数超过了 TLB 中的页数，就会产生大量的 TLB 未命中，运行速度就会变慢。这种现象被称
为超出 TLB 覆盖范围（TLB coverage），这对某些程序可能是相当严重的问题。解决这个问
题的一种方案是支持更大的页，把关键数据结构放在程序地址空间的某些区域，这些区域
被映射到更大的页，使 TLB 的有效覆盖率增加。对更大页的支持通常被数据库管理系统
（Database Management System，DBMS）这样的程序利用，它们的数据结构比较大，而且是
随机访问。



我们现在来解决分页引入的第二个问题：页表太大，因此消耗的内存太多。让我们从
线性页表开始。你可能会记得，线性页表变得相当大。假设一个 32 位地址空间，
4KB的页和一个 4 字节的页表项。一个地址空间中大约有一百万个虚拟页面
。乘以页表项的大小，你会发现页表大小为 4MB。回想一下：通常系统中的每个
进程都有一个页表！有一百个活动进程（在现代系统中并不罕见），就要为页表分配数百兆
的内存！因此，要寻找一些技术来减轻这种沉重的负担。

可以用一种简单的方法减小页表大小：使用更大的页。再以 32 位地址空间为例，但这
次假设用 16KB 的页。因此，会有 18 位的 VPN 加上 14 位的偏移量。假设每个页表项（4
字节）的大小相同，现在线性页表中有 218 个项，因此每个页表的总大小为 1MB，页表缩
到四分之一。

然而，这种方法的主要问题在于，大内存页会导致每页内的浪费，这被称为内部碎片（internal
fragmentation）问题（因为浪费在分配单元内部）。因此，结果是应用程序会分配页，但只用每
页的一小部分，而内存很快就会充满这些过大的页。因此，大多数系统在常见的情况下使用相
对较小的页大小：4KB（如 x86）或 8KB（如 SPARCv9）。

回忆在分段中，有一个基址（base）寄存器，告诉我们每个段在物理内存中的位
置，还有一个界限（bound）或限制（limit）寄存器，告诉我们该段的大小。在杂合方案中，
我们仍然在 MMU 中拥有这些结构。在这里，我们使用基址不是指向段本身，而是保存该
段的页表的物理地址。界限寄存器用于指示页表的结尾（即它有多少有效页）。

另一种方法并不依赖于分段，但也试图解决相同的问题：如何去掉页表中的所有无效
区域，而不是将它们全部保留在内存中？我们将这种方法称为多级页表（multi-level page
table），因为它将线性页表变成了类似树的东西。这种方法非常有效，许多现代系统都用它

多级页表的基本思想很简单。首先，将页表分成页大小的单元。然后，如果整页的页
表项（PTE）无效，就完全不分配该页的页表。为了追踪页表的页是否有效（以及如果有效，
它在内存中的位置），使用了名为页目录（page directory）的新结构。页目录因此可以告诉
你页表的页在哪里，或者页表的整个页不包含有效页。

图 20.2 展示了一个例子。图的左边是经典的线性页表。即使地址空间的大部分中间区
域无效，我们仍然需要为这些区域分配页表空间（即页表的中间两页）。右侧是一个多级页
表。页目录仅将页表的两页标记为有效（第一个和最后一个）；因此，页表的这两页就驻留
在内存中。因此，你可以形象地看到多级页表的工作方式：它只是让线性页表的一部分消
失（释放这些帧用于其他用途），并用页目录来记录页表的哪些页被分配。

在一个简单的两级页表中，页目录为每页页表包含了一项。它由多个页目录项（Page
Directory Entries，PDE）组成。PDE（至少）拥有有效位（valid bit）和页帧号（page frame number，
PFN），类似于 PTE。但是，正如上面所暗示的，这个有效位的含义稍有不同：如果 PDE 项
是有效的，则意味着该项指向的页表（通过 PFN）中至少有一页是有效的，即在该 PDE 所
指向的页中，至少一个 PTE，其有效位被设置为 1。如果 PDE 项无效（即等于零），则 PDE
的其余部分没有定义。
与我们至今为止看到的方法相比，多级页表有一些明显的优势。首先，也许最明显的
是，多级页表分配的页表空间，与你正在使用的地址空间内存量成比例。因此它通常很紧
凑，并且支持稀疏的地址空间。
[!image](./images/29.png)
其次，如果仔细构建，页表的每个部分都可以整齐地放入一页中，从而更容易管理内
存。操作系统可以在需要分配或增长页表时简单地获取下一个空闲页。将它与一个简单的
（非分页）线性页表相比①，后者仅是按 VPN 索引的 PTE 数组。用这样的结构，整个线性页
表必须连续驻留在物理内存中。对于一个大的页表（比如 4MB），找到如此大量的、未使用
的连续空闲物理内存，可能是一个相当大的挑战。有了多级结构，我们增加了一个间接层
（level of indirection），使用了页目录，它指向页表的各个部分。这种间接方式，让我们能够
将页表页放在物理内存的任何地方。

理解时空折中
在构建数据结构时，应始终考虑时间和空间的折中（time-space trade-off）。通常，如果你希望更快
地访问特定的数据结构，就必须为该结构付出空间的代价。

应该指出，多级页表是有成本的。在 TLB 未命中时，需要从内存加载两次，才能从页
表中获取正确的地址转换信息（一次用于页目录，另一次用于 PTE 本身），而用线性页表只
需要一次加载。因此，多级表是一个时间—空间折中（time-space trade-off）的小例子。我
们想要更小的表（并得到了），但不是没代价。尽管在常见情况下（TLB 命中），性能显然
是相同的，但 TLB 未命中时，则会因较小的表而导致较高的成本。
另一个明显的缺点是复杂性。无论是硬件还是操作系统来处理页表查找（在 TLB 未命
中时），这样做无疑都比简单的线性页表查找更复杂。通常我们愿意增加复杂性以提高性能
或降低管理费用。在多级表的情况下，为了节省宝贵的内存，我们使页表查找更加复杂。

详细的多级示例
为了更好地理解多级页表背后的想法，我们来看一个例子。设想一个大小为 16KB 的小
地址空间，其中包含 64 个字节的页。因此，我们有一个 14 位的虚拟地址空间，VPN 有 8
位，偏移量有 6 位。即使只有一小部分地址空间正在使用，线性页表也会有 28
（256）个项。




















