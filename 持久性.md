* 

* 标准设备 

  现在来看一个标准设备（不是真实存在的），通过它来帮助我们更好地理解设备交互的

  机制。从图 36.2 中，可以看到一个包含两部

  分重要组件的设备。第一部分是向系统其他部

  分展现的硬件接口（interface）。同软件一样，

  硬件也需要一些接口，让系统软件来控制它的

  操作。因此，所有设备都有自己的特定接口以

  及典型交互的协议。

  第 2 部分是它的内部结构（internal structure）。这部分包含设备相关的特定实现，负责

  具体实现设备展示给系统的抽象接口。非常简单的设备通常用一个或几个芯片来实现它们

  的功能。更复杂的设备会包含简单的 CPU、一些通用内存、设备相关的特定芯片，来完成

  它们的工作。

  [!image](./images/42.png)

* 标准协议 

  一个（简化的）设备接口包含 3 个寄存器：一个状态（status）寄存器，

  可以读取并查看设备的当前状态；一个命令（command）寄存器，用于通知设备执行某个具

  体任务；一个数据（data）寄存器，将数据传给设备或从设备接收数据。通过读写这些寄存

  器，操作系统可以控制设备的行为。

  我们现在来描述操作系统与该设备的典型交互，以便让设备为它做某事。

  While (STATUS == BUSY) 

   ; // wait until device is not busy 

  Write data to DATA register 

  Write command to COMMAND register 

   (Doing so starts the device and executes the command) 

  While (STATUS == BUSY) 

   ; // wait until device is done with your request 

  该协议包含 4 步。第 1 步，操作系统通过反复读取状态寄存器，等待设备进入可以接

  收命令的就绪状态。我们称之为轮询（polling）设备（基本上，就是问它正在做什么）。第

  2 步，操作系统下发数据到数据寄存器。例如，你可以想象如果这是一个磁盘，需要多次写

  入操作，将一个磁盘块（比如 4KB）传递给设备。如果主 CPU 参与数据移动（就像这个示

  例协议一样），我们就称之为编程的 I/O（programmed I/O，PIO）。第 3 步，操作系统将命令

  写入命令寄存器；这样设备就知道数据已经准备好了，它应该开始执行命令。最后一步，

  操作系统再次通过不断轮询设备，等待并判断设备是否执行完成命令（有可能得到一个指

  示成功或失败的错误码）。

  这个简单的协议好处是足够简单并且有效。但是难免会有一些低效和不方便。我们注

  意到这个协议存在的第一个问题就是轮询过程比较低效，在等待设备执行完成命令时浪费

  大量 CPU 时间，如果此时操作系统可以切换执行下一个就绪进程，就可以大大提高 CPU 的

  利用率。

* 利用中断减少 CPU 开销 

  多年前，工程师们发明了我们目前已经很常见的中断（interrupt）来减少 CPU 开销。有

  了中断后，CPU 不再需要不断轮询设备，而是向设备发出一个请求，然后就可以让对应进

  程睡眠，切换执行其他任务。当设备完成了自身操作，会抛出一个硬件中断，引发 CPU 跳

  转执行操作系统预先定义好的中断服务例程（Interrupt Service Routine，ISR），或更为简单

  的中断处理程序（interrupt handler）。中断处理程序是一小段操作系统代码，它会结束之前

  的请求（比如从设备读取到了数据或者错误码）并且唤醒等待 I/O 的进程继续执行。

  因此，中断允许计算与 I/O 重叠（overlap），这是提高 CPU 利用率的关键。

  注意，使用中断并非总是最佳方案。假如有一个非常高性能的设备，它处理请求很快：

  通常在 CPU 第一次轮询时就可以返回结果。此时如果使用中断，反而会使系统变慢：切换到

  其他进程，处理中断，再切换回之前的进程代价不小。因此，如果设备非常快，那么最好的

  办法反而是轮询。如果设备比较慢，那么采用允许发生重叠的中断更好。如果设备的速度未

  知，或者时快时慢，可以考虑使用混合（hybrid）策略，先尝试轮询一小段时间，如果设备没

  有完成操作，此时再使用中断。这种两阶段（two-phased）的办法可以实现两种方法的好处。

  另一个最好不要使用中断的场景是网络。网络端收到大量数据包，如果每一个包都发

  生一次中断，那么有可能导致操作系统发生活锁（livelock），即不断处理中断而无法处理用

  户层的请求。例如，假设一个 Web 服务器因为“点杠效应”而突然承受很重的负载。这种

  情况下，偶尔使用轮询的方式可以更好地控制系统的行为，并允许 Web 服务器先服务一些

  用户请求，再回去检查网卡设备是否有更多数据包到达。

  另一个基于中断的优化就是合并（coalescing）。设备在抛出中断之前往往会等待一小段

  时间，在此期间，其他请求可能很快完成，因此多次中断可以合并为一次中断抛出，从而

  降低处理中断的代价。当然，等待太长会增加请求的延迟，这是系统中常见的折中。

* 中断并非总是比 **PIO** 好

  尽管中断可以做到计算与 I/O 的重叠，但这仅在慢速设备上有意义。否则，额外的中断处理和上下

  文切换的代价反而会超过其收益。另外，如果短时间内出现大量的中断，可能会使得系统过载并且引发

  活锁[MR96]。这种情况下，轮询的方式可以在操作系统自身的调度上提供更多的控制，反而更有效。

* 利用 DMA 进行更高效的数据传送

  DMA 工作过程如下。为了能够将数据传送给设备，操作系统会通过编程告诉 DMA 引

  擎数据在内存的位置，要拷贝的大小以及要拷贝到哪个设备。在此之后，操作系统就可以

  处理其他请求了。当 DMA 的任务完成后，DMA 控制器会抛出一个中断来告诉操作系统自

  己已经完成数据传输。数据的拷贝工作都是由 DMA 控制器来完成的。

* 设备交互的方法 

  现在，我们了解了执行 I/O 涉及的效率问题后，还有其他一些问题需要解决，以便将设

  备合并到系统中。你可能已经注意到了一个问题：我们还没有真正讨论过操作系统究竟如

  何与设备进行通信！所以问题如下。

  随着技术的不断发展，主要有两种方式来实现与设备的交互。第一种办法相对老一些

  （在 IBM 主机中使用了多年），就是用明确的 I/O 指令。这些指令规定了操作系统将数据发

  送到特定设备寄存器的方法，从而允许构造上文提到的协议。

  例如在 x86 上，in 和 out 指令可以用来与设备进行交互。当需要发送数据给设备时，调

  用者指定一个存入数据的特定寄存器及一个代表设备的特定端口。执行这个指令就可以实

  现期望的行为。

  这些指令通常是特权指令（privileged）。操作系统是唯一可以直接与设备交互的实体。

  例如，设想如果任意程序都可以直接读写磁盘：完全混乱（总是会这样），因为任何用户程

  序都可以利用这个漏洞来取得计算机的全部控制权。

  第二种方法是内存映射 I/O（memory- mapped I/O）。通过这种方式，硬件将设备寄存器

  作为内存地址提供。当需要访问设备寄存器时，操作系统装载（读取）或者存入（写入）

  到该内存地址；然后硬件会将装载/存入转移到设备上，而不是物理内存。

  两种方法没有一种具备极大的优势。内存映射 I/O 的好处是不需要引入新指令来实现设

  备交互，但两种方法今天都在使用。

* 纳入操作系统：设备驱动程序 

  每个设备都有非常具体的接口，如何将它们纳入操作系统，而我们希望操作系统尽可能通用。例如文件系统，我们希望开发一个文件系统可以工作在

  SCSI 硬盘、IDE 硬盘、USB 钥匙串设备等设备之上，并且希望这个文件系统不那么清楚对

  这些不同设备发出读写请求的全部细节。

  这个问题可以通过古老的抽象（abstraction）技术来解决。在最底层，操作系统的一部

  分软件清楚地知道设备如何工作，我们将这部分软件称为设备驱动程序（device driver），所

  有设备交互的细节都封装在其中。我们来看看 Linux 文件系统栈，理解抽象技术如何应用于操作系统的设计和实现。图

  36.3 粗略地展示了 Linux 软件的组织方式。可以看出，文件系统（当然也包括在其之上的应

  用程序）完全不清楚它使用的是什么类型的磁盘。它只需要简单地向通用块设备层发送读

  写请求即可，块设备层会将这些请求路由给对应的设备驱动，然后设备驱动来完成真正的

  底层操作。

  [!image](./images/43.png)

  注意，这种封装也有不足的地方。例如，如果有一个设备可以提供很多特殊的功能，

  但为了兼容大多数操作系统它不得不提供一个通用的接口，这样就使得自身的特殊功能无

  法使用。这种情况在使用 SCSI 设备的 Linux 中就发生了。SCSI 设备提供非常丰富的报告错

  误信息，但其他的块设备（比如 ATA/IDE）只提供非常简单的报错处理，这样上层的所有

  软件只能在出错时收到一个通用的 EIO 错误码（一般 IO 错误），SCSI 可能提供的所有附加

  信息都不能报告给文件系统。

#### 磁盘驱动器

* 接口 

  我们先来了解一个现代磁盘驱动器的接口。所有现代驱动器的基本接口都很简单。驱

  动器由大量扇区（512 字节块）组成，每个扇区都可以读取或写入。在具有 *n* 个扇区的磁盘

  上，扇区从 0 到 *n*−1 编号。因此，我们可以将磁盘视为一组扇区，0 到 *n*−1 是驱动器的地址

  空间（address space）。

  多扇区操作是可能的。实际上，许多文件系统一次读取或写入 4KB（或更多）。但是，

  在更新磁盘时，驱动器制造商唯一保证的是单个 512 字节的写入是原子的（atomic，即它将

  完整地完成或者根本不会完成）。因此，如果发生不合时宜的掉电，则只能完成较大写入的

  一部分 [有时称为不完整写入（torn write）]。

  大多数磁盘驱动器的客户端都会做出一些假设，但这些假设并未直接在接口中指定。具体来说，通常可以假

  设访问驱动器地址空间内两个彼此靠近的块将比访问两个相隔很远的块更快。人们通常也

  可以假设访问连续块（即顺序读取或写入）是最快的访问模式，并且通常比任何更随机的

  访问模式快得多。

* 基本几何形状 

  让我们开始了解现代磁盘的一些组件。我们从一个盘片（platter）开始，它是一个圆形

  坚硬的表面，通过引入磁性变化来永久存储数据。磁盘可能有一个或多个盘片。每个盘片

  有两面，每面都称为表面。这些盘片通常由一些硬质材料（如铝）制成，然后涂上薄薄的

  磁性层，即使驱动器断电，驱动器也能持久存储数据位。

  所有盘片都围绕主轴（spindle）连接在一起，主轴连接到一个电机，以一个恒定（固定）

  的速度旋转盘片（当驱动器接通电源时）。旋转速率通常以每分钟转数（Rotations Per Minute，

  RPM）来测量，典型的现代数值在 7200～15000 RPM 范围内。请注意，我们经常会对单次

  旋转的时间感兴趣，例如，以 10000 RPM 旋转的驱动器意味着一次旋转需要大约 6ms。

  数据在扇区的同心圆中的每个表面上被编码。我们称这样的同心圆为一个磁道（track）。

  一个表面包含数以千计的磁道，紧密地排在一起，数百个磁道只有头发的宽度。

  要从表面进行读写操作，我们需要一种机制，使我们能够感应（即读取）磁盘上的磁

  性图案，或者让它们发生变化（即写入）。读写过程由磁头（disk head）完成；驱动器的每

  个表面有一个这样的磁头。磁头连接到单个磁盘臂（disk arm）上，磁盘臂在表面上移动，

  将磁头定位在期望的磁道上。

* 磁头必须等待期望的扇区旋转

  到磁头下。这种等待在现代驱动器中经常发生，并且是 I/O 服务时间的重要组成部分，它有

  一个特殊的名称：旋转延迟

  驱动器必须首先将磁盘臂移动到正确的磁道（在

  这种情况下，是最外面的磁道），通过一个所谓的寻道（seek）过程。寻道，以及旋转，是

  最昂贵的磁盘操作之一。

  I/O 的最后阶段将发生，称为传输（transfer），数据从表面

  读取或写入表面。因此，我们得到了完整的 I/O 时间图：首先寻道，然后等待转动延迟，最

  后传输。

* 一些其他细节 

  扇区往往会偏斜，因为从一个磁道切换到另一个磁道

  时，磁盘需要时间来重新定位磁头（即便移到相邻磁道）。

  如果没有这种偏斜，磁头将移动到下一个磁道，但所需的

  下一个块已经旋转到磁头下，因此驱动器将不得不等待整

  个旋转延迟，才能访问下一个块。

  外圈磁道通常比内圈磁道具有更多扇

  区，这是几何结构的结果。那里空间更多。这些磁道通常

  被称为多区域（multi-zoned）磁盘驱动器，其中磁盘被组

  织成多个区域，区域是表面上连续的一组磁道。每个区域

  每个磁道具有相同的扇区数量，并且外圈区域具有比内圈

  区域更多的扇区。

  最后，任何现代磁盘驱动器都有一个重要组成部分，即它的缓存（cache），由于历史原

  因有时称为磁道缓冲区（track buffer）。该缓存只是少量的内存（通常大约 8MB 或 16MB），

  驱动器可以使用这些内存来保存从磁盘读取或写入磁盘的数据。例如，当从磁盘读取扇区

  时，驱动器可能决定读取该磁道上的所有扇区并将其缓存在其存储器中。这样做可以让驱

  动器快速响应所有后续对同一磁道的请求。

  在写入时，驱动器面临一个选择：它应该在将数据放入其内存之后，还是写入实际写

  入磁盘之后，回报写入完成？前者被称为后写（write back）缓存（有时称为立即报告，immediate 

  reporting），后者则称为直写（write through）。后写缓存有时会使驱动器看起来“更快”，但

  可能有危险。如果文件系统或应用程序要求将数据按特定顺序写入磁盘以保证正确性，后

  写缓存可能会导致问题

* I/O 时间：用数学 

  既然我们有了一个抽象的磁盘模型，就可以通过一些分析来更好地理解磁盘性能。具

  体来说，现在可以将 I/O 时间表示为 3 个主要部分之和：

  *T*I/O = *T* 寻道 + *T* 旋转 + *T* 传输 

  请注意，通常比较驱动器用 I/O 速率（*R*I/O）更容易（如下所示），它很容易从时间计算

  出来。只要将传输的大小除以所花的时间：

  RI/O = 	传输大小/TI/O

  为了更好地感受 I/O 时间，我们执行以下计算。假设有两个我们感兴趣的工作负载。第

  一个工作负载称为随机（random）工作负载，它向磁盘上的随机位置发出小的（例如 4KB）

  读取请求。随机工作负载在许多重要的应用程序中很常见，包括数据库管理系统。第二种

  称为顺序（sequential）工作负载，只是从磁盘连续读取大量的扇区，不会跳过。顺序访问

  模式很常见，因此也很重要。

* 顺序地使用磁盘

  尽可能以顺序方式将数据传输到磁盘，并从磁盘传输数据。如果顺序不可行，至少应考虑以大块传

  输数据：越大越好。如果 I/O 是以小而随机方式完成的，则 I/O 性能将受到显著影响。

* 磁盘调度

  由于 I/O 的高成本，操作系统在决定发送给磁盘的 I/O 顺序方面历来发挥作用。更具体

  地说，给定一组 I/O 请求，磁盘调度程序检查请求并决定下一个要调度的请求。

  与任务调度不同，每个任务的长度通常是不知道的，对于磁盘调度，我们可以很好地

  猜测“任务”（即磁盘请求）需要多长时间。通过估计请求的查找和可能的旋转延迟，磁盘

  调度程序可以知道每个请求将花费多长时间，因此（贪婪地）选择先服务花费最少时间的

  请求。因此，磁盘调度程序将尝试在其操作中遵循 SJF（最短任务优先）的原则（principle of 

  SJF，shortest job first）。

  SSTF：最短寻道时间优先  SSTF 按磁道对 I/O 请求队列排序，选择在最近磁道上的请求先完成。 

  SSTF 不是万能的，原因如下。第

  一个问题，主机操作系统无法利用驱动器的几何结构，而

  是只会看到一系列的块。幸运的是，这个问题很容易解决。

  操作系统可以简单地实现最近块优先（Nearest-Block-First，NBF），而不是 SSTF，然后用最

  近的块地址来调度请求。

  第二个问题更为根本：饥饿（starvation）。想象一下，在我们上面的例子中，是否有对

  磁头当前所在位置的内圈磁道有稳定的请求。然后，纯粹的 SSTF 方法将完全忽略对其他磁

  道的请求。

  电梯（又称 SCAN 或 C-SCAN）  

  如何实现类 SSTF 调度，但避免饥饿？这个问题的答案是很久以前得到的，并且相对比较简单。该

  算法最初称为 SCAN，简单地以跨越磁道的顺序来服务磁盘请求。我们将一次跨越磁盘称为

  扫一遍。因此，如果请求的块所属的磁道在这次扫一遍中已经服务过了，它就不会立即处

  理，而是排队等待下次扫一遍。

  SCAN 有许多变种，所有这些变种都是一样的。例如，Coffman 等人引入了 F-SCAN，

  它在扫一遍时冻结队列以进行维护[CKR72]。这个操作会将扫一遍期间进入的请求放入队列

  中，以便稍后处理。这样做可以避免远距离请求饥饿，延迟了迟到（但更近）请求的服务。

  C-SCAN 是另一种常见的变体，即循环 SCAN（Circular SCAN）的缩写。不是在一个

  方向扫过磁盘，该算法从外圈扫到内圈，然后从内圈扫到外圈，如此下去。然而，SCAN 及其变种并不是最好的调度技术。特别是，SCAN（甚至 SSTF）实际上

  并没有严格遵守 SJF 的原则。具体来说，它们忽视了旋转。

  SPTF：最短定位时间优先 

* 其他调度问题 

  在这个基本磁盘操作，调度和相关主题的简要描述中，还有很多问题我们没有讨论。

  其中一个问题是：在现代系统上执行磁盘调度的地方在哪里？在较早的系统中，操作系统

  完成了所有的调度。在查看一系列挂起的请求之后，操作系统会选择最好的一个，并将其

  发送到磁盘。当该请求完成时，将选择下一个，如此下去。磁盘当年比较简单，生活也是。

  在现代系统中，磁盘可以接受多个分离的请求，它们本身具有复杂的内部调度程序（它

  们可以准确地实现 SPTF。在磁盘控制器内部，所有相关细节都可以得到，包括精确的磁头

  位置）。因此，操作系统调度程序通常会选择它认为最好的几个请求（如 16），并将它们全

  部发送到磁盘。磁盘然后利用其磁头位置和详细的磁道布局信息等内部知识，以最佳可能

  （SPTF）顺序服务于这些请求。

  磁盘调度程序执行的另一个重要相关任务是 I/O 合并（I/O merging）。调度程序执行的所有请求都基于合并后的请求。合并在操作系统级别尤其重要，因为它减少了发送到磁盘的请求数量，从而降低了开销。

  现代调度程序关注的最后一个问题是：在向磁盘发出 I/O 之前，系统应该等待多久？有

  人可能天真地认为，即使有一个磁盘 I/O，也应立即向驱动器发出请求。这种方法被称为工

  作保全（work-conserving），因为如果有请求就要服务，磁盘将永远不会闲下来。然而，对预

  期磁盘调度的研究表明，有时最好等待一段时间，即所谓的非工作保全

  （non-work-conserving）方法。通过等待，新的和“更好”的请求可能会到达磁盘，从而整

  体效率提高。当然，决定何时等待以及多久可能会非常棘手。

#### 廉价冗余磁盘阵列（Redundant Array of Inexpensive Disks）

* 这种技术使用多个磁盘一起构建更快、更大、更可靠的磁盘系统。从外部看，RAID 看起来像一个磁盘：一组可以读取或写入的块。在内部，RAID 是一

  个复杂的庞然大物，由多个磁盘、内存（包括易失性和非易失性）以及一个或多个处理器

  来管理系统。硬件 RAID 非常像一个计算机系统，专门用于管理一组磁盘。

  与单个磁盘相比，RAID 具有许多优点。一个好处就是性能。并行使用多个磁盘可以大

  大加快 I/O 时间。另一个好处是容量。大型数据集需要大型磁盘。最后，RAID 可以提高可

  靠性。在多个磁盘上传输数据（无 RAID 技术）会使数据容易受到单个磁盘丢失的影响。通

  过某种形式的冗余（redundancy），RAID 可以容许损失一个磁盘并保持运行，就像没有错误

  一样。	

* 透明支持部署

  在考虑如何向系统添加新功能时，应该始终考虑是否可以透明地（transparently）添加这样的功能，

  而不需要对系统其余部分进行更改。要求彻底重写现有软件（或激进的硬件更改）会减少创意产生影响

  的机会。RAID 就是一个很好的例子，它的透明肯定有助于它的成功。RAID 为使用它们的系统透明地（transparently）提供了这些优势，即

  RAID 对于主机系统看起来就像一个大磁盘。当然，透明的好处在于它可以简单地用 RAID

  替换磁盘，而不需要更换一行软件。操作系统和客户端应用程序无须修改，就可以继续运

  行。通过这种方式，透明极大地提高了 RAID 的可部署性（deployability），使用户和管理员

  可以使用 RAID，而不必担心软件兼容性问题。

* 当文件系统向 RAID 发出逻辑 I/O 请求时，RAID 内部必须计算要访问的磁盘（或多个

  磁盘）以完成请求，然后发出一个或多个物理 I/O 来执行此操作。这些物理 I/O 的确切性质

  取决于 RAID 级别，我们将在下面详细讨论。但是，举一个简单的例子，考虑一个 RAID，

  它保留每个块的两个副本（每个都在一个单独的磁盘上）。当写入这种镜像（mirrored）RAID

  系统时，RAID 必须为它发出的每一个逻辑 I/O 执行两个物理 I/O。

  RAID 系统通常构建为单独的硬件盒，并通过标准连接（例如，SCSI 或 SATA）接入主

  机。然而，在内部，RAID 相当复杂。它包括一个微控制器，运行固件以指导 RAID 的操作。

  它还包括 DRAM 这样的易失性存储器，在读取和写入时缓冲数据块。在某些情况下，还包

  括非易失性存储器，安全地缓冲写入。它甚至可能包含专用的逻辑电路，来执行奇偶校验

  计算（在某些 RAID 级别中非常有用，下面会提到）。在很高的层面上，RAID 是一个非常

  专业的计算机系统：它有一个处理器，内存和磁盘。然而，它不是运行应用程序，而是运

  行专门用于操作 RAID 的软件。

#### 文文件件和和目目录

* 永久存储设备永久地（或至少长时间地）存储信息，如传统硬盘驱动器（hard disk drive）或

  更现代的固态存储设备（solid-state storage device）。持久存储设备与内存不同。内存在断电

  时，其内容会丢失，而持久存储设备会保持这些数据不变。因此，操作系统必须特别注意

  这样的设备：用户用它们保存真正关心的数据。

* 文件和目录 

  随着时间的推移，存储虚拟化形成了两个关键的抽象。第一个是文件（file）。文件就是

  一个线性字节数组，每个字节都可以读取或写入。每个文件都有某种低级名称（low-level 

  name），通常是某种数字。用户通常不知道这个名字（我们稍后会看到）。由于历史原因，

  文件的低级名称通常称为 inode 号（inode number）。

  在大多数系统中，操作系统不太了解文件的结构（例如，它是图片、文本文件还是 C

  代码）。相反，文件系统的责任仅仅是将这些数据永久存储在磁盘上，并确保当你再次请求

  数据时，得到你原来放在那里的内容。

  第二个抽象是目录（directory）。一个目录，像一个文件一样，也有一个低级名字（即

  inode 号），但是它的内容非常具体：它包含一个（用户可读名字，低级名字）对的列表。例

  如，假设存在一个低级别名称为“10”的文件，它的用户可读的名称为“foo”。“foo”所在

  的目录因此会有条目（“foo”，“10”），将用户可读名称映射到低级名称。目录中的每个条目

  都指向文件或其他目录。通过将目录放入其他目录中，用户可以构建任意的目录树（directory 

  tree，或目录层次结构，directory hierarchy），在该目录树下存储所有文件和目录。

  目录层次结构从根目录（root directory）开始（在基于 UNIX 的系统中，根目录就记为

  “/”），并使用某种分隔符（separator）来命名后续子目录

  （sub-directories），直到命名所需的文件或目录。

  目录和文件可以具有相

  同的名称，只要它们位于文件系统树的不同位置

  名称在系统中很重要，因为访问任何资源的第一步是能够命名它。在 UNIX

  系统中，文件系统提供了一种统一的方式来访问磁盘、U 盘、CD-ROM、许多其他设备上的

  文件，事实上还有很多其他的东西，都位于单一目录树下。

* 文件系统接口

  创建一个文件。这可以通过 open 系统调用完成。open()的一个重要方面是它的返回值：文件描述符（file descriptor）。文件描述符只是一

  个整数，是每个进程私有的，在 UNIX 系统中用于访问文件。因此，一旦文件被打开，你

  就可以使用文件描述符来读取或写入文件，假定你有权这样做。这样，一个文件描述符就

  是一种权限（capability），即一个不透明的句柄，它可以让你执行某些操作。另一种

  看待文件描述符的方法，是将它作为指向文件类型对象的指针。一旦你有这样的对象，就

  可以调用其他“方法”来访问文件，如 read()和 write()。

  strace 的作用就是跟踪程序在运行时所做的每个系统调

  用，然后将跟踪结果显示在屏幕上供你查看。

  调用 **lseek()**不会执行磁盘寻道

  调用 lseek()与移动磁盘臂的磁盘的寻道（seek）操作无关。对 lseek()的调用只

  是改变内核中变量的值。执行 I/O 时，根据磁盘头的位置，磁盘可能会也可能不会执行实际

  的寻道来完成请求。命名糟糕的系统调用 lseek()让很多学生困惑，试图去理解磁盘以及其上的文件系统如何工作。不要混淆二者！lseek()调用只是在 OS 内存中更改一个变量，该变量跟踪特定进程的下一个读取或写入开始

  的偏移量。如果发送到磁盘的读取或写入与最后一次读取或写入不在同一磁道上，就会发生磁盘寻道，

  因此需要磁头移动。更令人困惑的是，调用 lseek()从文件的随机位置读取或写入文件，然后读取/写入这

  些随机位置，确实会导致更多的磁盘寻道。因此，调用 lseek()肯定会导致在即将进行的读取或写入中进

  行搜索，但绝对不会导致任何磁盘 I/O 自动发生。

  大多数情况下，当程序调用 write()时，它只是告诉文件系统：请在将来的某个时刻，将此数据写入持久存储。出于性能的原因，文件系统会将这些写入在内存中缓冲（buffer）一

  段时间（例如 5s 或 30s）。在稍后的时间点，写入将实际发送到存储设备。从调用应用程序

  的角度来看，写入似乎很快完成，并且只有在极少数情况下（例如，在 write()调用之后但写

  入磁盘之前，机器崩溃）数据会丢失。	为了支持这些类型的应用程序，大多数文件系统都提供了一些额外的控制 API。在 UNIX

  中，提供给应用程序的接口被称为 fsync(int fd)。当进程针对特定文件描述符调用 fsync()时，

  文件系统通过强制将所有脏（dirty）数据（即尚未写入的）写入磁盘来响应，针对指定文件

  描述符引用的文件。一旦所有这些写入完成，fsync()例程就会返回。

  rename()调用提供了一个有趣的保证：它（通常）是一个原子（atomic）调用，不论系

  统是否崩溃。如果系统在重命名期间崩溃，文件将被命名为旧名称或新名称，不会出现奇

  怪的中间状态。因此，对于支持某些需要对文件状态进行原子更新的应用程序，rename()非

  常重要。

  文件系统能够保存关于它正在存储的每个文件的大量

  信息。我们通常将这些数据称为文件元数据（metadata）事实表明，每个文件系统通常将这种类型的信息保存在一个名为 inode①的结构中。

  unlink()只需要待删除文件的名称，并在成功时返回零。	

  除了文件外，还可以使用一组与目录相关的系统调用来创建、读取和删除目录。请注

  意，你永远不能直接写入目录。因为目录的格式被视为文件系统元数据，所以你只能间接

  更新目录，例如，通过在其中创建文件、目录或其他对象类型。通过这种方式，文件系统

  可以确保目录的内容始终符合预期。

  要创建目录，可以用系统调用 mkdir()。

  opendir()、readdir()和 closedir()这 3 个调用来读取目录。调用 rmdir()来删除目录

  ink()系统调用有两个参数：一个旧路径名和一个新

  路径名。当你将一个新的文件名“链接”到一个旧的文件名时，你实际上创建了另一种引

  用同一个文件的方法。	link 只是在要创建链接的目录中创建了另一个名称，并将其指向原有文件的相同 inode

  号（即低级别名称）。该文件不以任何方式复制。相反，你现在就有了两个人类可读的名称，都指向同一个文件。通过打印每个文件的 inode 号。创建一个文件时，实际上做了两件事。

  首先，要构建一个结构（inode），它将跟踪几乎所有关于文件的信息，包括其大小、文件块

  在磁盘上的位置等等。其次，将人类可读的名称链接到该文件，并将该链接放入目录中。

  在创建文件的硬链接之后，在文件系统中，原有文件名（file）和新创建的文件名（file2）

  之间没有区别。实际上，它们都只是指向文件底层元数据的链接。当文件系统取消链接文件时，它检查 inode 号中的引用计数（reference 

  count）。该引用计数（有时称为链接计数，link count）允许文件系统跟踪有多少不同的文件

  名已链接到这个 inode。调用 unlink()时，会删除人类可读的名称（正在删除的文件）与给定

  inode 号之间的“链接”，并减少引用计数。只有当引用计数达到零时，文件系统才会释放

  inode 和相关数据块，从而真正“删除”该文件。

  符号链接（symbolic link），有时称为软链接（soft 

  link）。事实表明，硬链接有点局限：你不能创建目录的硬链接（因为担心会在目录树中创

  建一个环）。你不能硬链接到其他磁盘分区中的文件（因为 inode 号在特定文件系统中是唯

  一的，而不是跨文件系统），等等。因此，人们创建了一种称为符号链接的新型链接。符号链接实际上与硬链接完全不同。第一个区别是符号链

  接本身实际上是一个不同类型的文件。我们已经讨论过常规文件和目录。符号链接是文件

  系统知道的第三种类型。由于创建符号链接的方式，有可能造成所谓的悬空引用（dangling reference）。

  如何从许多底层文件系统组建完整的目录树。这项任务的实现是先制作文

  件系统，然后挂载它们，使其内容可以访问。为了创建一个文件系统，大多数文件系统提供了一个工具，通常名为 mkfs（发音为“make 

  fs”），它就是完成这个任务的。思路如下：作为输入，为该工具提供一个设备（例如磁盘分

  区，例如/dev/sda1），一种文件系统类型（例如 ext3），它就在该磁盘分区上写入一个空文件

  系统，从根目录开始。mkfs 说，要有文件系统！ 

  但是，一旦创建了这样的文件系统，就需要在统一的文件系统树中进行访问。这个任

  务是通过 mount 程序实现的（它使底层系统调用 mount()完成实际工作）。mount 的作用很简

  单：以现有目录作为目标挂载点（mount point），本质上是将新的文件系统粘贴到目录树的

  这个点上。

#### 文件件系系统统实实现

* 文件系统是纯软件。与 CPU 和内存虚拟化的开发不同，我们不会添加硬件功能来使文

  件系统的某些方面更好地工作（但我们需要注意设备特性，以确保文件系统运行良好）。

* 第一个方面是文件系统的数据结构（data structure）。换言之，文件系统在磁盘上使用哪

  些类型的结构来组织其数据和元数据？我们即将看到的第一个文件系统（包括下面的

  VSFS）使用简单的结构，如块或其他对象的数组，而更复杂的文件系统（如 SGI 的 XFS）

  使用更复杂的基于树的结构。

  文件系统的第二个方面是访问方法（access method）。如何将进程发出的调用，如 open()、

  read()、write()等，映射到它的结构上？在执行特定系统调用期间读取哪些结构？改写哪些

  结构？所有这些步骤的执行效率如何？

  对于文件系统，你

  的心智模型最终应该包含以下问题的答案：磁盘上的哪些结构存储文件系统的数据和元数据？当一个进

  程打开一个文件时会发生什么？在读取或写入期间访问哪些磁盘结构？

* 整体组织

  我们需要做的第一件

  事是将磁盘分成块（block）。简单的文件系统只使用一种块大小。我们对构建文件系统的磁盘分区的看法很简单：一系列块，每块大小为 4KB。

  在大小为 *N* 个 4KB 块的分区中，这些块的地址为从 0 到 *N*−1。

  将用于存放用户数据的磁盘区域称为数据区域（data region），文件系统必须记录每个文件的信息。该信息是元数据

  （metadata）的关键部分，并且记录诸如文件包含哪些数据块（在数据区域中）、文件的大小，

  其所有者和访问权限、访问和修改时间以及其他类似信息的事情。为了存储这些信息，文

  件系统通常有一个名为 inode 的结构。为了存放 inode，我们还需要在磁盘上留出一些空间。我们将这部分磁盘称为 inode 表 

  （inode table），它只是保存了一个磁盘上 inode 的数组。到目前为止，我们的文件系统有了数据块（D）和 inode（I），但还缺一些东西。你可

  能已经猜到，还需要某种方法来记录 inode 或数据块是空闲还是已分配。因此，这种分配结

  构（allocation structure）是所有文件系统中必需的部分。

  当然，可能有许多分配记录方法。例如，我们可以用一个空闲列表（free list），指向第

  一个空闲块，然后它又指向下一个空闲块，依此类推。我们选择一种简单而流行的结构，

  称为位图（bitmap），一种用于数据区域（数据位图，data bitmap），另一种用于 inode 表（inode

  位图，inode bitmap）。位图是一种简单的结构：每个位用于指示相应的对象/块是空闲（0）

  还是正在使用。

  超级块包含关于该特定文件系统的信息，

  包括例如文件系统中有多少个 inode 和数据块、inode 表的

  开始位置等等。它可能还包括一些幻数，来标识文件系统类型。因此，在挂载文件系统时，操作系统将首先读取超级块，初始化各种参数，然后将该

  卷添加到文件系统树中。当卷中的文件被访问时，系统就会知道在哪里查找所需的磁盘上

  的结构。

* 文件组织：inode 

  文件系统最重要的磁盘结构之一是 inode，几乎所有的文件系统都有类似的结构。名

  称 inode 是 index node（索引节点）的缩写，因为这些节点最初放在一个数组中，在访问特定 inode 时会用到该数

  组的索引。

  每个 inode 都由一个数字（称为 inumber）隐式引用，我们之前称之为文件的低级名称

  （low-level name）。在 VSFS（和其他简单的文件系统）中，给定一个 inumber，你应该能够

  直接计算磁盘上相应节点的位置。

  在每个 inode 中，实际上是所有关于文件的信息：文件类型（例如，常规文件、目录等）、

  大小、分配给它的块数、保护信息（如谁拥有该文件以及谁可以访问它）、一些时间信息（包

  括文件创建、修改或上次访问的时间文件下），以及有关其数据块驻留在磁盘上的位置的信

  息（如某种类型的指针）。我们将所有关于文件的信息称为元数据（metadata）。实际上，文

  件系统中除了纯粹的用户数据外，其他任何信息通常都称为元数据。

  设计 inode 时，最重要的决定之一是它如何引用数据块的位置。一种简单的方法是在

  inode 中有一个或多个直接指针（磁盘地址）。每个指针指向属于该文件的一个磁盘块。这种

  方法有局限：例如，如果你想要一个非常大的文件（例如，大于块的大小乘以直接指针数），

  那就不走运了。

  [!image](./images/44.png)
  
#### 崩溃一致性

* 与大多数数据结构不同（例
  如，正在运行的程序在内存中的数据结构），文件系统数据结构必须持久（persist），即它们
  必须长期存在，存储在断电也能保留数据的设备上（例如硬盘或基于闪存的 SSD）
  文件系统面临的一个主要挑战在于，如何在出现断电（power loss）或系统崩溃（system 
  crash）的情况下，更新持久数据结构。具体来说，如果在更新磁盘结构的过程中，有人绊
  到电源线并且机器断电，会发生什么？或者操作系统遇到错误并崩溃？由于断电和崩溃，
  更新持久性数据结构可能非常棘手，并导致了文件系统实现中一个有趣的新问题，称为崩
  溃一致性问题（crash-consistency problem）。
  这个问题很容易理解。想象一下，为了完成特定操作，你必须更新两个磁盘上的结构 A
  和 B。由于磁盘一次只为一个请求提供服务，因此其中一个请求将首先到达磁盘（A 或 B）。
  如果在一次写入完成后系统崩溃或断电，则磁盘上的结构将处于不一致（inconsistent）的状
  态。

  考虑到崩溃，如何更新磁盘
  系统可能在任何两次写入之间崩溃或断电，因此磁盘上状态可能仅部分地更新。崩溃后，系统启动
  并希望再次挂载文件系统（以便访问文件等）。鉴于崩溃可能发生在任意时间点，如何确保文件系统将
  磁盘上的映像保持在合理的状态？
  
  崩溃一致性问题 由于崩溃而导致磁盘文件系统映像可能出现的许
      多问题：在文件系统数据结构中可能存在不一致性。可能有空间泄露，可能将垃圾数据返
      回给用户，等等。理想的做法是将文件系统从一个一致状态（在文件被追加之前），原子地
      （atomically）移动到另一个状态（在 inode、位图和新数据块被写入磁盘之后）。遗憾的是，
      做到这一点不容易，因为磁盘一次只提交一次写入，而这些更新之间可能会发生崩溃或断
      电。我们将这个一般问题称为崩溃一致性问题（crash-consistency problem，也可以称为一致
      性更新问题，consistent-update problem）。
  

* 解决方案 1：文件系统检查程序 

  早期的文件系统采用了一种简单的方法来处理崩溃一致性。基本上，它们决定让不一

  致的事情发生，然后再修复它们（重启时）。这种偷懒方法的典型例子可以在一个工具中找

  到：fsck①。fsck 是一个 UNIX 工具，用于查找这些不一致并修复它们[M86]。在不同的系统

  上，存在检查和修复磁盘分区的类似工具。请注意，这种方法无法解决所有问题。例如，

  考虑上面的情况，文件系统看起来是一致的，但是 inode 指向垃圾数据。唯一真正的目标，

  是确保文件系统元数据内部一致。

  工具 fsck 在许多阶段运行，如 McKusick 和 Kowalski 的论文[MK96]所述。它在文件系

  统挂载并可用之前运行（fsck 假定在运行时没有其他文件系统活动正在进行）。一旦完成，

  磁盘上的文件系统应该是一致的，因此可以让用户访问。	

  以下是 fsck 的基本总结。

   超级块：fsck 首先检查超级块是否合理，主要是进行健全性检查，例如确保文件系

  统大小大于分配的块数。通常，这些健全性检查的目的是找到一个可疑的（冲突

  的）超级块。在这种情况下，系统（或管理员）可以决定使用超级块的备用副本。

   空闲块：接下来，fsck 扫描 inode、间接块、双重间接块等，以了解当前在文件系

  统中分配的块。它利用这些知识生成正确版本的分配位图。因此，如果位图和 inode

  之间存在任何不一致，则通过信任 inode 内的信息来解决它。对所有 inode 执行相

  同类型的检查，确保所有看起来像在用的 inode，都在 inode 位图中有标记。

   **inode** 状态：检查每个 inode 是否存在损坏或其他问题。例如，fsck 确保每个分配

  的 inode 具有有效的类型字段（即常规文件、目录、符号链接等）。如果 inode 字

  段存在问题，不易修复，则 inode 被认为是可疑的，并被 fsck 清除，inode 位图相

  应地更新。

   **inode** 链接：fsck 还会验证每个已分配的 inode 的链接数。你可能还记得，链接计

  数表示包含此特定文件的引用（即链接）的不同目录的数量。为了验证链接计数，

  fsck 从根目录开始扫描整个目录树，并为文件系统中的每个文件和目录构建自己的

  链接计数。如果新计算的计数与 inode 中找到的计数不匹配，则必须采取纠正措施，

  通常是修复 inode 中的计数。如果发现已分配的 inode 但没有目录引用它，则会将

  其移动到 lost + found 目录。

   重复：fsck 还检查重复指针，即两个不同的 inode 引用同一个块的情况。如果一个

  inode 明显不好，可能会被清除。或者，可以复制指向的块，从而根据需要为每个

  inode 提供其自己的副本。

   坏块：在扫描所有指针列表时，还会检查坏块指针。如果指针显然指向超出其有

  效范围的某个指针，则该指针被认为是“坏的”，例如，它的地址指向大于分区

  大小的块。在这种情况下，fsck 不能做任何太聪明的事情。它只是从 inode 或间接

  块中删除（清除）该指针。

   目录检查：fsck 不了解用户文件的内容。但是，目录包含由文件系统本身创建的特

  定格式的信息。因此，fsck 对每个目录的内容执行额外的完整性检查，确保“.”

  和“..”是前面的条目，目录条目中引用的每个 inode 都已分配，并确保整个层次

  结构中没有目录的引用超过一次。

  如你所见，构建有效工作的 fsck 需要复杂的文件系统知识。确保这样的代码在所有情

  况下都能正常工作可能具有挑战性[G+08]。然而，fsck（和类似的方法）有一个更大的、也

  许更根本的问题：它们太慢了。对于非常大的磁盘卷，扫描整个磁盘，以查找所有已分配

  的块并读取整个目录树，可能需要几分钟或几小时。随着磁盘容量的增长和 RAID 的普及，

  fsck 的性能变得令人望而却步（尽管最近取得了进展[M+13]）。

  在更高的层面上，fsck 的基本前提似乎有点不合理。考虑上面的示例，其中只有 3 个块

  写入磁盘。扫描整个磁盘，仅修复更新 3 个块期间出现的问题，这是非常昂贵的。

* 解决方案 2：日志（或预写日志） 

  对于一致更新问题，最流行的解决方案可能是从数据库管理系统的世界中借鉴的一个

  想法。这种名为预写日志（write-ahead logging）的想法，是为了解决这类问题而发明的。

  在文件系统中，出于历史原因，我们通常将预写日志称为日志（journaling）。第一个实现它

  的文件系统是 Cedar [H87]，但许多现代文件系统都使用这个想法，包括 Linux ext3 和 ext4、

  reiserfs、IBM 的 JFS、SGI 的 XFS 和 Windows NTFS。

  基本思路如下。更新磁盘时，在覆写结构之前，首先写下一点小注记（在磁盘上的其

  他地方，在一个众所周知的位置），描述你将要做的事情。写下这个注记就是“预写”部分，

  我们把它写入一个结构，并组织成“日志”。因此，就有了预写日志。

  通过将注释写入磁盘，可以保证在更新（覆写）正在更新的结构期间发生崩溃时，能

  够返回并查看你所做的注记，然后重试。因此，你会在崩溃后准确知道要修复的内容（以

  及如何修复它），而不必扫描整个磁盘。因此，通过设计，日志功能在更新期间增加了一些

  工作量，从而大大减少了恢复期间所需的工作量。

  我们现在将描述 Linux ext3（一种流行的日志文件系统）如何将日志记录到文件系统中。

  大多数磁盘上的结构与 Linux ext2 相同，例如，磁盘被分成块组，每个块组都有一个 inode

  和数据位图以及 inode 和数据块。新的关键结构是日志本身，它占用分区内或其他设备上的

  少量空间。因此，ext2 文件系统（没有日志）看起来像这样：

  [!image](./images/45.png)

  假设日志放在同一个文件系统映像中（虽然有时将它放在单独的设备上，或作为文件

  系统中的文件），带有日志的 ext3 文件系统如下所示：

  [!image](./images/46.png)

  真正的区别只是日志的存在，当然，还有它的使用方式。

* 数据日志 

  事务开始（TxB）告诉我们有关此更新的信息，包括对

  文件系统即将进行的更新的相关信息（例如，块 I[v2]、B[v2]和 Db 的最终地址），以及某种

  事务标识符（transaction identifier，TID）。中间的 3 个块只包含块本身的确切内容，这被称

  为物理日志（physical logging），因为我们将更新的确切物理内容放在日志中（另一种想法，

  逻辑日志（logical logging），在日志中放置更紧凑的更新逻辑表示，例如，“这次更新希望

  将数据块 Db 追加到文件 X”，这有点复杂，但可以节省日志中的空间，并可能提高性能）。

  一旦这个事务安全地存在于磁盘上，我们就可以覆写文件系统中的旧结构了。这个过

  程称为加检查点（checkpointing）。因此，为了对文件系统加检查点（checkpoint，即让它与

  日志中即将进行的更新一致），我们将 I[v2]、B[v2]和 Db 写入其磁盘位置，如上所示。如果

  这些写入成功完成，我们已成功地为文件系统加上了检查点，基本上完成了。因此，我们

  的初始操作顺序如下。

  1．日志写入：将事务（包括事务开始块，所有即将写入的数据和元数据更新以及事务

  结束块）写入日志，等待这些写入完成。

  2．加检查点：将待处理的元数据和数据更新写入文件系统中的最终位置

  在写入日志期间发生崩溃时，事情变得有点棘手。

  强制写入磁盘

  为了在两次磁盘写入之间强制执行顺序，现代文件系统必须采取一些额外的预防措施。在过去，强

  制在两个写入 A 和 B 之间进行顺序很简单：只需向磁盘发出 A 写入，等待磁盘在写入完成时中断 OS，

  然后发出写入 B。

  由于磁盘中写入缓存的使用增加，事情变得有点复杂了。启用写入缓冲后（有时称为立即报告，

  immediate reporting），如果磁盘已经放入磁盘的内存缓存中、但尚未到达磁盘，磁盘就会通知操作系统

  写入完成。如果操作系统随后发出后续写入，则无法保证它在先前写入之后到达磁盘。因此，不再保证

  写入之间的顺序。一种解决方案是禁用写缓冲。然而，更现代的系统采取额外的预防措施，发出明确的

  写入屏障（write barrier）。这样的屏障，当它完成时，能确保在屏障之前发出的所有写入，先于在屏障

  之后发出的所有写入到达磁盘。

  所有这些机制都需要对磁盘的正确操作有很大的信任。遗憾的是，最近的研究表明，为了提供“性

  能更高”的磁盘，一些磁盘制造商显然忽略了写屏障请求，从而使磁盘看起来运行速度更快，但存在操

  作错误的风险[C+13, R+11]。正如 Kahan 所说，快速几乎总是打败慢速，即使快速是错的。

  化日志写入

  你可能已经注意到，写入日志的效率特别低。也就是说，文件系统首先必须写出事务开始块和事务

  的内容。只有在这些写入完成后，文件系统才能将事务结束块发送到磁盘。如果你考虑磁盘的工作方式，

  性能影响很明显：通常会产生额外的旋转（请考虑原因）。

  我们以前的一个研究生 Vijayan Prabhakaran，用一个简单的想法解决了这个问题[P+05]。将事务写

  入日志时，在开始和结束块中包含日志内容的校验和。这样做可以使文件系统立即写入整个事务，而不

  会产生等待。如果在恢复期间，文件系统发现计算的校验和与事务中存储的校验和不匹配，则可以断定

  在写入事务期间发生了崩溃，从而丢弃了文件系统更新。因此，通过写入协议和恢复系统中的小调整，

  文件系统可以实现更快的通用情况性能。最重要的是，系统更可靠了，因为来自日志的任何读取现在都

  受到校验和的保护。

  为避免该问题，文件系统分两步发出事务写入。首先，它将除 TxE 块之外的所有块写

  入日志，同时发出这些写入。

  当这些写入完成时，文件系统会发出 TxE 块的写入，从而使日志处于最终的安全状态：

  此过程的一个重要方面是磁盘提供的原子性保证。事实证明，磁盘保证任何 512 字节

  写入都会发生或不发生（永远不会半写）。因此，为了确保 TxE 的写入是原子的，应该使它

  成为一个 512 字节的块。因此，我们当前更新文件系统的协议如下，3 个阶段中的每一个都

  标上了名称。

  1．日志写入：将事务的内容（包括 TxB、元数据和数据）写入日志，等待这些写入完成。

  2．日志提交：将事务提交块（包括 TxE）写入日志，等待写完成，事务被认为已提交

  （committed）。 

  3．加检查点：将更新内容（元数据和数据）写入其最终的磁盘位置。

* 恢复 

  现在来了解文件系统如何利用日志内容从崩溃中恢复（recover）。在这个更新序列期间，

  任何时候都可能发生崩溃。如果崩溃发生在事务被安全地写入日志之前（在上面的步骤 2

  完成之前），那么我们的工作很简单：简单地跳过待执行的更新。如果在事务已提交到日志

  之后但在加检查点完成之前发生崩溃，则文件系统可以按如下方式恢复（recover）更新。

  系统引导时，文件系统恢复过程将扫描日志，并查找已提交到磁盘的事务。然后，这些事

  务被重放（replayed，按顺序），文件系统再次尝试将事务中的块写入它们最终的磁盘位置。

  这种形式的日志是最简单的形式之一，称为重做日志（redo logging）。通过在日志中恢复已

  提交的事务，文件系统确保磁盘上的结构是一致的，因此可以继续工作，挂载文件系统并

  为新请求做好准备。

  请注意，即使在某些更新写入块的最终位置之后，在加检查点期间的任何时刻发生崩

  溃，都没问题。在最坏的情况下，其中一些更新只是在恢复期间再次执行。因为恢复是一

  种罕见的操作（仅在系统意外崩溃之后发生），所以几次冗余写入无须担心①。

* 批处理日志更新 

  你可能已经注意到，基本协议可能会增加大量额外的磁盘流量。例如，假设我们在同

  一目录中连续创建两个文件，称为 file1 和 file2。要创建一个文件，必须更新许多磁盘上的

  结构，至少包括：inode 位图（分配新的 inode），新创建的文件 inode，包含新文件目录条目

  的父目录的数据块，以及父目录的 inode（现在有一个新的修改时间）。通过日志，我们将所

  有这些信息逻辑地提交给我们的两个文件创建的日志。因为文件在同一个目录中，我们假

  设在同一个 inode 块中都有 inode，这意味着如果不小心，我们最终会一遍又一遍地写入这

  些相同的块。

  为了解决这个问题，一些文件系统不会一次一个地向磁盘提交每个更新（例如，Linux 

  ext3）。与此不同，可以将所有更新缓冲到全局事务中。在上面的示例中，当创建两个文件

  时，文件系统只将内存中的 inode 位图、文件的 inode、目录数据和目录 inode 标记为脏，并

  将它们添加到块列表中，形成当前的事务。当最后应该将这些块写入磁盘时（例如，在超

  时 5s 之后），会提交包含上述所有更新的单个全局事务。因此，通过缓冲更新，文件系统在

  许多情况下可以避免对磁盘的过多的写入流量。

* 使日志有限 

  因此，我们已经了解了更新文件系统磁盘结构的基本协议。文件系统缓冲内存中的更

  新一段时间。最后写入磁盘时，文件系统首先仔细地将事务的详细信息写入日志（即预写

  日志）。事务完成后，文件系统会加检查点，将这些块写入磁盘上的最终位置。

  但是，日志的大小有限。如果不断向它添加事务（如下所示），它将很快填满。你觉得

  会发生什么？

  日志满时会出现两个问题。第一个问题比较简单，但不太重要：日志越大，恢复时间

  越长，因为恢复过程必须重放日志中的所有事务（按顺序）才能恢复。第二个问题更重要：

  当日志已满（或接近满）时，不能向磁盘提交进一步的事务，从而使文件系统“不太有用”

  （即无用）。

  为了解决这些问题，日志文件系统将日志视为循环数据结构，一遍又一遍地重复使用。

  这就是为什么日志有时被称为循环日志（circular log）。为此，文件系统必须在加检查点之

  后的某个时间执行操作。具体来说，一旦事务被加检查点，文件系统应释放它在日志中占

  用的空间，允许重用日志空间。有很多方法可以达到这个目的。例如，你只需在日志超级

  块（journal superblock）中标记日志中最旧和最新的事务。所有其他空间都是空闲的。以下

  是这种机制的图形描述：

  [!image](./images/47.png)

  在日志超级块中（不要与主文件系统的超级块混淆），日志系统记录了足够的信息，以

  了解哪些事务尚未加检查点，从而减少了恢复时间，并允许以循环的方式重新使用日志。

  因此，我们在基本协议中添加了另一个步骤。

  1．日志写入：将事务的内容（包括 TxB 和更新内容）写入日志，等待这些写入完成。

  2．日志提交：将事务提交块（包括 TxE）写入日志，等待写完成，事务被认为已提交

  （committed）。 

  3．加检查点：将更新内容写入其最终的磁盘位置。

  4．释放：一段时间后，通过更新日志超级块，在日志中标记该事务为空闲。

  因此，我们得到了最终的数据日志协议。但仍然存在一个问题：我们将每个数据块写

  入磁盘两次，这是沉重的成本，特别是为了系统崩溃这样罕见的事情。你能找到一种方法

  来保持一致性，而无须两次写入数据吗？

* 元数据日志 

  尽管恢复现在很快（扫描日志并重放一些事务而不是扫描整个磁盘），但文件系统的正

  常操作比我们想要的要慢。特别是，对于每次写入磁盘，我们现在也要先写入日志，从而

  使写入流量加倍。在顺序写入工作负载期间，这种加倍尤为痛苦，现在将以驱动器峰值写

  入带宽的一半进行。此外，在写入日志和写入主文件系统之间，存在代价高昂的寻道，这

  为某些工作负载增加了显著的开销。

  由于将每个数据块写入磁盘的成本很高，人们为了提高性能，尝试了一些不同的东西。例

  如，我们上面描述的日志模式通常称为数据日志（data journaling，如在 Linux ext3 中），因为它

  记录了所有用户数据（除了文件系统的元数据之外）。一种更简单（也更常见）的日志形式有时

  称为有序日志（ordered journaling，或称为元数据日志，metadata journaling），它几乎相同，只

  是用户数据没有写入日志。因此，在执行与上述相同的更新时，以下信息将写入日志：

  [!image](./images/48.png)

  先前写入日志的数据块 Db 将改为写入文件系统，避免额外写入。考虑到磁盘的大多数

  I/O 流量是数据，不用两次写入数据会大大减少日志的 I/O 负载。然而，修改确实提出了一

  个有趣的问题：我们何时应该将数据块写入磁盘？

  再考虑一下文件追加的例子，以更好地理解问题。更新包含 3 个块：I[v2]、B[v2]和 Db。

  前两个都是元数据，将被记录，然后加检查点。后者只会写入文件系统一次。什么时候应

  该把 Db 写入磁盘？这有关系吗？

  事实证明，数据写入的顺序对于仅元数据日志很重要。例如，如果我们在事务（包含 I 

  [v2]和 B [v2]）完成后将 Db 写入磁盘如何？遗憾的是，这种方法存在一个问题：文件系统

  是一致的，但 I[v2]可能最终指向垃圾数据。具体来说，考虑写入了 I[v2]和 B[v2]，但 Db 没

  有写入磁盘的情况。然后文件系统将尝试恢复。由于 Db 不在日志中，因此文件系统将重放

  对 I[v2]和 B[v2]的写入，并生成一致的文件系统（从文件系统元数据的角度来看）。但是，

  I[v2]将指向垃圾数据，即指向 Db 中的任何数据。

  为了确保不出现这种情况，在将相关元数据写入磁盘之前，一些文件系统（例如，Linux 

  ext3）先将数据块（常规文件）写入磁盘。具体来说，协议有以下几个。

  1．数据写入：将数据写入最终位置，等待完成（等待是可选的，详见下文）。 

  2．日志元数据写入：将开始块和元数据写入日志，等待写入完成。

  3．日志提交：将事务提交块（包括 TxE）写入日志，等待写完成，现在认为事务（包

  括数据）已提交（committed）。 

  4．加检查点元数据：将元数据更新的内容写入文件系统中的最终位置。

  5．释放：稍后，在日志超级块中将事务标记为空闲。

  通过强制先写入数据，文件系统可以保证指针永远不会指向垃圾。实际上，这个“先

  写入被指对象，再写入指针对象”的规则是崩溃一致性的核心，并且被其他崩溃一致性方

  案[GP94]进一步利用。

  在大多数系统中，元数据日志（类似于 ext3 的有序日志）比完整数据日志更受欢迎。

  例如，Windows NTFS 和 SGI 的 XFS 都使用无序的元数据日志。Linux ext3 为你提供了选择

  数据、有序或无序模式的选项（在无序模式下，可以随时写入数据）。所有这些模式都保持

  元数据一致，它们的数据语义各不相同。

  最后，请注意，在发出写入日志（步骤 2）之前强制数据写入完成（步骤 1）不是正确

  性所必需的，如上面的协议所示。具体来说，可以发出数据写入，并向日志写入事务开始

  块和元数据。唯一真正的要求，是在发出日志提交块之前完成步骤 1 和步骤 2（步骤 3）。

* 棘手的情况：块复用 

  一些有趣的特殊情况让日志更加棘手，因此值得讨论。其中一些与块复用有关。正如

  Stephen Tweedie（ext3 背后的主要开发者之一）说的：

  “整个系统的可怕部分是什么？……是删除文件。与删除有关的一切都令人毛骨悚然。

  与删除有关的一切……如果块被删除然后重新分配，你会做噩梦。”[T00] 

  Tweedie 给出的具体例子如下。假设你正在使用某种形式的元数据日志（因此不记录文

  件的数据块）。假设你有一个名为 foo 的目录。用户向 foo 添加一个条目（例如通过创建文

  件），因此 foo 的内容（因为目录被认为是元数据）被写入日志。假设 foo 目录数据的位置

  是块 1000。因此日志包含如下内容：

  [!image](./images/49.png)

  此时，用户删除目录中的所有内容以及目录本身，从而释放块 1000 以供复用。最后，

  用户创建了一个新文件（比如 foobar），结果复用了过去属于 foo 的相同块（1000）。foobar

  的 inode 提交给磁盘，其数据也是如此。但是，请注意，因为正在使用元数据日志，所以只

  有 foobar 的 inode 被提交给日志，文件 foobar 中块 1000 中新写入的数据没有写入日志。

  现在假设发生了崩溃，所有这些信息仍然在日志中。在重放期间，恢复过程简单地重

  放日志中的所有内容，包括在块 1000 中写入目录数据。因此，重放会用旧目录内容覆盖当

  前文件 foobar 的用户数据！显然，这不是一个正确的恢复操作，当然，在阅读文件 foobar

  时，用户会感到惊讶。

  [!image](./images/50.png)

  这个问题有一些解决方案。例如，可以永远不再重复使用块，直到所述块的删除加上检

  查点，从日志中清除。Linux ext3 的做法是将新类型的记录添加到日志中，称为撤销（revoke）

  记录。在上面的情况中，删除目录将导致撤销记录被写入日志。在重放日志时，系统首先扫

  描这样的重新记录。任何此类被撤销的数据都不会被重放，从而避免了上述问题

* 我们介绍了崩溃一致性的问题，并讨论了处理这个问题的各种方法。构建文件系统检查程

  序的旧方法有效，但在现代系统上恢复可能太慢。因此，许多文件系统现在使用日志。日志可

  将恢复时间从 O（磁盘大小的卷）减少到 O（日志大小），从而在崩溃和重新启动后大大加快恢

  复速度。因此，许多现代文件系统都使用日志。我们还看到日志可以有多种形式。最常用的是有

  序元数据日志，它可以减少日志流量，同时仍然保证文件系统元数据和用户数据的合理一致性。

#### 日志结构文件系统

* 在 20 世纪 90 年代早期，由 John Ousterhout 教授和研究生 Mendel Rosenblum 领导的伯

  克利小组开发了一种新的文件系统，称为日志结构文件系统[RO91]。他们这样做的动机是

  基于以下观察。

   内存大小不断增长。随着内存越来越大，可以在内存中缓存更多数据。随着更多

  数据的缓存，磁盘流量将越来越多地由写入组成，因为读取将在缓存中进行处理。

  因此，文件系统性能很大程度上取决于写入性能。

   随机 **I/O** 性能与顺序 **I/O** 性能之间存在巨大的差距，且不断扩大：传输带宽每年增

  加约 **50%****～****100%**。寻道和旋转延迟成本下降得较慢，可能每年 5%～10%[P98]。

  因此，如果能够以顺序方式使用磁盘，则可以获得巨大的性能优势，随着时间的

  推移而增长。

   现有文件系统在许多常见工作负载上表现不佳。例如，FFS [MJLF84]会执行大量

  写入，以创建大小为一个块的新文件：一个用于新的 inode，一个用于更新 inode

  位图，一个用于文件所在的目录数据块，一个用于目录 inode 以更新它，一个用于

  新数据块，它是新文件的一部分，另一个是数据位图，用于将数据块标记为已分

  配。因此，尽管 FFS 会将所有这些块放在同一个块组中，但 FFS 会导致许多短寻

  道和随后的旋转延迟，因此性能远远低于峰值顺序带宽。

   文件系统不支持 **RAID**。例如，RAID-4 和 RAID-5 具有小写入问题（small-write 

  problem），即对单个块的逻辑写入会导致 4 个物理 I/O 发生。现有的文件系统不

  会试图避免这种最坏情况的 RAID 写入行为。

  因此，理想的文件系统会专注于写入性能，并尝试利用磁盘的顺序带宽。此外，它在

  常见工作负载上表现良好，这种负载不仅写出数据，还经常更新磁盘上的元数据结构。最

  后，它可以在 RAID 和单个磁盘上运行良好。

  引入的新型文件系统 Rosenblum 和 Ousterhout 称为 LFS，是日志结构文件系统

  （Log-structured File System）的缩写。写入磁盘时，LFS 首先将所有更新（包括元数据！）缓

  冲在内存段中。当段已满时，它会在一次长时间的顺序传输中写入磁盘，并传输到磁盘的

  未使用部分。LFS 永远不会覆写现有数据，而是始终将段写入空闲位置。由于段很大，因此

  可以有效地使用磁盘，并且文件系统的性能接近其峰值。

#### 数据完整性

* ，确保放入存储系统的数据就是存储系统返回的数据。

* 磁盘故障模式 

  正如你在关于 RAID 的章节中所了解到的，磁盘并不完美，并且可能会发生故障（有时）。

  在早期的 RAID 系统中，故障模型非常简单：要么整个磁盘都在工作，要么完全失败，而且检

  测到这种故障很简单。这种磁盘故障的故障—停止（fail-stop）模型使构建 RAID 相对简单[S90]。

  你不知道的是现代磁盘展示的所有其他类型的故障模式。具体来说，正如 Bairavasundaram

  等人的详细研究[B+07，B+08]，现代磁盘似乎大部分时间正常工作，但是无法成功访问一个

  或几个块。具体来说，两种类型的单块故障是常见的，值得考虑：潜在扇区错误（Latent-Sector 

  Errors，LSE）和块讹误（block corruption）

  当磁盘扇区（或扇区组）以某种方式讹误时，会出现 LSE。例如，如果磁头由于某种

  原因接触到表面（磁头碰撞，head crash，在正常操作期间不应发生的情况），则可能会讹误

  表面，使得数据位不可读。宇宙射线也会导致数据位翻转，使内容不正确。幸运的是，驱

  动器使用磁盘内纠错码（Error Correcting Code，ECC）来确定块中的磁盘位是否良好，并且

  在某些情况下，修复它们。如果它们不好，并且驱动器没有足够的信息来修复错误，则在

  发出请求读取它们时，磁盘会返回错误。

  还存在一些情况，磁盘块出现讹误（corrupt），但磁盘本身无法检测到。例如，有缺陷

  的磁盘固件可能会将块写入错误的位置。在这种情况下，磁盘 ECC 指示块内容很好，但是

  从客户端的角度来看，在随后访问时返回错误的块。类似地，当一个块通过有故障的总线

  从主机传输到磁盘时，它可能会讹误。由此产生的讹误数据会存入磁盘，但它不是客户所

  希望的。这些类型的故障特别隐蔽，因为它们是无声的故障（silent fault）。返回故障数据时，

  磁盘没有报告问题。

  Prabhakaran 等人将这种更现代的磁盘故障视图描述为故障—部分（fail-partial）磁盘故

  障模型[P+05]。在此视图中，磁盘仍然可以完全失败（像传统的故障停止模型中的情况）。

  然而，磁盘也可以似乎正常工作，并有一个或多个块变得不可访问（即 LSE）或保存了错

  误的内容（即讹误）。因此，当访问看似工作的磁盘时，偶尔会在尝试读取或写入给定块时

  返回错误（非无声的部分故障），偶尔可能只是返回错误的数据（一个无声的部分错误）。

* 处理潜在的扇区错误 

  事实证明，潜在的扇区错误很容易处理，因为它们（根据定义）很容易被检测到。当

  存储系统尝试访问块，并且磁盘返回错误时，存储系统应该就用它具有的任何冗余机制，

  来返回正确的数据。例如，在镜像 RAID 中，系统应该访问备用副本。在基于奇偶校验的

  RAID-4 或 RAID-5 系统中，系统应通过奇偶校验组中的其他块重建该块。因此，利用标准

  冗余机制，可以容易地恢复诸如 LSE 这样的容易检测到的问题。

  多年来，LSE 的日益增长影响了 RAID 设计。当全盘故障和 LSE 接连发生时，RAID-4/5

  系统会出现一个特别有趣的问题。具体来说，当整个磁盘发生故障时，RAID 会尝试读取奇

  偶校验组中的所有其他磁盘，并重新计算缺失值，来重建（reconstruct）磁盘（例如，在热

  备用磁盘上）。如果在重建期间，在任何一个其他磁盘上遇到 LSE，我们就会遇到问题：重

  建无法成功完成。

  为了解决这个问题，一些系统增加了额外的冗余度。例如，NetApp 的 RAID-DP 相当于

  两个奇偶校验磁盘，而不是一个[C+04]。在重建期间发现 LSE 时，额外的校验盘有助于重

  建丢失的块。与往常一样，这有成本，因为为每个条带维护两个奇偶校验块的成本更高。

  但是，NetApp WAFL 文件系统的日志结构特性在许多情况下降低了成本[HLM94]。另外的

  成本是空间，需要额外的磁盘来存放第二个奇偶校验块。














































