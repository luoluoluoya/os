#### 标准设备 

* 从图 36.2 中，可以看到一个包含两部分重要组件的设备。

* 第一部分是向系统其他部分展现的硬件接口(interface)。同软件一样，硬件也需要一些接口，让系统软件来控制它的操作。因此，所有设备都有自己的特定接口以及典型交互的协议。

* 第 2 部分是它的内部结构(internal structure)。这部分包含设备相关的特定实现，负责具体实现设备展示给系统的抽象接口。非常简单的设备通常用一个或几个芯片来实现它们的功能。更复杂的设备会包含简单的 CPU、一些通用内存、设备相关的特定芯片，来完成它们的工作。

  [!image](./images/42.png)

##### 标准协议 

* 一个(简化的)设备接口包含 3 个寄存器：一个状态(status)寄存器，可以读取并查看设备的当前状态；一个命令(command)寄存器，用于通知设备执行某个具体任务；一个数据(data)寄存器，将数据传给设备或从设备接收数据。通过读写这些寄存器，操作系统可以控制设备的行为。

* 我们现在来描述操作系统与该设备的典型交互，以便让设备为它做某事。

  ```
  While (STATUS == BUSY)  ; // wait until device is not busy 
  Write data to DATA register 
  Write command to COMMAND register 
  (Doing so starts the device and executes the command) 
  While (STATUS == BUSY)  ; // wait until device is done with your request 
  ```

* 该协议包含 4 步。
  * 第 1 步，操作系统通过反复读取状态寄存器，等待设备进入可以接收命令的就绪状态。我们称之为轮询设备(基本上，就是问它正在做什么)。
  * 第2 步，操作系统下发数据到数据寄存器。例如，你可以想象如果这是一个磁盘，需要多次写入操作，将一个磁盘块(比如 4KB)传递给设备。如果主 CPU 参与数据移动(就像这个示例协议一样)，我们就称之为编程的 I/O(programmed I/O，PIO)。
  * 第 3 步，操作系统将命令写入命令寄存器；这样设备就知道数据已经准备好了，它应该开始执行命令。
  * 最后一步，操作系统再次通过不断轮询设备，等待并判断设备是否执行完成命令(有可能得到一个指示成功或失败的错误码)。
* 这个简单的协议好处是足够简单并且有效。但是难免会有一些低效和不方便。我们注意到这个协议存在的第一个问题就是轮询过程比较低效，在等待设备执行完成命令时浪费大量 CPU 时间，如果此时操作系统可以切换执行下一个就绪进程，就可以大大提高 CPU 的利用率。

##### 利用中断减少 CPU 开销 

* 多年前，工程师们发明了我们目前已经很常见的中断来减少 CPU 开销。有了中断后，CPU 不再需要不断轮询设备，而是向设备发出一个请求，然后就可以让对应进程睡眠，切换执行其他任务。当设备完成了自身操作，会抛出一个硬件中断，引发 CPU 跳转执行操作系统预先定义好的中断服务例程(Interrupt Service Routine，ISR)，或更为简单的中断处理程序(interrupt handler)。
* 中断处理程序是一小段操作系统代码，它会结束之前的请求(比如从设备读取到了数据或者错误码)并且唤醒等待 I/O 的进程继续执行。因此，中断允许计算与 I/O 重叠，这是提高 CPU 利用率的关键。
* 注意，使用中断并非总是最佳方案。假如有一个非常高性能的设备，它处理请求很快：通常在 CPU 第一次轮询时就可以返回结果。此时如果使用中断，反而会使系统变慢：切换到其他进程，处理中断，再切换回之前的进程代价不小。因此，如果设备非常快，那么最好的办法反而是轮询。如果设备比较慢，那么采用允许发生重叠的中断更好。如果设备的速度未知，或者时快时慢，可以考虑使用混合策略，先尝试轮询一小段时间，如果设备没有完成操作，此时再使用中断。这种两阶段的办法可以实现两种方法的好处。
* 一个最好不要使用中断的场景是网络。网络端收到大量数据包，如果每一个包都发生一次中断，那么有可能导致操作系统发生活锁，即不断处理中断而无法处理用户层的请求。例如，假设一个 Web 服务器因为“点杠效应”而突然承受很重的负载。这种情况下，偶尔使用轮询的方式可以更好地控制系统的行为，并允许 Web 服务器先服务一些用户请求，再回去检查网卡设备是否有更多数据包到达。
* 一个基于中断的优化就是合并。设备在抛出中断之前往往会等待一小段时间，在此期间，其他请求可能很快完成，因此多次中断可以合并为一次中断抛出，从而降低处理中断的代价。当然，等待太长会增加请求的延迟，这是系统中常见的折中。

* 尽管中断可以做到计算与 I/O 的重叠，但这仅在慢速设备上有意义。否则，额外的中断处理和上下文切换的代价反而会超过其收益。另外，如果短时间内出现大量的中断，可能会使得系统过载并且引发活锁。这种情况下，轮询的方式可以在操作系统自身的调度上提供更多的控制，反而更有效。

##### 设备交互的方法 

* 了解了执行 I/O 涉及的效率问题后，还有其他一些问题需要解决，以便将设备合并到系统中。操作系统究竟如何与设备进行通信?
* 第一种办法相对老一些(在 IBM 主机中使用了多年)，就是用明确的 I/O 指令。这些指令规定了操作系统将数据发送到特定设备寄存器的方法，从而允许构造上文提到的协议。例如在 x86 上，in 和 out 指令可以用来与设备进行交互。当需要发送数据给设备时，调用者指定一个存入数据的特定寄存器及一个代表设备的特定端口。执行这个指令就可以实现期望的行为。这些指令通常是特权指令。操作系统是唯一可以直接与设备交互的实体。例如，设想如果任意程序都可以直接读写磁盘：完全混乱(总是会这样)，因为任何用户程序都可以利用这个漏洞来取得计算机的全部控制权。
* 第二种方法是内存映射 I/O(memory- mapped I/O)。通过这种方式，硬件将设备寄存器作为内存地址提供。当需要访问设备寄存器时，操作系统装载(读取)或者存入(写入)到该内存地址；然后硬件会将装载/存入转移到设备上，而不是物理内存。内存映射 I/O 的好处是不需要引入新指令来实现设备交互。

* 每个设备都有非常具体的接口，如何将它们纳入操作系统，而我们希望操作系统尽可能通用。例如文件系统，我们希望开发一个文件系统可以工作在 SCSI 硬盘、IDE 硬盘、USB 钥匙串设备等设备之上，并且希望这个文件系统不那么清楚对这些不同设备发出读写请求的全部细节。在最底层，操作系统的一部分软件清楚地知道设备如何工作，我们将这部分软件称为设备驱动程序，所有设备交互的细节都封装在其中。我们来看看 Linux 文件系统栈，理解抽象技术如何应用于操作系统的设计和实现。图36.3 粗略地展示了 Linux 软件的组织方式。可以看出，文件系统(当然也包括在其之上的应用程序)完全不清楚它使用的是什么类型的磁盘。它只需要简单地向通用块设备层发送读写请求即可，块设备层会将这些请求路由给对应的设备驱动，然后设备驱动来完成真正的底层操作。

  [!image](./images/43.png)

  注意，这种封装也有不足的地方。例如，如果有一个设备可以提供很多特殊的功能，但为了兼容大多数操作系统它不得不提供一个通用的接口，这样就使得自身的特殊功能无法使用。这种情况在使用 SCSI 设备的 Linux 中就发生了。SCSI 设备提供非常丰富的报告错误信息，但其他的块设备(比如 ATA/IDE)只提供非常简单的报错处理，这样上层的所有软件只能在出错时收到一个通用的 EIO 错误码(一般 IO 错误)，SCSI 可能提供的所有附加信息都不能报告给文件系统。


#### 磁盘驱动器

* 驱动器由大量扇区(512 字节块)组成，每个扇区都可以读取或写入。在具有 n 个扇区的磁盘上，扇区从 0 到 n−1 编号。因此，我们可以将磁盘视为一组扇区，0 到 n−1 是驱动器的地址空间。
* 多扇区操作是可能的。实际上，许多文件系统一次读取或写入 4KB(或更多)。但是，在更新磁盘时，驱动器制造商唯一保证的是单个 512 字节的写入是原子的。因此，如果发生不合时宜的掉电，则只能完成较大写入的一部分(有时称为不完整写)。
* 大多数磁盘驱动器的客户端都会做出一些假设，但这些假设并未直接在接口中指定。具体来说，通常可以假设访问驱动器地址空间内两个彼此靠近的块将比访问两个相隔很远的块更快。人们通常也可以假设访问连续块(即顺序读取或写入)是最快的访问模式，并且通常比任何更随机的访问模式快得多。

##### 基本几何形状 

* 让我们开始了解现代磁盘的一些组件。我们从一个盘片开始，它是一个圆形坚硬的表面，通过引入磁性变化来永久存储数据。磁盘可能有一个或多个盘片。每个盘片有两面，每面都称为表面。这些盘片通常由一些硬质材料(如铝)制成，然后涂上薄薄的磁性层，即使驱动器断电，驱动器也能持久存储数据位。
* 所有盘片都围绕主轴(spindle)连接在一起，主轴连接到一个电机，以一个恒定(固定)的速度旋转盘片(当驱动器接通电源时)。旋转速率通常以每分钟转数(Rotations Per Minute，RPM)来测量，典型的现代数值在 7200～15000 RPM 范围内。请注意，我们经常会对单次旋转的时间感兴趣，例如，以 10000 RPM 旋转的驱动器意味着一次旋转需要大约 6ms。
* 数据在扇区的同心圆中的每个表面上被编码。我们称这样的同心圆为一个磁道。一个表面包含数以千计的磁道，紧密地排在一起，数百个磁道只有头发的宽度。
* 要从表面进行读写操作，我们需要一种机制，使我们能够感应(即读取)磁盘上的磁性图案，或者让它们发生变化(即写入)。读写过程由磁头完成；驱动器的每个表面有一个这样的磁头。
* 磁头连接到单个磁盘臂上，磁盘臂在表面上移动，将磁头定位在期望的磁道上。

* 磁头必须等待期望的扇区旋转到磁头下。这种等待在现代驱动器中经常发生，并且是 I/O 服务时间的重要组成部分，它有一个特殊的名称：旋转延迟。驱动器必须首先将磁盘臂移动到正确的磁道(在这种情况下，是最外面的磁道)，通过一个所谓的寻道(seek)过程。寻道，以及旋转，是最昂贵的磁盘操作之一。I/O 的最后阶段称为传输，数据从表面读取或写入表面。因此，我们得到了完整的 I/O 时间图：首先寻道，然后等待转动延迟，最后传输。

* 扇区往往会偏斜，因为从一个磁道切换到另一个磁道时，磁盘需要时间来重新定位磁头(即便移到相邻磁道)。如果没有这种偏斜，磁头将移动到下一个磁道，但所需的下一个块已经旋转到磁头下，因此驱动器将不得不等待整个旋转延迟，才能访问下一个块。
* 外圈磁道通常比内圈磁道具有更多扇区，这是几何结构的结果。那里空间更多。这些磁道通常被称为多区域磁盘驱动器，其中磁盘被组织成多个区域，区域是表面上连续的一组磁道。每个区域每个磁道具有相同的扇区数量，并且外圈区域具有比内圈区域更多的扇区。
* 任何现代磁盘驱动器都有一个重要组成部分，即它的缓存，由于历史原因有时称为磁道缓冲区。该缓存只是少量的内存(通常大约 8MB 或 16MB)，驱动器可以使用这些内存来保存从磁盘读取或写入磁盘的数据。例如，当从磁盘读取扇区时，驱动器可能决定读取该磁道上的所有扇区并将其缓存在其存储器中。这样做可以让驱动器快速响应所有后续对同一磁道的请求。在写入时，驱动器面临一个选择：它应该在将数据放入其内存之后，还是写入实际磁盘之后，回报写入完成？前者被称为后写缓存(有时称为立即报告)，后者则称为直写。后写缓存有时会使驱动器看起来“更快”，但可能有危险。如果文件系统或应用程序要求将数据按特定顺序写入磁盘以保证正确性，后写缓存可能会导致问题。

##### I/O 时间

* 既然我们有了一个抽象的磁盘模型，就可以通过一些分析来更好地理解磁盘性能。具体来说，现在可以将 I/O 时间表示为 3 个主要部分之和：`TI/O = T寻道 + T旋转 + T传输 `。请注意，通常比较驱动器用 I/O 速率(RI/O)更容易，它很容易从时间计算出来。只要将传输的大小除以所花的时间：`RI/O = 传输大小/TI/O`。
* 为了更好地感受 I/O 时间，我们执行以下计算。假设有两个我们感兴趣的工作负载。第一个工作负载称为随机工作负载，它向磁盘上的随机位置发出小的读取请求。随机工作负载在许多重要的应用程序中很常见，包括数据库管理系统。第二种称为顺序工作负载，只是从磁盘连续读取大量的扇区，不会跳过。顺序访问模式很常见，因此也很重要。

* 顺序地使用磁盘尽可能以顺序方式将数据传输到磁盘，并从磁盘传输数据。如果顺序不可行，至少应考虑以大块传输数据：越大越好。如果 I/O 是以小而随机方式完成的，则 I/O 性能将受到显著影响。


##### 磁盘调度

* 由于 I/O 的高成本，操作系统在决定发送给磁盘的 I/O 顺序方面历来发挥作用。更具体地说，给定一组 I/O 请求，磁盘调度程序检查请求并决定下一个要调度的请求。
* 与任务调度不同，每个任务的长度通常是不知道的，对于磁盘调度，我们可以很好地猜测“任务”(即磁盘请求)需要多长时间。通过估计请求的查找和可能的旋转延迟，磁盘调度程序可以知道每个请求将花费多长时间，因此(贪婪地)选择先服务花费最少时间的请求。因此，磁盘调度程序将尝试在其操作中遵循 SJF(最短任务优先)的原则。
* SSTF：最短寻道时间优先  SSTF 按磁道对 I/O 请求队列排序，选择在最近磁道上的请求先完成。 SSTF 不是万能的，原因如下。
  * 第一个问题，主机操作系统无法利用驱动器的几何结构，而是只会看到一系列的块。幸运的是，这个问题很容易解决。作系统可以简单地实现最近块优先(Nearest-Block-First，NBF)，而不是 SSTF，然后用最近的块地址来调度请求。
  * 第二个问题更为根本：饥饿(starvation)。当对磁头当前所在位置的磁道有稳定的请求时, 纯粹的 SSTF 方法将完全忽略对其较远磁道的请求。

* 如何实现类 SSTF 调度，但避免饥饿？这个问题的答案是很久以前得到的，并且相对比较简单。该算法最初称为电梯(又称 SCAN 或 C-SCAN) 调度算法 ，简单地以跨越磁道的顺序来服务磁盘请求。我们将一次跨越磁盘称为扫一遍。因此，如果请求的块所属的磁道在这次扫一遍中已经服务过了，它就不会立即处理，而是排队等待下次扫一遍。
* SCAN 有许多变种，所有这些变种都是一样的。例如，Coffman 等人引入了 F-SCAN，它在扫一遍时冻结队列以进行维护。这个操作会将扫一遍期间进入的请求放入队列中，以便稍后处理。这样做可以避免远距离请求饥饿，延迟了迟到(但更近)请求的服务。C-SCAN 是另一种常见的变体，即循环 SCAN(Circular SCAN)的缩写。不是在一个方向扫过磁盘，该算法从外圈扫到内圈，然后从内圈扫到外圈，如此下去。然而，SCAN 及其变种并不是最好的调度技术。特别是，SCAN(甚至 SSTF)实际上并没有严格遵守 SJF 的原则。具体来说，它们忽视了旋转。

* SPTF：最短定位时间优先 

* 在现代系统上执行磁盘调度的地方在哪里？在较早的系统中，操作系统完成了所有的调度。在查看一系列挂起的请求之后，操作系统会选择最好的一个，并将其发送到磁盘。当该请求完成时，将选择下一个，如此下去。磁盘当年比较简单。在现代系统中，磁盘可以接受多个分离的请求，它们本身具有复杂的内部调度程序(它们可以准确地实现 SPTF。在磁盘控制器内部，所有相关细节都可以得到，包括精确的磁头位置)。因此，操作系统调度程序通常会选择它认为最好的几个请求，并将它们全部发送到磁盘。磁盘然后利用其磁头位置和详细的磁道布局信息等内部知识，以最佳可能(SPTF)顺序服务于这些请求。
* 磁盘调度程序执行的另一个重要相关任务是 I/O 合并。调度程序执行的所有请求都基于合并后的请求。合并在操作系统级别尤其重要，因为它减少了发送到磁盘的请求数量，从而降低了开销。
* 现代调度程序关注的最后一个问题是：在向磁盘发出 I/O 之前，系统应该等待多久？有人可能天真地认为，即使有一个磁盘 I/O，也应立即向驱动器发出请求。这种方法被称为工作保全(work-conserving)，因为如果有请求就要服务，磁盘将永远不会闲下来。然而，对预期磁盘调度的研究表明，有时最好等待一段时间，即所谓的非工作保全(non-work-conserving)方法。通过等待，新的和“更好”的请求可能会到达磁盘，从而整体效率提高。当然，决定何时等待以及多久可能会非常棘手。

#### 廉价冗余磁盘阵列(Redundant Array of Inexpensive Disks)

* 这种技术使用多个磁盘一起构建更快、更大、更可靠的磁盘系统。从外部看，RAID 看起来像一个磁盘：一组可以读取或写入的块。在内部，RAID 是一个复杂的庞然大物，由多个磁盘、内存(包括易失性和非易失性)以及一个或多个处理器来管理系统。硬件 RAID 非常像一个计算机系统，专门用于管理一组磁盘。
* 与单个磁盘相比，RAID 具有许多优点。一个好处就是性能。并行使用多个磁盘可以大大加快 I/O 时间。另一个好处是容量。大型数据集需要大型磁盘。最后，RAID 可以提高可靠性。在多个磁盘上传输数据(无 RAID 技术)会使数据容易受到单个磁盘丢失的影响。通过某种形式的冗余，RAID 可以容许损失一个磁盘并保持运行，就像没有错误一样。	
* 在考虑如何向系统添加新功能时，应该始终考虑是否可以透明地添加这样的功能，而不需要对系统其余部分进行更改。要求彻底重写现有软件(或激进的硬件更改)会减少创意产生影响的机会。RAID 就是一个很好的例子，它的透明肯定有助于它的成功。RAID 为使用它们的系统透明地提供了这些优势，即RAID 对于主机系统看起来就像一个大磁盘。当然，透明的好处在于它可以简单地用 RAID 替换磁盘，而不需要更换一行软件。操作系统和客户端应用程序无须修改，就可以继续运行。通过这种方式，透明极大地提高了 RAID 的可部署性，使用户和管理员可以使用 RAID，而不必担心软件兼容性问题。

* 当文件系统向 RAID 发出逻辑 I/O 请求时，RAID 内部必须计算要访问的磁盘(或多个磁盘)以完成请求，然后发出一个或多个物理 I/O 来执行此操作。这些物理 I/O 的确切性质取决于 RAID 级别。举一个简单的例子，考虑一个 RAID，它保留每个块的两个副本(每个都在一个单独的磁盘上)。当写入这种镜像RAID系统时，RAID 必须为它发出的每一个逻辑 I/O 执行两个物理 I/O。RAID 系统通常构建为单独的硬件盒，并通过标准连接(例如，SCSI 或 SATA)接入主机。然而，在内部，RAID 相当复杂。它包括一个微控制器，运行固件以指导 RAID 的操作。它还包括 DRAM 这样的易失性存储器，在读取和写入时缓冲数据块。在某些情况下，还包括非易失性存储器，安全地缓冲写入。它甚至可能包含专用的逻辑电路，来执行奇偶校验计算。在很高的层面上，RAID 是一个非常专业的计算机系统：它有一个处理器，内存和磁盘。然而，它不是运行应用程序，而是运行专门用于操作 RAID 的软件。


#### 文文件件和和目目录

* 永久存储设备永久地(或至少长时间地)存储信息，如传统硬盘驱动器或更现代的固态存储设备。持久存储设备与内存不同。内存在断电时，其内容会丢失，而持久存储设备会保持这些数据不变。因此，操作系统必须特别注意这样的设备：用户用它们保存真正关心的数据。

* 文件和目录 
* 随着时间的推移，存储虚拟化形成了两个关键的抽象。第一个是文件(file)。文件就是一个线性字节数组，每个字节都可以读取或写入。每个文件都有某种低级名称(low-level name)，通常是某种数字。用户通常不知道这个名字。由于历史原因，文件的低级名称通常称为 inode 号。在大多数系统中，操作系统不太了解文件的结构(例如，它是图片、文本文件还是 C代码)。相反，文件系统的责任仅仅是将这些数据永久存储在磁盘上，并确保当你再次请求数据时，得到你原来放在那里的内容。第二个抽象是目录(directory)。一个目录，像一个文件一样，也有一个低级名字(即 inode 号)，但是它的内容非常具体：它包含一个(用户可读名字，低级名字)对的列表。例如，假设存在一个低级别名称为“10”的文件，它的用户可读的名称为“foo”。“foo”所在的目录因此会有条目(“foo”，“10”)，将用户可读名称映射到低级名称。目录中的每个条目都指向文件或其他目录。通过将目录放入其他目录中，用户可以构建任意的目录树，在该目录树下存储所有文件和目录。
* 目录层次结构从根目录开始(在基于 UNIX 的系统中，根目录就记为 “/”)，并使用某种分隔符来命名后续子目录，直到命名所需的文件或目录。目录和文件可以具有相同的名称，只要它们位于文件系统树的不同位置。
* 名称在系统中很重要，因为访问任何资源的第一步是能够命名它。在 UNIX 系统中，文件系统提供了一种统一的方式来访问磁盘、U 盘、CD-ROM、许多其他设备上的文件，事实上还有很多其他的东西，都位于单一目录树下。

##### 文件系统接口

* 创建一个文件。这可以通过 open 系统调用完成。open() 的一个重要方面是它的返回值：文件描述符。文件描述符只是一个整数，是每个进程私有的，在 UNIX 系统中用于访问文件。因此，一旦文件被打开，你就可以使用文件描述符来读取或写入文件，假定你有权这样做。这样，一个文件描述符就是一种权限，即一个不透明的句柄，它可以让你执行某些操作。另一种看待文件描述符的方法，是将它作为指向文件类型对象的指针。一旦你有这样的对象，就可以调用其他“方法”来访问文件，如 read() 和 write()。
* 调用 lseek()与移动磁盘臂的磁盘的寻道(seek)操作无关。对 lseek()的调用只是改变内核中变量的值。执行 I/O 时，根据磁盘头的位置，磁盘可能会也可能不会执行实际的寻道来完成请求。命名糟糕的系统调用 lseek()让很多学生困惑，试图去理解磁盘以及其上的文件系统如何工作。不要混淆二者！lseek()调用只是在OS 内存中更改一个变量，该变量跟踪特定进程的下一个读取或写入开始的偏移量。如果发送到磁盘的读取或写入与最后一次读取或写入不在同一磁道上，就会发生磁盘寻道，因此需要磁头移动。更令人困惑的是，调用 lseek()从文件的随机位置读取或写入文件，然后读取/写入这些随机位置，确实会导致更多的磁盘寻道。因此，调用 lseek() 肯定会导致在即将进行的读取或写入中进行搜索，但绝对不会导致任何磁盘 I/O 自动发生。
* 大多数情况下，当程序调用 write()时，它只是告诉文件系统：请在将来的某个时刻，将此数据写入持久存储。出于性能的原因，文件系统会将这些写入在内存中缓冲(buffer)一段时间(例如 5s 或 30s)。在稍后的时间点，写入将实际发送到存储设备。从调用应用程序的角度来看，写入似乎很快完成，并且只有在极少数情况下(例如，在 write()调用之后但写入磁盘之前，机器崩溃)数据会丢失。
* 在 UNIX中，提供给应用程序的接口被称为 fsync(int fd)。当进程针对特定文件描述符调用 fsync()时，文件系统通过强制将所有脏(dirty)数据(即尚未写入的)写入磁盘来响应，针对指定文件描述符引用的文件。一旦所有这些写入完成，fsync()例程就会返回。
* rename()调用提供了一个有趣的保证：它(通常)是一个原子(atomic)调用，不论系统是否崩溃。如果系统在重命名期间崩溃，文件将被命名为旧名称或新名称，不会出现奇怪的中间状态。因此，对于支持某些需要对文件状态进行原子更新的应用程序，rename()非常重要。
* 文件系统能够保存关于它正在存储的每个文件的大量信息。我们通常将这些数据称为文件元数据。事实表明，每个文件系统通常将这种类型的信息保存在一个名为 inode的结构中。
* unlink()只需要待删除文件的名称，并在成功时返回零。	
* 除了文件外，还可以使用一组与目录相关的系统调用来创建、读取和删除目录。请注意，你永远不能直接写入目录。因为目录的格式被视为文件系统元数据，所以你只能间接更新目录，例如，通过在其中创建文件、目录或其他对象类型。通过这种方式，文件系统可以确保目录的内容始终符合预期。
* 要创建目录，可以用系统调用 mkdir()。
* opendir()、readdir()和 closedir()这 3 个调用来读取目录。
* 调用 rmdir()来删除目录。
* link()系统调用有两个参数：一个旧路径名和一个新路径名。当你将一个新的文件名“链接”到一个旧的文件名时，你实际上创建了另一种引用同一个文件的方法。link 只是在要创建链接的目录中创建了另一个名称，并将其指向原有文件的相同 inode 号(即低级别名称)。该文件不以任何方式复制。相反，你现在就有了两个人类可读的名称，都指向同一个文件。
* 创建一个文件时，实际上做了两件事。首先，要构建一个结构(inode)，它将跟踪几乎所有关于文件的信息，包括其大小、文件块在磁盘上的位置等等。其次，将人类可读的名称链接到该文件，并将该链接放入目录中。
* 在创建文件的硬链接之后，在文件系统中，原有文件名(file)和新创建的文件名(file2)之间没有区别。实际上，它们都只是指向文件底层元数据的链接。当文件系统取消链接文件时，它检查 inode 号中的引用计数(reference count)。该引用计数(有时称为链接计数，link count)允许文件系统跟踪有多少不同的文件名已链接到这个 inode。
* 调用 unlink()时，会删除人类可读的名称(正在删除的文件)与给定 inode 号之间的“链接”，并减少引用计数。只有当引用计数达到零时，文件系统才会释放inode 和相关数据块，从而真正“删除”该文件。
* 符号链接(symbolic link)，有时称为软链接(soft link)。事实表明，硬链接有点局限：你不能创建目录的硬链接(因为担心会在目录树中创建一个环)。你不能硬链接到其他磁盘分区中的文件(因为 inode 号在特定文件系统中是唯一的，而不是跨文件系统)，等等。因此，人们创建了一种称为符号链接的新型链接。符号链接实际上与硬链接完全不同。第一个区别是符号链接本身实际上是一个不同类型的文件。我们已经讨论过常规文件和目录。符号链接是文件系统知道的第三种类型。由于创建符号链接的方式，有可能造成所谓的悬空引用。
* 如何从许多底层文件系统组建完整的目录树。这项任务的实现是先制作文件系统，然后挂载它们，使其内容可以访问。为了创建一个文件系统，大多数文件系统提供了一个工具，通常名为 mkfs，它就是完成这个任务的。思路如下：作为输入，为该工具提供一个设备(例如磁盘分区，例如/dev/sda1)，一种文件系统类型(例如 ext3)，它就在该磁盘分区上写入一个空文件系统，从根目录开始。mkfs 说，要有文件系统！ 但是，一旦创建了这样的文件系统，就需要在统一的文件系统树中进行访问。这个任务是通过 mount 程序实现的(它使底层系统调用 mount()完成实际工作)。
* mount 的作用很简单：以现有目录作为目标挂载点(mount point)，本质上是将新的文件系统粘贴到目录树的这个点上。

#### 文件系统实现

* 文件系统是纯软件。与 CPU 和内存虚拟化的开发不同，我们不会添加硬件功能来使文件系统的某些方面更好地工作(但我们需要注意设备特性，以确保文件系统运行良好)。

* 第一个方面是文件系统的数据结构。换言之，文件系统在磁盘上使用哪些类型的结构来组织其数据和元数据？我们即将看到的第一个文件系统使用简单的结构，如块或其他对象的数组，而更复杂的文件系统(如 SGI 的 XFS)使用更复杂的基于树的结构。
* 文件系统的第二个方面是访问方法。如何将进程发出的调用，如 open()、read()、write()等，映射到它的结构上？在执行特定系统调用期间读取哪些结构？改写哪些结构？所有这些步骤的执行效率如何？对于文件系统，你的心智模型最终应该包含以下问题的答案：磁盘上的哪些结构存储文件系统的数据和元数据？当一个进程打开一个文件时会发生什么？在读取或写入期间访问哪些磁盘结构？
* 将磁盘分成块。简单的文件系统只使用一种块大小。我们对构建文件系统的磁盘分区的看法很简单：一系列块，每块大小为 4KB。在大小为 N 个 4KB 块的分区中，这些块的地址为从 0 到 N−1。将用于存放用户数据的磁盘区域称为数据区域，文件系统必须记录每个文件的信息。该信息是元数据的关键部分，并且记录诸如文件包含哪些数据块(在数据区域中)、文件的大小、其所有者和访问权限、访问和修改时间以及其他类似信息的事情。为了存储这些信息，文件系统通常有一个名为 inode 的结构。为了存放 inode，我们还需要在磁盘上留出一些空间。我们将这部分磁盘称为 inode 表 (inode table)，它只是保存了一个磁盘上 inode 的数组。到目前为止，我们的文件系统有了数据块(D)和 inode(I)，但还缺一些东西，还需要某种方法来记录 inode 或数据块是空闲还是已分配。因此，这种分配结构是所有文件系统中必需的部分。当然，可能有许多分配记录方法。例如，我们可以用一个空闲列表，指向第一个空闲块，然后它又指向下一个空闲块，依此类推。我们选择一种简单而流行的结构，称为位图，一种用于数据区域(数据位图，data bitmap)，另一种用于 inode 表(inode位图，inode bitmap)。位图是一种简单的结构：每个位用于指示相应的对象/块是空闲还是正在使用。
* 超级块包含关于该特定文件系统的信息，包括例如文件系统中有多少个 inode 和数据块、inode 表的开始位置等等。它可能还包括一些幻数，来标识文件系统类型。因此，在挂载文件系统时，操作系统将首先读取超级块，初始化各种参数，然后将该卷添加到文件系统树中。当卷中的文件被访问时，系统就会知道在哪里查找所需的磁盘上的结构。

##### 文件组织：inode 

* 文件系统最重要的磁盘结构之一是 inode，几乎所有的文件系统都有类似的结构。名称 inode 是 index node(索引节点)的缩写，因为这些节点最初放在一个数组中，在访问特定 inode 时会用到该数组的索引。每个 inode 都由一个数字(称为 inumber)隐式引用，我们之前称之为文件的低级名称(low-level name)。在 VSFS(和其他简单的文件系统)中，给定一个 inumber，你应该能够直接计算磁盘上相应节点的位置。

* 在每个 inode 中，实际上是所有关于文件的信息：文件类型(例如，常规文件、目录等)、大小、分配给它的块数、保护信息(如谁拥有该文件以及谁可以访问它)、一些时间信息(包括文件创建、修改或上次访问的时间文件下)、以及有关其数据块驻留在磁盘上的位置的信息(如某种类型的指针)。我们将所有关于文件的信息称为元数据(metadata)。实际上，文件系统中除了纯粹的用户数据外，其他任何信息通常都称为元数据。

* 设计 inode 时，最重要的决定之一是它如何引用数据块的位置。一种简单的方法是在 inode 中有一个或多个直接指针(磁盘地址)。每个指针指向属于该文件的一个磁盘块。这种方法有局限：例如，如果你想要一个非常大的文件(例如，大于块的大小乘以直接指针数)，那就不走运了。

  [!image](./images/44.png)
#### 崩溃一致性

* 与大多数数据结构不同(例如，正在运行的程序在内存中的数据结构)，文件系统数据结构必须持久，即它们必须长期存在，存储在断电也能保留数据的设备上。文件系统面临的一个主要挑战在于，如何在出现断电或系统崩溃的情况下，更新持久数据结构。具体来说，如果在更新磁盘结构的过程中，有人绊到电源线并且机器断电，会发生什么？或者操作系统遇到错误并崩溃？由于断电和崩溃，更新持久性数据结构可能非常棘手，并导致了文件系统实现中一个有趣的新问题，称为崩溃一致性问题(crash-consistency problem)。
* 为了完成特定操作，你必须更新两个磁盘上的结构 A和 B。由于磁盘一次只为一个请求提供服务，因此其中一个请求将首先到达磁盘(A 或 B)。如果在一次写入完成后系统崩溃或断电，则磁盘上的结构将处于不一致(inconsistent)的状态。
* 系统可能在任何两次写入之间崩溃或断电，因此磁盘上状态可能仅部分地更新。崩溃后，系统启动并希望再次挂载文件系统(以便访问文件等)。鉴于崩溃可能发生在任意时间点，如何确保文件系统将磁盘上的映像保持在合理的状态？崩溃一致性问题由于崩溃而导致磁盘文件系统映像可能出现的许多问题：
* 在文件系统数据结构中可能存在不一致性。可能有空间泄露，可能将垃圾数据返回给用户，等等。理想的做法是将文件系统从一个一致状态(在文件被追加之前)，原子地移动到另一个状态(在 inode、位图和新数据块被写入磁盘之后)。遗憾的是，做到这一点不容易，因为磁盘一次只提交一次写入，而这些更新之间可能会发生崩溃或断电)。

##### 文件系统检查程序 

* 早期的文件系统采用了一种简单的方法来处理崩溃一致性。基本上，它们决定让不一致的事情发生，然后再修复它们(重启时)。这种偷懒方法的典型例子可以在一个工具中找到：fsck。fsck 是一个 UNIX 工具，用于查找这些不一致并修复它们。在不同的系统上，存在检查和修复磁盘分区的类似工具。请注意，这种方法无法解决所有问题。例如，文件系统看起来是一致的，但是 inode 指向垃圾数据(在inode和inode位图保存成功后发生的奔溃)。唯一真正的目标，是确保文件系统元数据内部一致。
* 工具 fsck 在许多阶段运行。它在文件系统挂载并可用之前运行(fsck 假定在运行时没有其他文件系统活动正在进行)。一旦完成，磁盘上的文件系统应该是一致的，因此可以让用户访问。	
* 超级块：fsck 首先检查超级块是否合理，主要是进行健全性检查，例如确保文件系统大小大于分配的块数。通常，这些健全性检查的目的是找到一个可疑的(冲突的)超级块。在这种情况下，系统(或管理员)可以决定使用超级块的备用副本。
*  空闲块：接下来，fsck 扫描 inode、间接块、双重间接块等，以了解当前在文件系统中分配的块。它利用这些知识生成正确版本的分配位图。因此，如果位图和 inode 之间存在任何不一致，则通过信任 inode 内的信息来解决它。对所有 inode 执行相同类型的检查，确保所有看起来像在用的 inode，都在 inode 位图中有标记。
*  inode 状态：检查每个 inode 是否存在损坏或其他问题。例如，fsck 确保每个分配的 inode 具有有效的类型字段(即常规文件、目录、符号链接等)。如果 inode 字段存在问题，不易修复，则 inode 被认为是可疑的，并被 fsck 清除，inode 位图相应地更新。
*  inode 链接：fsck 还会验证每个已分配的 inode 的链接数。你可能还记得，链接计数表示包含此特定文件的引用(即链接)的不同目录的数量。为了验证链接计数，fsck 从根目录开始扫描整个目录树，并为文件系统中的每个文件和目录构建自己的链接计数。如果新计算的计数与 inode 中找到的计数不匹配，则必须采取纠正措施，通常是修复 inode 中的计数。如果发现已分配的 inode 但没有目录引用它，则会将其移动到 lost + found 目录。
* 重复：fsck 还检查重复指针，即两个不同的 inode 引用同一个块的情况。如果一个 inode 明显不好，可能会被清除。或者，可以复制指向的块，从而根据需要为每个 inode 提供其自己的副本。
*  坏块：在扫描所有指针列表时，还会检查坏块指针。如果指针显然指向超出其有效范围的某个指针，则该指针被认为是“坏的”，例如，它的地址指向大于分区大小的块。在这种情况下，fsck 不能做任何太聪明的事情。它只是从 inode 或间接块中删除(清除)该指针。
* 目录检查：fsck 不了解用户文件的内容。但是，目录包含由文件系统本身创建的特定格式的信息。因此，fsck 对每个目录的内容执行额外的完整性检查，确保“.”和“..”是前面的条目，目录条目中引用的每个 inode 都已分配，并确保整个层次结构中没有目录的引用超过一次。
* 如你所见，构建有效工作的 fsck 需要复杂的文件系统知识。确保这样的代码在所有情况下都能正常工作可能具有挑战性。然而，fsck(和类似的方法)有一个更大的、也许更根本的问题：它们太慢了。对于非常大的磁盘卷，扫描整个磁盘，以查找所有已分配的块并读取整个目录树，可能需要几分钟或几小时。随着磁盘容量的增长和 RAID 的普及，fsck 的性能变得令人望而却步。

##### 日志(或预写日志) 

* 对于一致更新问题，最流行的解决方案可能是从数据库管理系统的世界中借鉴的一个想法。这种名为预写日志的想法，是为了解决这类问题而发明的。

* 在文件系统中，出于历史原因，我们通常将预写日志称为日志。第一个实现它的文件系统是 Cedar，但许多现代文件系统都使用这个想法，包括 Linux ext3 和 ext4、reiserfs、IBM 的 JFS、SGI 的 XFS 和 Windows NTFS。

* 基本思路如下。更新磁盘时，在覆写结构之前，首先写下一点小注记(在磁盘上的其他地方，在一个众所周知的位置)，描述你将要做的事情。写下这个注记就是“预写”部分，

* 我们把它写入一个结构，并组织成“日志”。因此，就有了预写日志。通过将注释写入磁盘，可以保证在更新(覆写)正在更新的结构期间发生崩溃时，能够返回并查看你所做的注记，然后重试。因此，你会在崩溃后准确知道要修复的内容(以及如何修复它)，而不必扫描整个磁盘。因此，通过设计，日志功能在更新期间增加了一些工作量，从而大大减少了恢复期间所需的工作量。

* 我们现在将描述 Linux ext3如何将日志记录到文件系统中。大多数磁盘上的结构与 Linux ext2 相同，例如，磁盘被分成块组，每个块组都有一个 inode 和数据位图以及 inode 和数据块。新的关键结构是日志本身，它占用分区内或其他设备上的少量空间。因此，ext2 文件系统(没有日志)看起来像这样：

  [!image](./images/45.png)

  假设日志放在同一个文件系统映像中(虽然有时将它放在单独的设备上，或作为文件系统中的文件)，带有日志的 ext3 文件系统如下所示：

  [!image](./images/46.png)

  真正的区别只是日志的存在，当然，还有它的使用方式。

##### 数据日志 

* 事务开始(TxB)告诉我们有关此更新的信息，包括对文件系统即将进行的更新的相关信息，以及某种事务标识符(transaction identifier，TID)。中间的 3 个块只包含块本身的确切内容，这被称为物理日志，因为我们将更新的确切物理内容放在日志中(另一种想法，逻辑日志(logical logging)，在日志中放置更紧凑的更新逻辑表示，例如，“这次更新希望将数据块 Db 追加到文件 X”，这有点复杂，但可以节省日志中的空间，并可能提高性能)。
* 一旦这个事务安全地存在于磁盘上，我们就可以覆写文件系统中的旧结构了。这个过程称为加检查点。因此，为了对文件系统加检查点(checkpoint，即让它与日志中即将进行的更新一致)，我们将待处理的元数据和数据更新写入其磁盘位置。如果这些写入成功完成，我们已成功地为文件系统加上了检查点，基本上完成了。因此，我们的初始操作顺序如下。
  * 日志写入：将事务(包括事务开始块，所有即将写入的数据和元数据更新以及事务结束块)写入日志，等待这些写入完成。
  * 加检查点：将待处理的元数据和数据更新写入文件系统中的最终位置
* 在写入日志期间发生崩溃时，事情变得有点棘手。
* 强制写入磁盘为了在两次磁盘写入之间强制执行顺序，现代文件系统必须采取一些额外的预防措施。在过去，强制在两个写入 A 和 B 之间进行顺序很简单：只需向磁盘发出 A 写入，等待磁盘在写入完成时中断 OS，然后发出写入 B。由于磁盘中写入缓存的使用增加，事情变得有点复杂了。启用写入缓冲后(有时称为立即报告，immediate reporting)，如果磁盘已经放入磁盘的内存缓存中、但尚未到达磁盘，磁盘就会通知操作系统写入完成。如果操作系统随后发出后续写入，则无法保证它在先前写入之后到达磁盘。因此，不再保证写入之间的顺序。一种解决方案是禁用写缓冲。然而，更现代的系统采取额外的预防措施，发出明确的写入屏障。这样的屏障，当它完成时，能确保在屏障之前发出的所有写入，先于在屏障之后发出的所有写入到达磁盘。
* 所有这些机制都需要对磁盘的正确操作有很大的信任。遗憾的是，最近的研究表明，为了提供“性能更高”的磁盘，一些磁盘制造商显然忽略了写屏障请求，从而使磁盘看起来运行速度更快，但存在操作错误的风险。

##### 优化日志写入

* 你可能已经注意到，写入日志的效率特别低。也就是说，文件系统首先必须写出事务开始块和事务的内容。只有在这些写入完成后，文件系统才能将事务结束块发送到磁盘。

* 将事务写入日志时，在开始和结束块中包含日志内容的校验和。这样做可以使文件系统立即写入整个事务，而不会产生等待。如果在恢复期间，文件系统发现计算的校验和与事务中存储的校验和不匹配，则可以断定在写入事务期间发生了崩溃，从而丢弃了文件系统更新。因此，通过写入协议和恢复系统中的小调整，文件系统可以实现更快的通用情况性能。最重要的是，系统更可靠了，因为来自日志的任何读取现在都受到校验和的保护。

* 文件系统分两步发出事务写入。首先，它将除 TxE 块之外的所有块写入日志，同时发出这些写入。当这些写入完成时，文件系统会发出 TxE 块的写入，从而使日志处于最终的安全状态：此过程的一个重要方面是磁盘提供的原子性保证。事实证明，磁盘保证任何 512 字节写入都会发生或不发生(永远不会半写)。因此，为了确保 TxE 的写入是原子的，应该使它成为一个 512 字节的块。因此，我们当前更新文件系统的协议如下，3 个阶段中的每一个都标上了名称。

  1. 日志写入：将事务的内容(包括 TxB、元数据和数据)写入日志，等待这些写入完成。

  2. 日志提交：将事务提交块(包括 TxE)写入日志，等待写完成，事务被认为已提交(committed)。 

  3. 加检查点：将更新内容(元数据和数据)写入其最终的磁盘位置。

##### 恢复 

* 现在来了解文件系统如何利用日志内容从崩溃中恢复。在这个更新序列期间，任何时候都可能发生崩溃。如果崩溃发生在事务被安全地写入日志之前(在上面的步骤 2完成之前)，那么我们的工作很简单：简单地跳过待执行的更新。如果在事务已提交到日志之后但在加检查点完成之前发生崩溃，则文件系统可以按如下方式恢复更新。
* 系统引导时，文件系统恢复过程将扫描日志，并查找已提交到磁盘的事务。然后，这些事务被重放(replayed，按顺序)，文件系统再次尝试将事务中的块写入它们最终的磁盘位置。这种形式的日志是最简单的形式之一，称为重做日志。通过在日志中恢复已提交的事务，文件系统确保磁盘上的结构是一致的，因此可以继续工作，挂载文件系统并为新请求做好准备。
* 即使在某些更新写入块的最终位置之后，在加检查点期间的任何时刻发生崩溃，都没问题。在最坏的情况下，其中一些更新只是在恢复期间再次执行。因为恢复是一种罕见的操作(仅在系统意外崩溃之后发生)，所以几次冗余写入无须担心。

* 基本协议可能会增加大量额外的磁盘流量。例如，假设我们在同一目录中连续创建两个文件，称为 file1 和 file2。要创建一个文件，必须更新许多磁盘上的结构，至少包括：inode 位图(分配新的 inode)，新创建的文件 inode，包含新文件目录条目的父目录的数据块，以及父目录的 inode(现在有一个新的修改时间)。通过日志，我们将所有这些信息逻辑地提交给我们的两个文件创建的日志。因为文件在同一个目录中，我们假设在同一个 inode 块中都有 inode，这意味着如果不小心，我们最终会一遍又一遍地写入这些相同的块。为了解决这个问题，一些文件系统不会一次一个地向磁盘提交每个更新(例如，Linux ext3)。与此不同，可以将所有更新缓冲到全局事务中。当创建两个文件时，文件系统只将内存中的 inode 位图、文件的 inode、目录数据和目录 inode 标记为脏，并将它们添加到块列表中，形成当前的事务。当最后应该将这些块写入磁盘时(例如，在超时 5s 之后)，会提交包含上述所有更新的单个全局事务。因此，通过缓冲更新，文件系统在许多情况下可以避免对磁盘的过多的写入流量。

##### 使日志有限 

* 我们已经了解了更新文件系统磁盘结构的基本协议。文件系统缓冲内存中的更新一段时间。最后写入磁盘时，文件系统首先仔细地将事务的详细信息写入日志(即预写日志)。事务完成后，文件系统会加检查点，将这些块写入磁盘上的最终位置。但是，日志的大小有限。如果不断向它添加事务，它将很快填满。你觉得会发生什么？

* 日志满时会出现两个问题。第一个问题比较简单，但不太重要：日志越大，恢复时间越长，因为恢复过程必须重放日志中的所有事务(按顺序)才能恢复。第二个问题更重要：当日志已满(或接近满)时，不能向磁盘提交进一步的事务，从而使文件系统“不太有用”(即无用)。

* 为了解决这些问题，日志文件系统将日志视为循环数据结构，一遍又一遍地重复使用。这就是为什么日志有时被称为循环日志。为此，文件系统必须在加检查点之后的某个时间执行操作。具体来说，一旦事务被加检查点，文件系统应释放它在日志中占用的空间，允许重用日志空间。有很多方法可以达到这个目的。例如，你只需在日志超级块(journal superblock)中标记日志中最旧和最新的事务。所有其他空间都是空闲的。以下是这种机制的图形描述：

  [!image](./images/47.png)

  在日志超级块中(不要与主文件系统的超级块混淆)，日志系统记录了足够的信息，以了解哪些事务尚未加检查点，从而减少了恢复时间，并允许以循环的方式重新使用日志。因此，我们在基本协议中添加了另一个步骤。

  1. 日志写入：将事务的内容(包括 TxB 和更新内容)写入日志，等待这些写入完成。
  2. 日志提交：将事务提交块(包括 TxE)写入日志，等待写完成，事务被认为已提交。 
  3. 加检查点：将更新内容写入其最终的磁盘位置。
  4. 释放：一段时间后，通过更新日志超级块，在日志中标记该事务为空闲。

  因此，我们得到了最终的数据日志协议。但仍然存在一个问题：我们将每个数据块写入磁盘两次，这是沉重的成本，特别是为了系统崩溃这样罕见的事情。你能找到一种方法来保持一致性，而无须两次写入数据吗？

##### 元数据日志 

* 尽管恢复现在很快(扫描日志并重放一些事务而不是扫描整个磁盘)，但文件系统的正常操作比我们想要的要慢。特别是，对于每次写入磁盘，我们现在也要先写入日志，从而使写入流量加倍。在顺序写入工作负载期间，这种加倍尤为痛苦，现在将以驱动器峰值写入带宽的一半进行。此外，在写入日志和写入主文件系统之间，存在代价高昂的寻道，这为某些工作负载增加了显著的开销。

* 由于将每个数据块写入磁盘的成本很高，人们为了提高性能，尝试了一些不同的东西。例如，我们上面描述的日志模式通常称为数据日志，因为它记录了所有用户数据(除了文件系统的元数据之外)。一种更简单(也更常见)的日志形式有时称为有序日志(ordered journaling，或称为元数据日志，metadata journaling)，它几乎相同，只是用户数据没有写入日志。因此，在执行与上述相同的更新时，以下信息将写入日志：

  [!image](./images/48.png)

  先前写入日志的数据块 Db 将改为写入文件系统，避免额外写入。考虑到磁盘的大多数 I/O 流量是数据，不用两次写入数据会大大减少日志的 I/O 负载。然而，修改确实提出了一个有趣的问题：我们何时应该将数据块写入磁盘？

* 事实证明，数据写入的顺序对于仅元数据日志很重要。例如，如果我们在事务(包含 I[v2]和 B[v2])完成后将 Db 写入磁盘如何？遗憾的是，这种方法存在一个问题：文件系统是一致的，但 I[v2]可能最终指向垃圾数据。具体来说，考虑写入了 I[v2]和 B[v2]，但 Db 没有写入磁盘的情况。然后文件系统将尝试恢复。由于 Db 不在日志中，因此文件系统将重放对 I[v2]和 B[v2]的写入，并生成一致的文件系统。但是，I[v2]将指向垃圾数据，即指向 Db 中的任何数据。为了确保不出现这种情况，在将相关元数据写入磁盘之前，一些文件系统先将数据块(常规文件)写入磁盘。具体来说，协议有以下几个。

  1. 数据写入：将数据写入最终位置，等待完成(等待是可选的)。 
  2. 日志元数据写入：将开始块和元数据写入日志，等待写入完成。
  3. 日志提交：将事务提交块(包括 TxE)写入日志，等待写完成，现在认为事务(包括数据)已提交(committed)。 
  4. 加检查点元数据：将元数据更新的内容写入文件系统中的最终位置。
  5. 释放：稍后，在日志超级块中将事务标记为空闲。

  通过强制先写入数据，文件系统可以保证指针永远不会指向垃圾。实际上，这个“先写入被指对象，再写入指针对象”的规则是崩溃一致性的核心，并且被其他崩溃一致性方案进一步利用。

* 在大多数系统中，元数据日志(类似于 ext3 的有序日志)比完整数据日志更受欢迎。例如，Windows NTFS 和 SGI 的 XFS 都使用无序的元数据日志。Linux ext3 为你提供了选择数据、有序或无序模式的选项(在无序模式下，可以随时写入数据)。所有这些模式都保持元数据一致，它们的数据语义各不相同。最后，请注意，在发出写入日志(步骤 2)之前强制数据写入完成(步骤 1)不是正确性所必需的，如上面的协议所示。具体来说，可以发出数据写入，并向日志写入事务开始块和元数据。唯一真正的要求，是在发出日志提交块之前完成步骤 1 和步骤 2。

#### 日志结构文件系统

* 在 20 世纪 90 年代早期，由 John Ousterhout 教授和研究生 Mendel Rosenblum 领导的伯克利小组开发了一种新的文件系统，称为日志结构文件系统。他们这样做的动机是基于以下观察。 内存大小不断增长。随着内存越来越大，可以在内存中缓存更多数据。随着更多数据的缓存，磁盘流量将越来越多地由写入组成，因为读取将在缓存中进行处理。因此，文件系统性能很大程度上取决于写入性能。
*  随机 I/O 性能与顺序 I/O 性能之间存在巨大的差距，且不断扩大：传输带宽每年增加约 50%～100%。寻道和旋转延迟成本下降得较慢，可能每年 5%～10%。因此，如果能够以顺序方式使用磁盘，则可以获得巨大的性能优势，随着时间的推移而增长。现有文件系统在许多常见工作负载上表现不佳。例如，FFS会执行大量写入，以创建大小为一个块的新文件：一个用于新的 inode，一个用于更新 inode 位图，一个用于文件所在的目录数据块，一个用于目录 inode 以更新它，一个用于新数据块，它是新文件的一部分，另一个是数据位图，用于将数据块标记为已分配。
* 因此，尽管 FFS 会将所有这些块放在同一个块组中，但 FFS 会导致许多短寻道和随后的旋转延迟，因此性能远远低于峰值顺序带宽。文件系统不支RAID。例如，RAID-4 和 RAID-5 具有小写入问题，即对单个块的逻辑写入会导致 4 个物理 I/O 发生。现有的文件系统不会试图避免这种最坏情况的 RAID 写入行为。
* 因此，理想的文件系统会专注于写入性能，并尝试利用磁盘的顺序带宽。此外，它在常见工作负载上表现良好，这种负载不仅写出数据，还经常更新磁盘上的元数据结构。最后，它可以在 RAID 和单个磁盘上运行良好。引入的新型文件系统 Rosenblum 和 Ousterhout 称为 LFS，是日志结构文件系统(Log-structured File System)的缩写。写入磁盘时，LFS 首先将所有更新(包括元数据！)缓冲在内存段中。当段已满时，它会在一次长时间的顺序传输中写入磁盘，并传输到磁盘的未使用部分。LFS 永远不会覆写现有数据，而是始终将段写入空闲位置。由于段很大，因此可以有效地使用磁盘，并且文件系统的性能接近其峰值。

#### 数据完整性

* 确保放入存储系统的数据就是存储系统返回的数据。
* 现代磁盘似乎大部分时间正常工作，但是无法成功访问一个或几个块。具体来说，两种类型的单块故障是常见的，值得考虑：潜在扇区错误(Latent-Sector Errors，LSE)和块讹误(block corruption)。
* 当磁盘扇区(或扇区组)以某种方式讹误时，会出现 LSE。例如，如果磁头由于某种原因接触到表面(磁头碰撞，head crash，在正常操作期间不应发生的情况)，则可能会讹误表面，使得数据位不可读。宇宙射线也会导致数据位翻转，使内容不正确。幸运的是，驱动器使用磁盘内纠错码(Error Correcting Code，ECC)来确定块中的磁盘位是否良好，并且在某些情况下，修复它们。如果它们不好，并且驱动器没有足够的信息来修复错误，则在发出请求读取它们时，磁盘会返回错误。
* 还存在一些情况，磁盘块出现讹误(corrupt)，但磁盘本身无法检测到。例如，有缺陷的磁盘固件可能会将块写入错误的位置。在这种情况下，磁盘 ECC 指示块内容很好，但是从客户端的角度来看，在随后访问时返回错误的块。类似地，当一个块通过有故障的总线从主机传输到磁盘时，它可能会讹误。由此产生的讹误数据会存入磁盘，但它不是客户所希望的。这些类型的故障特别隐蔽，因为它们是无声的故障(silent fault)。返回故障数据时，磁盘没有报告问题
* Prabhakaran 等人将这种更现代的磁盘故障视图描述为故障—部分磁盘故障模型。在此视图中，磁盘仍然可以完全失败(像传统的故障停止模型中的情况)。然而，磁盘也可以似乎正常工作，并有一个或多个块变得不可访问(即 LSE)或保存了错误的内容(即讹误)。因此，当访问看似工作的磁盘时，偶尔会在尝试读取或写入给定块时返回错误(非无声的部分故障)，偶尔可能只是返回错误的数据(一个无声的部分错误)。
* 潜在的扇区错误很容易处理，因为它们很容易被检测到。当存储系统尝试访问块，并且磁盘返回错误时，存储系统应该就用它具有的任何冗余机制，来返回正确的数据。例如，在镜像 RAID 中，系统应该访问备用副本。在基于奇偶校验的 RAID-4 或 RAID-5 系统中，系统应通过奇偶校验组中的其他块重建该块。因此，利用标准冗余机制，可以容易地恢复诸如 LSE 这样的容易检测到的问题。
* 多年来，LSE 的日益增长影响了 RAID 设计。当全盘故障和 LSE 接连发生时，RAID-4/5系统会出现一个特别有趣的问题。具体来说，当整个磁盘发生故障时，RAID 会尝试读取奇偶校验组中的所有其他磁盘，并重新计算缺失值，来重建磁盘(例如，在热备用磁盘上)。如果在重建期间，在任何一个其他磁盘上遇到 LSE，我们就会遇到问题：重建无法成功完成。
* 为了解决这个问题，一些系统增加了额外的冗余度。例如，NetApp 的 RAID-DP 相当于两个奇偶校验磁盘，而不是一个。在重建期间发现 LSE 时，额外的校验盘有助于重建丢失的块。与往常一样，这有成本，因为为每个条带维护两个奇偶校验块的成本更高。但是，NetApp WAFL 文件系统的日志结构特性在许多情况下降低了成本。另外的成本是空间，需要额外的磁盘来存放第二个奇偶校验块。


#### 分布式系统

* 分布式系统改变了世界的面貌。当你的 Web 浏览器连接到地球上其他地方的 Web 服务器时，它就会参与似乎是简单形式的客户端/服务器（client/server）分布式系统。当你连上Google 和 Facebook 等现代网络服务时，不只是与一台机器进行交互。在幕后，这些复杂的服务是利用大量机器（成千上万台）来提供的，每台机器相互合作，以提供站点的特定服务。
* 构建分布式系统时会出现许多新的挑战。我们关注的主要是故障（failure）。机器、磁盘、网络和软件都会不时故障，因为我们不知道（并且可能永远不知道）如何构建“完美”的组件和系统。但是，构建一个现代的 Web 服务时，我们希望它对客户来说就像永远不会失败一样。
* 虽然故障是构建分布式系统的核心挑战，但它也代表着一个机遇。是的，机器会故障。但是机器故障这一事实并不意味着整个系统必须失败。通过聚集一组机器，我们可以构建一个看起来很少失败的系统，尽管它的组件经常出现故障。这种现实是分布式系统的核心优点和价值，也是为什么它们几乎支持了你使用的所有现代 Web 服务，包括Google、Facebook 等。
* 几乎在所有情况下，将通信视为根本不可靠的活动是很好的。位讹误、关闭或无效的链接和机器，以及缺少传入数据包的缓冲区空间，都会导致相同的结果：数据包有时无法到达目的地。为了在这种不可靠的网络上建立可靠的服务，我们必须考虑能够应对数据包丢失的技术。
* 系统性能（performance）通常很关键。对于将分布式系统连接在一起的网络，系统设计人员必须经常仔细考虑如何完成给定的任务，尝试减少发送的消息数量，并进一步使通信尽可能高效（低延迟、高带宽）。
* 安全（security）也是必要的考虑因素。连接到远程站点时，确保远程方是他们声称的那些人，这成为一个核心问题。此外，确保第三方无法监听或改变双方之间正在进行的通信，也是一项挑战。
* 布式系统中最基本的新方面：通信（communication）。也就是说，分布式系统中的机器应该如何相互通信？我们将从可用的最基本原语（消息）开始，并在它们之上构建一些更高级的原语。正如上面所说的，故障将是重点：通信层应如何处理故障？

##### 通信基础 

* 现代网络的核心原则是，通信基本是不可靠的。无论是在广域 Internet，还是 Infiniband 等局域高速网络中，数据包都会经常丢失、损坏，或无法到达目的地。
* 数据包丢失或损坏的原因很多。有时，在传输过程中，由于电气或其他类似问题，某些位会被翻转。有时，系统中的某个元素（例如网络链接或数据包路由器，甚至远程主机）会以某种方式损坏，或以其他方式无法正常工作。网络电缆确实会意外地被切断，至少有时候。然而，更基本的是由于网络交换机、路由器或终端节点内缺少缓冲，而导致数据包丢失。
* 具体来说，即使我们可以保证所有链路都能正常工作，并且系统中的所有组件（交换机、路由器、终端主机）都按预期启动并运行，仍然可能出现丢失，原因如下。想象一下数据包到达路由器。对于要处理的数据包，它必须放在路由器内某处的内存中。如果许多此类数据包同时到达，则路由器内的内存可能无法容纳所有数据包。此时路由器唯一的选择是丢弃一个或多个数据包。同样的行为也发生在终端主机上。当你向单台机器发送大量消息时，机器的资源很容易变得不堪重负，从而再次出现丢包现象。因此，丢包是网络的基本现象。所以问题变成：应该如何处理丢包？

##### 不可靠的通信层 

* 一个简单的方法是：我们不处理它。由于某些应用程序知道如何处理数据包丢失，因此让它们用基本的不可靠消息传递层进行通信有时很有用，这是端到端的论点的一个例子。这种不可靠层的一个很好的例子，就是几乎所有现代系统中都有的 UDP/IP 网络栈。要使用 UDP，进程使用套接字（socket）API 来创建通信端点（communication endpoint）。其他机器（或同一台机器上）的进程将 UDP 数据报（datagram）发送到前面的进程（数据报是一个固定大小的消息，有最大大小）。
* 一个基于 UDP/IP 构建的简单客户端和服务器。客户端可以向服务器发送消息，然后服务器响应回复。UDP 是不可靠通信层的一个很好的例子。如果你使用它，就会遇到数据包丢失（丢弃），从而无法到达目的地的情况。发送方永远不会被告知丢失。但是，这并不意味着 UDP 根本不能防止任何故障。例如，UDP 包含校验和（checksum），以检测某些形式的数据包损坏。
* 但是，由于许多应用程序只是想将数据发送到目的地，而不想考虑丢包，所以我们需要更多。具体来说，我们需要在不可靠的网络之上进行可靠的通信。
* 校验和是在现代系统中快速有效地检测讹误的常用方法。一个简单的校验和是加法：就是将一大块数据的字节加起来。当然，人们还创建了许多其他更复杂的校验和，包括基本的循环冗余校验码（CRC）、Fletcher 校验和以及许多其他方法。
* 在网络中，校验和使用如下：在将消息从一台计算机发送到另一台计算机之前，计算消息字节的校验和。然后将消息和校验和发送到目的地。在目的地，接收器也计算传入消息的校验和。如果这个计算的校验和与发送的校验和匹配，则接收方可以确保数据在传输期间很可能没有被破坏。
* 校验和可以从许多不同的方面进行评估。有效性是一个主要考虑因素：数据的变化是否会导致校验和的变化？校验和越强，数据变化就越难被忽视。性能是另一个重要标准：计算校验和的成本是多少？遗憾的是，有效性和性能通常是不一致的，这意味着高质量的校验和通常很难计算。

##### 可靠的通信层 

* 为了构建可靠的通信层，我们需要一些新的机制和技术来处理数据包丢失。考虑一个简单的示例，其中客户端通过不可靠的连接向服务器发送消息。我们必须回答的第一个问题是：发送方如何知道接收方实际收到了消息？我们要使用的技术称为确认（acknowledgment），或简称为 ack。这个想法很简单：发送方向接收方发送消息，接收方然后发回短消息确认收到。当发送方收到该消息的确认时，它可以放心接收方确实收到了原始消息。但是，如果没有收 到确认，发送方应该怎么办？

* 为了处理这种情况，我们需要一种额外的机制，称为超时（timeout）。当发送方发送消息时，发送方现在将计时器设置为在一段时间后关闭。如果在此时间内未收到确认，则发送方断定该消息已丢失。发送方然后就重试（retry）发送，再次发送相同的消息，希望这次它能送达。要让这种方法起作用，发送方必须保留一份消息副本，以防它需要再次发送。超时和重试的组合导致一些人称这种方法为超时/重试（timeout/retry）。

* 遗憾的是，这种形式的超时/重试还不够。图 47.5 展示了可能导致故障的数据包丢失示例。在这个例子中，丢失的不是原始消息，而是确认消息。从发送方的角度来看，情况似乎是相同的：没有收到确认，因此超时和重试是合适的。但是从接收方的角度来看，完全不同：现在相同的消息收到了两次！虽然可能存在这种情况，但通常情况并非如此。设想下载文件时，在下载过程中重复多个数据包，会发生什么。因此，如果目标是可靠的消息层，我们通常还希望保证接收方每个消息只接收一次（exactly once）。

  [!image](./images/55.png)

  为了让接收方能够检测重复的消息传输，发送方必须以某种独特的方式标识每个消息，并且接收方需要某种方式来追踪它是否已经看过每个消息。当接收方看到重复传输时，它只是简单地响应消息，但（严格地说）不会将消息传递给接收数据的应用程序。因此，发送方收到确认，但消息未被接收两次，保证了上面提到的一次性语义。

* 为了让接收方能够检测重复的消息传输，发送方必须以某种独特的方式标识每个消息，并且接收方需要某种方式来追踪它是否已经看过每个消息。当接收方看到重复传输时，它只是简单地响应消息，但（严格地说）不会将消息传递给接收数据的应用程序。因此，发送方收到确认，但消息未被接收两次，保证了上面提到的一次性语义。有许多方法可以检测重复的消息。例如，发送方可以为每条消息生成唯一的 ID。接收方可以追踪它所见过的每个 ID。这种方法可行，但它的成本非常高，需要无限的内存来跟踪所有 ID。一种更简单的方法，只需要很少的内存，解决了这个问题，该机制被称为顺序计数器（sequence counter）。利用顺序计数器，发送方和接收方就每一方将维护的计数器的起始值达成一致（例如 1）。无论何时发送消息，计数器的当前值都与消息一起发送。此计数器值（N）作为消息的 ID。发送消息后，发送方递增该值（到 N + 1）。接收方使用其计数器值，作为发送方传入消息的 ID 的预期值。如果接收的消息（N） 的 ID 与接收方的计数器匹配（也是 N），它将确认该消息，将其传递给上层的应用程序。在这种情况下，接收方断定这是第一次收到此消息。接收方然后递增其计数器（到 N + 1）并等待下一条消息。如果确认丢失，则发送方将超时，并重新发送消息 N。这次，接收器的计数器更高（N+1），因此接收器知道它已经接收到该消息。因此它会确认该消息，但不会将其传递给应用程序。以这种简单的方式，顺序计数器可以避免重复。
* 最常用的可靠通信层称为 TCP/IP，或简称为 TCP。TCP 比上面描述的要复杂得多，包括处理网络拥塞的机制，多个未完成的请求，以及数百个其他的小调整和优化。
* 正确设置超时值，是使用超时重试消息发送的一个重要方面。如果超时太小，发送方将不必要地重新发送消息，从而浪费发送方的 CPU 时间和网络资源。如果超时太大，则发送方为重发等待太长时间，因此会感到发送方的性能降低。所以，从单个客户端和服务器的角度来看，“正确”值就是等待足够长的时间来检测数据包丢失，但不能再长。但是，分布式系统中通常不只有一个客户端和服务器。在许多客户端发送到单个服务器的情况下，服务器上的数据包丢失可能表明服务器过载。如果是这样，则客户端可能会以不同的自适应方式重试。例如，在第一次超时之后，客户端可能会将其超时值增加到更高的量，可能是原始值的两倍。这种指数倒退（exponential back-off）方案，在早期的 Aloha 网络中实施，并在早期的以太网中采用，避免了资源因过量重发而过载的情况。健壮的系统力求避免这种过载。

##### DSM

* 多年来，系统社区开发了许多方法。其中一项工作涉及操作系统抽象，将其扩展到在分布式环境中运行。例如，分布式共享内存（Distributed Shared Memory，DSM）系统使不同机器上的进程能够共享一个大的虚拟地址空间。这种抽象将分布式计算变成貌似多线程应用程序。唯一的区别是这些线程在不同的机器上运行，而不是在同一台机器上的不同处理器上。
* 大多数 DSM 系统的工作方式是通过操作系统的虚拟内存系统。在一台计算机上访问页面时，可能会发生两种情况。在第一种（最佳）情况下，页面已经是机器上的本地页面，因此可以快速获取数据。在第二种情况下，页面目前在其他机器上。发生页面错误，页面错误处理程序将消息发送到其他计算机以获取页面，将其装入请求进程的页表中，然后继续执行。
* 由于许多原因，这种方法今天并未广泛使用。DSM 最大的问题是它如何处理故障。例如，想象一下，如果机器出现故障。那台机器上的页面会发生什么？如果分布式计算的数据结构分布在整个地址空间怎么办？在这种情况下，这些数据结构的一部分将突然变得不可用。如果部分地址空间丢失，处理故障会很难。想象一下链表，其中下一个指针指向已经消失的地址空间的一部分。另一个问题是性能。人们通常认为，在编写代码时，访问内存的成本很低。在 DSM 系统中，一些访问是便宜的，但是其他访问导致页面错误和远程机器的昂贵提取。因此，这种 DSM 系统的程序员必须非常小心地组织计算，以便几乎不发生任何通信，从而打败了这种方法的主要出发点。虽然在这个领域进行了大量研究，但实际影响不大。没有人用 DSM构建可靠的分布式系统。

##### 远程过程调用（RPC） 

* 虽然最终结果表明，操作系统抽象对于构建分布式系统来说是一个糟糕的选择，但编程语言（PL）抽象要有意义得多。最主要的抽象是基于远程过程调用（Remote Procedure Call），或简称 RPC。
* 远程过程调用包都有一个简单的目标：使在远程机器上执行代码的过程像调用本地函数一样简单直接。因此，对于客户端来说，进行一个过程调用，并在一段时间后返回结果。服务器只是定义了一些它希望导出的例程。其余的由 RPC 系统处理，RPC 系统通常有两部分：存根生成器（stub generator，有时称为协议编译器，protocol compiler）和运行时库（run-time library）。

##### 存根生成器 

* 存根生成器的工作很简单：通过自动化，消除将函数参数和结果打包成消息的一些痛苦。这有许多好处：通过设计避免了手工编写此类代码时出现的简单错误。此外，存根生成器也许可以优化此类代码，从而提高性能。

* 这种编译器的输入就是服务器希望导出到客户端的一组调用。从概念上讲，它可能就像这样简单：

  ```
  interface { 
      int func1(int arg1); 
      int func2(int arg1, int arg2); 
  }; 
  ```

  存根生成器接受这样的接口，并生成一些不同的代码片段。对于客户端，生成客户端存根（client stub），其中包含接口中指定的每个函数。希望使用此 RPC 服务的客户端程序将链接此客户端存根，调用它以进行 RPC。在内部，客户端存根中的每个函数都执行远程过程调用所需的所有工作。对于客户端，代码只是作为函数调用出现（例如，客户端调用 func1(x)）。在内部，func1()的客户端存根中的代码执行此操作：

  * 创建消息缓冲区。消息缓冲区通常只是某种大小的连续字节数组。
  * 将所需信息打包到消息缓冲区中。该信息包括要调用的函数的某种标识符，以及函数所需的所有参数。将所有这些信息放入单个连续缓冲区的过程，有时被称为参数的封送处理或消息的序列化。
  * 将消息发送到目标 RPC 服务器。与 RPC 服务器的通信，以及使其正常运行所需的所有细节，都由 RPC 运行时库处理，如下所述。
  * 等待回复。由于函数调用通常是同步的（synchronous），因此调用将等待其完成。
  * 解包返回代码和其他参数。如果函数只返回一个返回码，那么这个过程很简单。但是，较复杂的函数可能会返回更复杂的结果（例如，列表），因此存根可能也需要对它们解包。此步骤也称为解封送处理或反序列化。
  * 返回调用者。最后，只需从客户端存根返回到客户端代码。

* 对于服务器，也会生成代码。在服务器上执行的步骤如下：
  * 解包消息。此步骤称为解封送处理（unmarshaling）或反序列化（deserialization），
  * 将信息从传入消息中取出。提取函数标识符和参数。
  * 调用实际函数。终于，我们到了实际执行远程函数的地方。RPC 运行时调用 ID 指定的函数，并传入所需的参数。
  * 打包结果。返回参数被封送处理，放入一个回复缓冲区。
  * 发送回复。回复最终被发送给调用者。
* 在存根编译器中还有一些其他重要问题需要考虑。第一个是复杂的参数，即一个包如何发送复杂的数据结构？例如，调用 write()系统调用时，会传入 3 个参数：一个整数文件描述符，一个指向缓冲区的指针，以及一个大小，指示要写入多少字节（从指针开始）。如果向 RPC 包传入了一个指针，它需要能够弄清楚如何解释该指针，并执行正确的操作。通常，这是通过众所周知的类型（例如，用于传递给定大小的数据块的缓冲区 t，RPC 编译器可以理解），或通过使用更多信息注释数据结构来实现的，从而让编译器知道哪些字节需要序列化。另一个重要问题是关于并发性的服务器组织方式。一个简单的服务器只是在一个简单的循环中等待请求，并一次处理一个请求。但是，你可能已经猜到，这可能非常低效。如果一个 RPC 调用阻塞（例如，在 I/O 上），就会浪费服务器资源。因此，大多数服务器以某种并发方式构造。常见的组织方式是线程池（thread pool）。在这种组织方式中，服务器启动时会创建一组有限的线程。消息到达时，它被分派给这些工作线程之一，然后执行 RPC调用的工作，最终回复。在此期间，主线程不断接收其他请求，并可能将其发送给其他工作线程。这样的组织方式支持服务器内并发执行，从而提高其利用率。标准成本也会出现，主要是编程复杂性，因为 RPC 调用现在可能需要使用锁和其他同步原语来确保它们的正确运行。

##### 运行时库 

* 运行时库处理 RPC 系统中的大部分繁重工作。这里处理大多数性能和可靠性问题。接下来讨论构建此类运行时层的一些主要挑战。
* 我们必须克服的首要挑战之一，是如何找到远程服务。这个命名（naming）问题在分布式系统中很常见，在某种意义上超出了我们当前讨论的范围。最简单的方法建立在现有命名系统上，例如，当前互联网协议提供的主机名和端口号。在这样的系统中，客户端必须知道运行所需 RPC 服务的机器的主机名或 IP 地址，以及它正在使用的端口号（端口号就是在机器上标识发生的特定通信活动的一种方式，允许同时使用多个通信通道）。然后，协议套件必须提供一种机制，将数据包从系统中的任何其他机器路由到特定地址。
* 一旦客户端知道它应该与哪个服务器通信，以获得特定的远程服务，下一个问题是应该构建 RPC 的传输级协议。具体来说，RPC 系统应该使用可靠的协议（如 TCP/IP），还是建立在不可靠的通信层（如 UDP/IP）上
* 显然，我们希望将请求可靠地传送到远程服务器，显然，我们希望能够可靠地收到回复。因此，我们应该选择 TCP 这样的可靠传输协议，对吗？遗憾的是，在可靠的通信层之上构建 RPC 可能会导致性能的低效率。回顾上面的讨论，可靠的通信层如何工作：确认和超时/重试。因此，当客户端向服务器发送 RPC 请求时，服务器以确认响应，以便调用者知道收到了请求。类似地，当服务器将回复发送到客户端时，客户端会对其进行确认，以便服务器知道它已被接收。在可靠的通信层之上构建请求/响应协议（例如 RPC），必须发送两个“额外”消息。因此，许多 RPC 软件包都建立在不可靠的通信层之上，例如 UDP。这样做可以实现更高效的 RPC 层，但确实增加了为 RPC 系统提供可靠性的责任。
* RPC 层通过使用超时/重试和确认来实现所需的责任级别，就像我们上面描述的那样。通过使用某种形式的序列编号，通信层可以保证每个 RPC 只发生一次（在没有故障的情况下），或者最多只发生一次（在发生故障的情况下）。

##### 其他问题 

* 还有一些其他问题，RPC 的运行时也必须处理。例如，当远程调用需要很长时间才能完成时，会发生什么？鉴于我们的超时机制，长时间运行的远程调用可能被客户端认为是故障，从而触发重试，因此需要小心。一种解决方案是在没有立即生成回复时使用显式确认（从接收方到发送方）。这让客户端知道服务器收到了请求。然后，经过一段时间后，客户端可以定期询问服务器是否仍在处理请求。如果服务器一直说“是”，客户端应该继续等待（毕竟，有时过程调用可能需要很长时间才能完成执行）。
* 运行时还必须处理具有大参数的过程调用，大于可以放入单个数据包的过程。一些底层的网络协议提供这样的发送方分组（fragmentation，较大的包分成一组较小的包）和接收方重组（reassembly，较小的部分组成一个较大的逻辑整体）。如果没有，RPC 运行时可能必须自己实现这样的功能。
* 许多系统要处理的一个问题是字节序（byte ordering）。有些机器存储值时采用所谓的大端序（big endian），而其他机器采用小端序（little endian）。大端序存储从最高有效位到最低有效位的字节（比如整数），非常像阿拉伯数字。小端序则相反。两者对存储数字信息同样有效。这里的问题是如何在不同字节序的机器之间进行通信。RPC 包通常在其消息格式中提供明确定义的字节序，从而处理该问题。在 Sun 的 RPC包中，XDR（eXternal Data Representation，外部数据表示）层提供此功能。如果发送或接收消息的计算机与 XDR 的字节顺序匹配，就会按预期发送和接收消息。但是，如果机器通信具有不同的字节序，则必须转换消息中的每条信息。因此，字节顺序的差异可以有一点性能成本。
* 最后一个问题是：是否向客户端暴露通信的异步性质，从而实现一些性能优化。具体来说，典型的 RPC 是同步（synchronously）的，即当客户端发出过程调用时，它必须等待过程调用返回，然后再继续。因为这种等待可能很长，而且因为客户端可能正在执行其他工作，所以某些 RPC 包让你能够异步（asynchronously）地调用 RPC。当发出异步 RPC 时，RPC 包发送请求并立即返回。然后，客户端可以自由地执行其他工作，例如调用其他 RPC，或进行其他有用的计算。客户端在某些时候会希望看到异步 RPC 的结果。因此它再次调用 RPC 层，告诉它等待未完成的 RPC 完成，此时可以访问返回的结果。

##### 端到端的论点

* 端到端的论点（end-to-end argument）表明，系统中的最高层（通常是“末端”的应用程序）最终是分层系统中唯一能够真正实现某些功能的地方。
* 两台机器之间可靠的文件传输。如果要将文件从机器 A 传输到机器 B，并确保最终在 B 上的字节与从 A 开始的字节完全相同，则必须对此进行“端到端”检查。较低级别的可靠机制，例如在网络或磁盘中，不提供这种保证。
* 与此相对的是一种方法，尝试通过向系统的较低层添加可靠性，来解决可靠文件传输问题。例如，假设我们构建了一个可靠的通信协议，并用它来构建可靠的文件传输。通信协议保证发送方发送的每个字节都将由接收方按顺序接收，例如使用超时/重试、确认和序列号。遗憾的是，使用这样的协议并不能实现可靠的文件传输。想象一下，在通信发生之前，发送方内存中的字节被破坏，或者当接收方将数据写入磁盘时发生了一些不好的事情。在这些情况下，即使字节在网络上可靠地传递，文件传输最终也不可靠。要构建可靠的文件传输，必须包括端到端的可靠性检查，例如，在整个传输完成后，读取接收方磁盘上的文件，计算校验和，并将该校验和与发送方文件的校验和进行比较。按照这个准则的推论是，有时候，较低层提供额外的功能确实可以提高系统性能，或在其他方面优化系统。因此，不应该排除在系统中较低层的这种机制。实际上，你应该小心考虑这种机制的实用性，考虑它最终对整个系统或应用程序的作用。

#### NFS

* 分布式客户端/服务器计算的首次使用之一，是在分布式文件系统领域。在这种环境中，有许多客户端机器和一个服务器（或几个）。服务器将数据存储在其磁盘上，客户端通过结构良好的协议消息请求数据。

  [!image](./images/56.png)



































